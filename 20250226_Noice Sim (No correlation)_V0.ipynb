{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Study for Noise Filtering\n",
    "\n",
    "This is the v0 for the simulation study on the sparse jump model comparison with HMM, to show that SJM is able to filter away noisy data by using the weighting in the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from jumpmodels.sparse_jump import SparseJumpModel    # Sparse JM class\n",
    "from jumpmodels.jump import JumpModel    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Simulation & Utility Functions\n",
    "def simulate_data(T, P, mu, random_state=None): \"\"\" Simulate data from a 3-state Gaussian HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(T, P, mu, random_state=None):\n",
    "    \"\"\"\n",
    "    Simulate data from a 3-state Gaussian HMM.\n",
    "    \n",
    "    Parameters:\n",
    "        T (int): Number of observations.\n",
    "        P (int): Total number of features (only first 15 are informative).\n",
    "        mu (float): Signal magnitude for informative features.\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        X (ndarray): Simulated observations (T x P).\n",
    "        states (ndarray): True state sequence (length T).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    \n",
    "    # Transition matrix as given in your original code\n",
    "    transmat = np.array([[0.9903, 0.0047, 0.0050],\n",
    "                         [0.0157, 0.9666, 0.0177],\n",
    "                         [0.0284, 0.0300, 0.9416]])\n",
    "    transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute stationary distribution (eigenvector corresponding to eigenvalue 1)\n",
    "    eigvals, eigvecs = np.linalg.eig(transmat.T)\n",
    "    stat = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n",
    "    stat = stat[:, 0]\n",
    "    stat = stat / np.sum(stat)\n",
    "    \n",
    "    # Generate state sequence\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    states[0] = rng.choice(np.arange(3), p=stat)\n",
    "    for t in range(1, T):\n",
    "        states[t] = rng.choice(np.arange(3), p=transmat[states[t-1]])\n",
    "    \n",
    "    # Define means for each state\n",
    "    means = np.zeros((3, P))\n",
    "    # State 0: +mu in first 15 features\n",
    "    # State 1: 0\n",
    "    # State 2: -mu in first 15 features\n",
    "    if P >= 15:\n",
    "        means[0, :15] = mu\n",
    "        means[2, :15] = -mu\n",
    "    else:\n",
    "        means[0, :P] = mu\n",
    "        means[2, :P] = -mu\n",
    "    \n",
    "    # Generate observations: N(means[state], I_P)\n",
    "    X = np.zeros((T, P))\n",
    "    for t in range(T):\n",
    "        X[t] = rng.normal(loc=means[states[t]], scale=1.0, size=P)\n",
    "    \n",
    "    return X, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Aligning Predicted Labels With True Labels using the Hungarian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_labels(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Align predicted labels with true labels using the Hungarian algorithm.\n",
    "    \n",
    "    Returns:\n",
    "        aligned (ndarray): Predicted labels after optimal permutation.\n",
    "    \"\"\"\n",
    "    D = confusion_matrix(true_labels, pred_labels)\n",
    "    row_ind, col_ind = linear_sum_assignment(-D)\n",
    "    mapping = {col: row for row, col in zip(row_ind, col_ind)}\n",
    "    aligned = np.array([mapping[x] for x in pred_labels])\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up the function to calcuate the BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bac(true_states, pred_states):\n",
    "    \"\"\"\n",
    "    Compute the Balanced Accuracy (BAC) after aligning the predicted state labels.\n",
    "    \"\"\"\n",
    "    aligned_pred = align_labels(true_states, pred_states)\n",
    "    return balanced_accuracy_score(true_states, aligned_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions for model formulation\n",
    "\n",
    "### 4.1 HMM With Nystrup (2021) initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hmm(X, n_components=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Fit a Gaussian HMM to the data X with the following initialization:\n",
    "      - Self-transition probability set to 0.95.\n",
    "      - Covariance prior set to 1.0 (for regularization).\n",
    "      - Up to 100 iterations of the EM algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Data matrix.\n",
    "        n_components (int): Number of hidden states.\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        pred_states (ndarray): Predicted state sequence using Viterbi decoding.\n",
    "    \"\"\"\n",
    "    model = hmm.GaussianHMM(\n",
    "        n_components=n_components,             # Number of hidden states\n",
    "        covariance_type='diag',                # Diagonal covariance matrices\n",
    "        n_iter=100,                            # Maximum number of EM iterations\n",
    "        random_state=random_state,             # Seed for reproducibility\n",
    "        init_params=\"mc\",                      # Initialize means ('m') and covariances ('c')\n",
    "        covars_prior=1.0                   # Regularization: prior added to covariance estimates\n",
    "    )\n",
    "    # Set uniform start probabilities\n",
    "    model.startprob_ = np.full(n_components, 1.0 / n_components)\n",
    "    # Initialize transition matrix: 0.95 on the diagonal, the remaining probability spread evenly\n",
    "    transmat = np.full((n_components, n_components), 0.05 / (n_components - 1))\n",
    "    np.fill_diagonal(transmat, 0.95)\n",
    "    model.transmat_ = transmat\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X)\n",
    "    # Predict the hidden state sequence using the Viterbi algorithm\n",
    "    pred_states = model.predict(X)\n",
    "    return pred_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normal (Standard) Jump Model with Grid Search over λ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_jump_model_grid_search(X, true_states, n_components=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Perform a grid search over 14 lambda values (logspace from 1e-2 to 1e4) for the jump model.\n",
    "    \n",
    "    For each lambda value:\n",
    "      - A JumpModel is initialized and fitted.\n",
    "      - The jump penalty (lambda) controls the cost of switching states.\n",
    "        - A low lambda allows frequent state changes.\n",
    "        - A high lambda penalizes state changes, resulting in fewer jumps.\n",
    "      - The parameter 'cont' specifies whether the jump model is continuous (True) or discrete (False).\n",
    "      - 'max_iter' defines the maximum number of iterations for the model fitting procedure.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Data matrix.\n",
    "        true_states (ndarray): The true hidden state sequence.\n",
    "        n_components (int): Number of states.\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        best_labels (ndarray): Predicted state sequence for the best lambda.\n",
    "        best_bac (float): Best balanced accuracy achieved.\n",
    "    \"\"\"\n",
    "    # Create 14 lambda values logarithmically spaced from 0.01 to 10,000.\n",
    "    lambda_values = np.logspace(0.001, 10000, 14)\n",
    "    best_bac = -1\n",
    "    best_labels = None\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        # Create a JumpModel instance with the following parameters:\n",
    "        model = JumpModel(\n",
    "            n_components=n_components,    # Number of hidden states\n",
    "            jump_penalty=lam,             # Lambda: penalty for a state transition\n",
    "            cont=False,                   # 'cont': if False, model uses discrete jumps\n",
    "            max_iter=10,                  # Maximum number of iterations for fitting the model\n",
    "            random_state=random_state     # Seed for reproducibility\n",
    "        )\n",
    "        # Fit the jump model to the data X\n",
    "        model.fit(X)\n",
    "        # Retrieve predicted state labels from the model\n",
    "        labels = model.labels_\n",
    "        # Calculate Balanced Accuracy (BAC) after aligning predicted labels with true states\n",
    "        bac = calculate_bac(true_states, labels)\n",
    "        # Update the best result if this lambda gives a higher BAC\n",
    "        if bac > best_bac:\n",
    "            best_bac = bac\n",
    "            best_labels = labels\n",
    "    \n",
    "    return best_labels, best_bac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sparse Jump Model with Grid Search over λ and kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sparse_jump_model_grid_search(X, true_states, n_components=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Perform a grid search for the best combination of jump_penalty (lambda) and feature selection\n",
    "    level for the Sparse Jump Model (SJM). In SJM, feature selection is controlled by 'max_feats',\n",
    "    which is defined as the square of 'kappa'. Here we vary kappa from 1 to sqrt(P) and set\n",
    "    max_feats = kappa**2.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        Data matrix of shape (T, P) where T is the number of observations and P the number of features.\n",
    "    true_states : ndarray\n",
    "        The true hidden state sequence.\n",
    "    n_components : int, default=3\n",
    "        Number of hidden states (clusters).\n",
    "    random_state : int or None, optional\n",
    "        Seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_labels : ndarray\n",
    "        Predicted state sequence for the best combination of parameters.\n",
    "    best_bac : float\n",
    "        The best balanced accuracy achieved.\n",
    "    \n",
    "    Model Parameters (as per documentation)\n",
    "    -----------------------------------------\n",
    "    - jump_penalty : float\n",
    "        The penalty for state transitions. In the SJM, this penalty is internally scaled by 1/sqrt(n_features).\n",
    "    - cont : bool, default=False\n",
    "        Use discrete jumps (False) rather than continuous.\n",
    "    - max_feats : float, default=100.\n",
    "        Controls the number of features included. This is set to kappa^2.\n",
    "    - max_iter : int, default=30\n",
    "        Maximum number of iterations for the coordinate descent algorithm (feature selection).\n",
    "    - Other parameters (tol_w, max_iter_jm, tol_jm, n_init_jm, verbose) use their default values.\n",
    "    \"\"\"\n",
    "    # Define 7 lambda values on a log-scale from 1e-1 to 1e2.\n",
    "    lambdas = np.logspace(0.1, 100, 7)\n",
    "    p = X.shape[1]  # Total number of features\n",
    "    # Define 14 kappa values ranging from 1 to sqrt(P).\n",
    "    kappas = np.linspace(1, np.sqrt(p), 14)\n",
    "    \n",
    "    best_bac = -1\n",
    "    best_labels = None\n",
    "    \n",
    "    # Grid search over all combinations of lambda and kappa.\n",
    "    for lam in lambdas:\n",
    "        for kappa in kappas:\n",
    "            # Compute max_feats as the square of kappa (as per documentation).\n",
    "            max_feats = kappa**2\n",
    "            \n",
    "            # Create the SparseJumpModel instance.\n",
    "            # Key parameters:\n",
    "            # - n_components: number of states.\n",
    "            # - jump_penalty: lambda value controlling cost of switching states.\n",
    "            # - cont: set to False for the discrete jump model.\n",
    "            # - max_feats: effective number of features (kappa^2).\n",
    "            # - max_iter: maximum iterations for the coordinate descent algorithm (default=30 per docs).\n",
    "            # - random_state: seed for reproducibility.\n",
    "            model = SparseJumpModel(\n",
    "                n_components=n_components,\n",
    "                jump_penalty=lam,\n",
    "                cont=False,\n",
    "                max_feats=max_feats,     # effective number of features = kappa^2\n",
    "                max_iter=30,             # default from documentation (30 iterations)\n",
    "                random_state=random_state\n",
    "                # Additional parameters such as tol_w, max_iter_jm, tol_jm, n_init_jm, and verbose\n",
    "                # will use their default values.\n",
    "            )\n",
    "            # Fit the Sparse Jump Model to the data.\n",
    "            model.fit(X)\n",
    "            # Retrieve the predicted state labels.\n",
    "            labels = model.labels_\n",
    "            # Calculate the Balanced Accuracy (BAC) by aligning labels to true states.\n",
    "            bac = calculate_bac(true_states, labels)\n",
    "            # Update best_bac and best_labels if this combination yields a higher BAC.\n",
    "            if bac > best_bac:\n",
    "                best_bac = bac\n",
    "                best_labels = labels\n",
    "    \n",
    "    return best_labels, best_bac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Main Execution\n",
    " We split the code into three sections for each model and then combine results at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 58\u001b[0m\n\u001b[1;32m     50\u001b[0m jump_bac_list\u001b[38;5;241m.\u001b[39mappend(bac_jump)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ----------------- 3) Sparse Jump Model --------------\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Perform grid search over a combination of lambda and kappa values.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# lambda (jump_penalty) controls the cost of state transitions.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# kappa controls the sparsity (feature selection) in the model.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# 'max_feats=500' limits the number of features considered.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# 'max_iter=10' limits the number of iterations.\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m _, bac_sparse \u001b[38;5;241m=\u001b[39m run_sparse_jump_model_grid_search(X, true_states,\n\u001b[1;32m     59\u001b[0m                                                   n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     60\u001b[0m                                                   random_state\u001b[38;5;241m=\u001b[39msim)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Save the BAC score.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m sparse_bac_list\u001b[38;5;241m.\u001b[39mappend(bac_sparse)\n",
      "Cell \u001b[0;32mIn[35], line 72\u001b[0m, in \u001b[0;36mrun_sparse_jump_model_grid_search\u001b[0;34m(X, true_states, n_components, random_state)\u001b[0m\n\u001b[1;32m     61\u001b[0m model \u001b[38;5;241m=\u001b[39m SparseJumpModel(\n\u001b[1;32m     62\u001b[0m     n_components\u001b[38;5;241m=\u001b[39mn_components,\n\u001b[1;32m     63\u001b[0m     jump_penalty\u001b[38;5;241m=\u001b[39mlam,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# will use their default values.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Fit the Sparse Jump Model to the data.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Retrieve the predicted state labels.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m labels \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/jumpmodels/sparse_jump.py:380\u001b[0m, in \u001b[0;36mSparseJumpModel.fit\u001b[0;34m(self, X, ret_ser, sort_by)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m: jm\u001b[38;5;241m.\u001b[39mcenters_ \u001b[38;5;241m=\u001b[39m centers_unweighted \u001b[38;5;241m*\u001b[39m feat_weights    \n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# fit JM on weighted data\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m jm\u001b[38;5;241m.\u001b[39mfit(X, ret_ser\u001b[38;5;241m=\u001b[39mret_ser, feat_weights\u001b[38;5;241m=\u001b[39mfeat_weights, sort_by\u001b[38;5;241m=\u001b[39msort_by)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Step 2: optimize w\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# update (unweighted) centers\u001b[39;00m\n\u001b[1;32m    383\u001b[0m centers_unweighted \u001b[38;5;241m=\u001b[39m weighted_mean_cluster(X_arr, jm\u001b[38;5;241m.\u001b[39mproba_)\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/jumpmodels/jump.py:519\u001b[0m, in \u001b[0;36mJumpModel.fit\u001b[0;34m(self, X, ret_ser, feat_weights, sort_by)\u001b[0m\n\u001b[1;32m    517\u001b[0m     centers_ \u001b[38;5;241m=\u001b[39m weighted_mean_cluster(X_arr, proba_) \n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# E step\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     proba_, labels_, val_ \u001b[38;5;241m=\u001b[39m do_E_step(X_arr, centers_, jump_penalty_mx, prob_vecs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob_vecs)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_init_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-th init. val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# compare with previous initializations\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/jumpmodels/jump.py:214\u001b[0m, in \u001b[0;36mdo_E_step\u001b[0;34m(X, centers_, penalty_mx, prob_vecs, return_value_mx)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_value_mx: \u001b[38;5;28;01mreturn\u001b[39;00m dp(loss_mx, penalty_mx, return_value_mx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# do a full E step\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m labels_, val_ \u001b[38;5;241m=\u001b[39m dp(loss_mx, penalty_mx, return_value_mx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)     \u001b[38;5;66;03m# output labels_ is of type int\u001b[39;00m\n\u001b[1;32m    215\u001b[0m proba_ \u001b[38;5;241m=\u001b[39m raise_JM_labels_to_proba(labels_, n_c, prob_vecs)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proba_, labels_, val_\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/jumpmodels/jump.py:130\u001b[0m, in \u001b[0;36mdp\u001b[0;34m(loss_mx, penalty_mx, return_value_mx)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# DP iteration\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_s):\n\u001b[0;32m--> 130\u001b[0m     values[t] \u001b[38;5;241m=\u001b[39m loss_mx[t] \u001b[38;5;241m+\u001b[39m (values[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m+\u001b[39m penalty_mx)\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# values[t-1][:, np.newaxis] turns the (t-1)-th row into a column\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_value_mx:\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/numpy/core/_methods.py:43\u001b[0m, in \u001b[0;36m_amin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_maximum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Settings ---\n",
    "    # T: Number of time points/observations.\n",
    "    # mu_values: List of signal magnitudes for the informative features.\n",
    "    # p_values: List of numbers of features (dimensions) to simulate.\n",
    "    # n_simulations: Number of simulation runs per (mu, P) combination.\n",
    "    T = 500\n",
    "    mu_values = [0.25, 0.5, 0.75, 1.0]   # Different signal magnitudes\n",
    "    p_values = [15, 30, 60, 150, 300]      # Different numbers of features\n",
    "    n_simulations = 10                   # More simulations yield more robust results\n",
    "    \n",
    "    # final_rows will store the summary results for each (mu, P) combination.\n",
    "    final_rows = []\n",
    "    \n",
    "    # Loop over each combination of signal magnitude and number of features.\n",
    "    for mu in mu_values:\n",
    "        for P in p_values:\n",
    "            \n",
    "            # Create lists to collect Balanced Accuracy (BAC) scores from each model\n",
    "            # across multiple simulation runs.\n",
    "            hmm_bac_list = []      # For the Gaussian HMM\n",
    "            jump_bac_list = []     # For the standard Jump Model\n",
    "            sparse_bac_list = []   # For the Sparse Jump Model\n",
    "            \n",
    "            # Run multiple simulations to obtain robust performance estimates.\n",
    "            for sim in range(n_simulations):\n",
    "                # Simulate data for the current settings:\n",
    "                # X: data matrix of shape (T, P)\n",
    "                # true_states: true underlying state sequence.\n",
    "                X, true_states = simulate_data(T, P, mu, random_state=sim)\n",
    "                \n",
    "                # ----------------- 1) Gaussian HMM -------------------\n",
    "                # Fit the Gaussian HMM (with Nystrup 2021 initialization) and predict states.\n",
    "                pred_hmm = run_hmm(X, n_components=3, random_state=sim)\n",
    "                # Calculate the Balanced Accuracy (BAC) by aligning predicted states with true states.\n",
    "                bac_hmm = calculate_bac(true_states, pred_hmm)\n",
    "                # Save the BAC score.\n",
    "                hmm_bac_list.append(bac_hmm)\n",
    "                \n",
    "                # ----------------- 2) Normal Jump Model --------------\n",
    "                # Perform grid search over lambda values for the standard Jump Model.\n",
    "                # jump_penalty (lambda) controls the cost of switching states.\n",
    "                # 'cont=False' specifies that the model uses discrete state jumps.\n",
    "                # 'max_iter=10' limits the number of iterations for model convergence.\n",
    "                _, bac_jump = run_jump_model_grid_search(X, true_states, \n",
    "                                                         n_components=3, \n",
    "                                                         random_state=sim)\n",
    "                # Save the BAC score.\n",
    "                jump_bac_list.append(bac_jump)\n",
    "                \n",
    "                # ----------------- 3) Sparse Jump Model --------------\n",
    "                # Perform grid search over a combination of lambda and kappa values.\n",
    "                # lambda (jump_penalty) controls the cost of state transitions.\n",
    "                # kappa controls the sparsity (feature selection) in the model.\n",
    "                # 'max_feats=500' limits the number of features considered.\n",
    "                # 'max_iter=10' limits the number of iterations.\n",
    "                _, bac_sparse = run_sparse_jump_model_grid_search(X, true_states,\n",
    "                                                                  n_components=3,\n",
    "                                                                  random_state=sim)\n",
    "                # Save the BAC score.\n",
    "                sparse_bac_list.append(bac_sparse)\n",
    "            \n",
    "            # Compute the mean and standard deviation of BAC scores for each method.\n",
    "            hmm_mean, hmm_std = np.mean(hmm_bac_list), np.std(hmm_bac_list)\n",
    "            jump_mean, jump_std = np.mean(jump_bac_list), np.std(jump_bac_list)\n",
    "            sparse_mean, sparse_std = np.mean(sparse_bac_list), np.std(sparse_bac_list)\n",
    "            \n",
    "            # Perform a paired t-test between the standard Jump Model and the Sparse Jump Model.\n",
    "            # The null hypothesis is that their mean BAC scores are equal.\n",
    "            tstat, pval = stats.ttest_rel(jump_bac_list, sparse_bac_list)\n",
    "            # If the Sparse Jump Model has a significantly higher mean (p < 0.05), mark it in bold.\n",
    "            if (pval < 0.05) and (sparse_mean > jump_mean):\n",
    "                sparse_str = f\"**{sparse_mean:.2f} ± {sparse_std:.2f}**\"\n",
    "            else:\n",
    "                sparse_str = f\"{sparse_mean:.2f} ± {sparse_std:.2f}\"\n",
    "            \n",
    "            # Store the results for this (mu, P) combination in a dictionary.\n",
    "            final_rows.append({\n",
    "                \"mu\": mu,\n",
    "                \"P\": P,\n",
    "                \"HMM (mean ± std)\": f\"{hmm_mean:.2f} ± {hmm_std:.2f}\",\n",
    "                \"Jump (mean ± std)\": f\"{jump_mean:.2f} ± {jump_std:.2f}\",\n",
    "                \"Sparse Jump (mean ± std)\": sparse_str,\n",
    "                \"p-value (Jump vs Sparse)\": f\"{pval:.3g}\"\n",
    "            })\n",
    "    \n",
    "    # Convert the final results into a pandas DataFrame and display it.\n",
    "    df_results = pd.DataFrame(final_rows)\n",
    "    print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
