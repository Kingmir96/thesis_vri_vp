{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SJM-BL Simulation study (scenario 1)\n",
    "### 1.0 Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For HMM and model training\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import wilcoxon\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Import jumpmodels (including our custom preprocess functions)\n",
    "from jumpmodels.sparse_jump import SparseJumpModel\n",
    "from jumpmodels.jump import JumpModel\n",
    "from jumpmodels.preprocess import StandardScalerPD, DataClipperStd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Importing Portfolio Packages\n",
    "from pypfopt.black_litterman import BlackLittermanModel, market_implied_risk_aversion\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import base_optimizer, expected_returns, risk_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Simulation\n",
    "\n",
    "#### 2.1 Simulating the 1-state data\n",
    "We are simulating 6 fictional assets which are representing the 6 factors in our framework\n",
    "- **1-State:** A single regime with Student‑t returns.\n",
    "- **2-State:** A two-regime (bull/bear) HMM with state-dependent parameters.\n",
    "- **3-State:** A three-regime HMM with specified means and volatilitie\n",
    "\n",
    "All assets have the same expected return and volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define asset names and seed RNG for reproducibility.\n",
    "assets = [\"Value\", \"Growth\", \"LowVol\", \"Size\", \"Momentum\", \"Quality\"]\n",
    "n_assets = len(assets)\n",
    "rng = np.random.default_rng(42)  # random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_1state_data(T):\n",
    "    \"\"\"\n",
    "    1-state model: returns follow a Student-t distribution.\n",
    "    \"\"\"\n",
    "    mu = 0.000461\n",
    "    sig = 0.008388\n",
    "    dof = 5\n",
    "    # Create a correlation matrix with off-diagonals = 0.185\n",
    "    corr = np.full((n_assets, n_assets), 0.185)\n",
    "    np.fill_diagonal(corr, 1.0)  # set diagonals to 1\n",
    "    Cov = (sig * np.ones(n_assets))[:, None] @ (sig * np.ones(n_assets))[None, :] * corr  # covariance matrix\n",
    "\n",
    "    # Generate multivariate draws and scale using chi-square factor\n",
    "    z = rng.multivariate_normal(mean=np.zeros(n_assets), cov=Cov, size=T)\n",
    "    chi = rng.chisquare(dof, size=T)\n",
    "    factor = np.sqrt(dof / chi)\n",
    "    rets = mu + z * factor[:, np.newaxis]\n",
    "    \n",
    "    return pd.DataFrame(rets, columns=assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Simulating 2-state data\n",
    "\n",
    "This function simulates a 2-state HMM (bull/bear) with state‐dependent Student‑t returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_2state_data(T):\n",
    "    \"\"\"\n",
    "    2-state HMM: bull and bear regimes.\n",
    "    \"\"\"\n",
    "    transmat = np.array([[0.9976, 0.0024],\n",
    "                          [0.0232, 0.9768]])\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    states[0] = rng.integers(2)  # random starting state\n",
    "    for t in range(1, T):\n",
    "        states[t] = rng.choice(2, p=transmat[states[t - 1]])  # choose next state\n",
    "\n",
    "    mu_dict = {0: 0.0006, 1: -0.000881}\n",
    "    sig_dict = {0: 0.00757, 1: 0.0163}\n",
    "    corr = np.full((n_assets, n_assets), 0.185)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    \n",
    "    rets = np.zeros((T, n_assets))\n",
    "    dof = 5\n",
    "    for t in range(T):\n",
    "        s = states[t]\n",
    "        mu_s = np.full(n_assets, mu_dict[s])\n",
    "        sig_s = np.full(n_assets, sig_dict[s])\n",
    "        Cov_s = np.outer(sig_s, sig_s) * corr  # compute state-dependent covariance\n",
    "        z = rng.multivariate_normal(mean=np.zeros(n_assets), cov=Cov_s)\n",
    "        chi = rng.chisquare(dof)\n",
    "        factor = np.sqrt(dof / chi)\n",
    "        rets[t] = mu_s + factor * z\n",
    "\n",
    "    return pd.DataFrame(rets, columns=assets), states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Simulating 3-state data\n",
    "\n",
    "We are simulating 6 fictional assets which are representing the 6 factors in our framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_3state_data(T):\n",
    "    \"\"\"\n",
    "    3-state HMM: three regimes with specified means and volatilities.\n",
    "    \"\"\"\n",
    "    transmat = np.array([[0.9950, 0.004335, 0.000665],\n",
    "                         [0.01667, 0.95, 0.03333],\n",
    "                         [0.00652, 0.04348, 0.9500]])\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    states[0] = rng.integers(3)\n",
    "    for t in range(1, T):\n",
    "        states[t] = rng.choice(3, p=transmat[states[t - 1]])\n",
    "\n",
    "    mu_list = [0.0005862, 0.0, -0.0008672]\n",
    "    sig_list = [0.0075313, 0.0135351, 0.0163387]\n",
    "    corr = np.full((n_assets, n_assets), 0.185)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    \n",
    "    rets = np.zeros((T, n_assets))\n",
    "    dof = 5\n",
    "    for t in range(T):\n",
    "        s = states[t]\n",
    "        mu_s = np.full(n_assets, mu_list[s])\n",
    "        sig_s = np.full(n_assets, sig_list[s])\n",
    "        Cov_s = np.outer(sig_s, sig_s) * corr  # state-specific covariance\n",
    "        z = rng.multivariate_normal(mean=np.zeros(n_assets), cov=Cov_s)\n",
    "        chi = rng.chisquare(dof)\n",
    "        factor = np.sqrt(dof / chi)\n",
    "        rets[t] = mu_s + factor * z\n",
    "\n",
    "    return pd.DataFrame(rets, columns=assets), states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Training Regime Models\n",
    "\n",
    "#### 3.1 Training HMM using kmeans clustering initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmm_kmeans(X, n_components=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Fit a GaussianHMM using k-means for initialization.\n",
    "    \"\"\"\n",
    "    model = GaussianHMM(n_components=n_components, covariance_type=\"diag\",\n",
    "                        n_iter=100, random_state=random_state)\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10, random_state=random_state)\n",
    "    labels = kmeans.fit_predict(X)  # initial clustering\n",
    "    means, covars = [], []\n",
    "    for i in range(n_components):\n",
    "        obs_i = X[labels == i]\n",
    "        means.append(obs_i.mean(axis=0))  # compute mean for cluster\n",
    "        covars.append(obs_i.var(axis=0) + 1e-2)  # compute variance, add small value\n",
    "    # Set HMM parameters\n",
    "    model.startprob_ = np.ones(n_components) / n_components\n",
    "    model.transmat_  = np.ones((n_components, n_components)) / n_components\n",
    "    model.means_     = np.array(means)\n",
    "    model.covars_    = np.array(covars)\n",
    "    model.init_params = 'tmc'\n",
    "    \n",
    "    model.fit(X)  # train HMM\n",
    "    pred_states = model.predict(X)  # predict state sequence\n",
    "    return model, pred_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Training Sparse Jump model with max_feats=9 and lambda=80\n",
    "##### 3.2.1 Defining feature selection framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(X):\n",
    "    \"\"\"\n",
    "    Standardize features (clip then scale).\n",
    "    \"\"\"\n",
    "    X_df = pd.DataFrame(X)\n",
    "    clipper = DataClipperStd(mul=3.0)  # clip outliers\n",
    "    scaler = StandardScalerPD()         # standardize data\n",
    "    X_clipped = clipper.fit_transform(X_df)\n",
    "    X_scaled = scaler.fit_transform(X_clipped)\n",
    "    return X_scaled.values\n",
    "\n",
    "def compute_temporal_features(y, l):\n",
    "    \"\"\"\n",
    "    Compute 9 temporal features using a window of length l.\n",
    "    \"\"\"\n",
    "    T = len(y)\n",
    "    feats = np.zeros((T, 9), dtype=float)\n",
    "    half = (l - 1) // 2\n",
    "    for t in range(T):\n",
    "        feats[t, 0] = y[t]  # current observation\n",
    "        feats[t, 1] = abs(y[t] - y[t-1]) if t > 0 else 0.  # left change\n",
    "        feats[t, 2] = abs(y[t+1] - y[t]) if t < (T-1) else 0.  # right change\n",
    "        left_c = max(0, t - half)\n",
    "        right_c = min(T, t + half + 1)\n",
    "        window_c = y[left_c:right_c]\n",
    "        feats[t, 3] = np.mean(window_c)  # local mean\n",
    "        feats[t, 4] = np.std(window_c)   # local std\n",
    "        left_l = max(0, t - l)\n",
    "        right_l = t\n",
    "        window_l = y[left_l:right_l]\n",
    "        feats[t, 5] = np.mean(window_l) if len(window_l) > 0 else 0.  # left window mean\n",
    "        feats[t, 6] = np.std(window_l) if len(window_l) > 0 else 0.   # left window std\n",
    "        left_r = t\n",
    "        right_r = min(T, t + l)\n",
    "        window_r = y[left_r:right_r]\n",
    "        feats[t, 7] = np.mean(window_r) if len(window_r) > 0 else 0.  # right window mean\n",
    "        feats[t, 8] = np.std(window_r) if len(window_r) > 0 else 0.   # right window std\n",
    "    return feats\n",
    "\n",
    "def combine_features(y, l_list=[5, 13]):\n",
    "    \"\"\"\n",
    "    Compute and stack features for each window length in l_list.\n",
    "    \"\"\"\n",
    "    feat_list = []\n",
    "    for l in l_list:\n",
    "        feat_list.append(compute_temporal_features(y, l))\n",
    "    return np.hstack(feat_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sjm(X, max_feats=9.0, lam=80.0, n_components=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a SparseJumpModel with given hyperparameters.\n",
    "    \"\"\"\n",
    "    sjm = SparseJumpModel(n_components=n_components,\n",
    "                          max_feats=max_feats,\n",
    "                          jump_penalty=lam,\n",
    "                          cont=False,\n",
    "                          max_iter=20,          # maximum iterations for coordinate descent\n",
    "                          random_state=random_state)\n",
    "    sjm.fit(X)  # train model\n",
    "    return sjm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Allocation simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Allocation workhorse functions\n",
    "In this code we create the in which we fit the following models (each done in a seperate for loop such that we can store the relevant data such as return, weights, etc. in seperate dfs):\n",
    "1. Equal weigted\n",
    "2. Inverse volatility weighted\n",
    "3. Mean-Variance-Optimal static portfolio\n",
    "4. Hidden Markov Model Black Litterman where infered states are the identified regimes\n",
    "5. Sparse Jump Model Black Litterman where infered states are the identified regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_mvo_allocation(returns, risk_free_rate=0.02):\n",
    "    mu = expected_returns.mean_historical_return(returns)\n",
    "    S = risk_models.sample_cov(returns)\n",
    "    S_reg = S + 1e-6 * np.eye(S.shape[0])\n",
    "    ef = EfficientFrontier(mu, S_reg, weight_bounds=(0, 1))\n",
    "    # Force a different solver:\n",
    "    weights = ef.max_sharpe(risk_free_rate=risk_free_rate, solver=\"ECOS\")  \n",
    "    return ef.clean_weights()\n",
    "\n",
    "def inverse_vol_weights(returns):\n",
    "    \"\"\"\n",
    "    Compute weights based on inverse volatility.\n",
    "    \"\"\"\n",
    "    stds = returns.std(axis=0).values\n",
    "    inv = 1.0 / (stds + 1e-12)  # avoid division by zero\n",
    "    return inv / inv.sum()\n",
    "\n",
    "def backtest_portfolio(returns, weights):\n",
    "    \"\"\"\n",
    "    Backtest a portfolio with fixed weights.\n",
    "    \"\"\"\n",
    "    T = len(returns)\n",
    "    pv = np.zeros(T)\n",
    "    pv[0] = 1.0  # initial portfolio value\n",
    "    for t in range(T - 1):\n",
    "        r = returns.iloc[t].values\n",
    "        pv[t + 1] = pv[t] * (1.0 + np.dot(weights, r))  # update portfolio value\n",
    "    return pv\n",
    "\n",
    "def bl_allocation(view_vector, prior_cov, tau=0.05, risk_free_rate=0.02):\n",
    "    \"\"\"\n",
    "    Compute BL allocation given a view vector.\n",
    "    \"\"\"\n",
    "    viewdict = {asset: view for asset, view in zip(assets, view_vector)}\n",
    "    bl = BlackLittermanModel(cov_matrix=prior_cov, absolute_views=viewdict, tau=tau, risk_aversion=1)\n",
    "    weights = bl.bl_weights()  # get BL weights\n",
    "    return np.array([weights[asset] for asset in assets])\n",
    "\n",
    "def regime_based_bl_backtest(returns, states, regime_mu_dict, prior_cov, tau=0.05, risk_free_rate=0.02, trans_cost=0.0005):\n",
    "    \"\"\"\n",
    "    Perform dynamic BL allocation using regime signals.\n",
    "    \"\"\"\n",
    "    T = len(returns)\n",
    "    n = returns.shape[1]\n",
    "    pv = np.zeros(T)\n",
    "    pv[0] = 1.0  # initial portfolio value\n",
    "    w_hist = np.zeros((T, n))\n",
    "    w_hist[0] = np.ones(n) / n  # start with equal weights\n",
    "    \n",
    "    for t in range(T - 1):\n",
    "        r = returns.iloc[t].values\n",
    "        pv[t + 1] = pv[t] * (1.0 + np.dot(w_hist[t], r))  # update using previous weights\n",
    "        # Update weights if regime has changed\n",
    "        if t == 0 or (states[t] != states[t - 1]):\n",
    "            Q = regime_mu_dict[states[t]]\n",
    "            w_new = bl_allocation(Q, prior_cov, tau=tau, risk_free_rate=risk_free_rate)\n",
    "        else:\n",
    "            w_new = w_hist[t]\n",
    "        w_hist[t + 1] = w_new  # store new weights\n",
    "        \n",
    "        # Calculate transaction costs if applicable\n",
    "        if t > 0:\n",
    "            turnover = np.sum(np.abs(w_hist[t + 1] - w_hist[t]))\n",
    "            cost = trans_cost * turnover\n",
    "        else:\n",
    "            cost = 0.0\n",
    "        pv[t + 1] *= (1.0 - cost)  # apply transaction cost\n",
    "        \n",
    "    # Final day update with last available weights\n",
    "    r_last = returns.iloc[-1].values\n",
    "    pv[-1] = pv[-2] * (1.0 + np.dot(w_hist[-2], r_last))\n",
    "    return pv, w_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Performance metric evaluation:\n",
    "Here we divide the performance metric into. We assume 250 data points to be 1 year off trading:\n",
    "1. Return-Based Metrics \n",
    "\n",
    "Annualized Return: Average return per year. \n",
    "\n",
    "Cumulative Return: Total portfolio growth over time. \n",
    "\n",
    "2. Risk-Based Metrics \n",
    "\n",
    "Volatility: Standard deviation of returns. \n",
    "\n",
    "Downside Deviation: Measures negative return fluctuations. \n",
    "\n",
    "Max Drawdown (MDD): Largest portfolio decline from peak to trough. \n",
    "\n",
    "3. Risk-Adjusted Metrics \n",
    "\n",
    "Sharpe Ratio: Return per unit of total risk. \n",
    "\n",
    "Sortino Ratio: Return per unit of downside risk. \n",
    "\n",
    "Calmar Ratio: Return relative to max drawdown. \n",
    "\n",
    "4. Portfolio Stability & Adaptation \n",
    "\n",
    "Turnover Rate: Measures frequency of asset reallocation. \n",
    "\n",
    "\n",
    "We further split the performance three seperate tables with 1-state process, 2-state process, 3-state process\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(pv, annual_factor=250):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for a given portfolio value series.\n",
    "    \"\"\"\n",
    "    pv = np.array(pv)\n",
    "    rets = np.diff(pv) / pv[:-1]\n",
    "    ann_ret = rets.mean() * annual_factor  # annualized return\n",
    "    cum_ret = pv[-1] / pv[0] - 1  # cumulative return\n",
    "    ann_vol = rets.std() * np.sqrt(annual_factor)  # annualized volatility\n",
    "    sharpe = ann_ret / (ann_vol + 1e-12)  # Sharpe ratio\n",
    "    running_max = np.maximum.accumulate(pv)\n",
    "    drawdown = (pv - running_max) / running_max\n",
    "    max_dd = drawdown.min()  # maximum drawdown\n",
    "    return {\"AnnRet\": ann_ret, \"CumRet\": cum_ret, \"AnnVol\": ann_vol, \"Sharpe\": sharpe, \"MaxDD\": max_dd}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.0 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allocation(df, states=None, scenario_name=\"\"):\n",
    "    # Split data into 80% training and 20% testing.\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    df_train = df.iloc[:split_idx]\n",
    "    df_test = df.iloc[split_idx:]\n",
    "    \n",
    "    # Preprocess training data and transform test data.\n",
    "    clipper = DataClipperStd(mul=3.0)\n",
    "    scaler = StandardScalerPD()\n",
    "    df_train_clipped = clipper.fit_transform(df_train)\n",
    "    df_train_scaled = scaler.fit_transform(df_train_clipped)\n",
    "    X_train = df_train_scaled.values\n",
    "    \n",
    "    df_test_clipped = clipper.transform(df_test)\n",
    "    df_test_scaled = scaler.transform(df_test_clipped)\n",
    "    X_test = df_test_scaled.values\n",
    "    \n",
    "    # Optionally, temporal features can be added here if needed.\n",
    "    \n",
    "    # Train regime models on the training set.\n",
    "    hmm_model, hmm_states_train = train_hmm_kmeans(X_train, n_components=2, random_state=42)\n",
    "    sjm_model = train_sjm(X_train, max_feats=9.0, lam=80.0, n_components=2, random_state=42)\n",
    "    \n",
    "    # Compute regime-specific mean returns for BL views.\n",
    "    def get_regime_means(df_orig, labels):\n",
    "        regs = {}\n",
    "        for lab in np.unique(labels):\n",
    "            idx = np.where(labels == lab)[0]\n",
    "            regs[lab] = df_orig.iloc[idx].mean(axis=0).values\n",
    "        return regs\n",
    "\n",
    "    hmm_mu = get_regime_means(df_train, hmm_states_train)\n",
    "    sjm_mu = get_regime_means(df_train, sjm_model.predict(X_train))\n",
    "    \n",
    "    # Define baseline priors.\n",
    "    prior_mu = df_train.mean(axis=0).values\n",
    "    prior_cov = df_train.cov().values\n",
    "    \n",
    "    # Compute allocations on the test set:\n",
    "    # 1) Equal Weighted\n",
    "    w_ew = np.ones(n_assets) / n_assets\n",
    "    pv_ew = backtest_portfolio(df_test, w_ew)\n",
    "    \n",
    "    # 2) Inverse Volatility\n",
    "    w_iv = inverse_vol_weights(df_test)\n",
    "    pv_iv = backtest_portfolio(df_test, w_iv)\n",
    "    \n",
    "    # 3) Static MVO using PyPortfolioOpt\n",
    "    w_mvo = static_mvo_allocation(df_test, risk_free_rate=0.02)\n",
    "    w_mvo_arr = np.array([w_mvo[asset] for asset in assets])\n",
    "    pv_mvo = backtest_portfolio(df_test, w_mvo_arr)\n",
    "    \n",
    "    # 4) HMM-BL dynamic allocation\n",
    "    hmm_test_states = hmm_model.predict(X_test)\n",
    "    pv_hmmbl, _ = regime_based_bl_backtest(df_test, hmm_test_states, hmm_mu, prior_cov, tau=0.05, risk_free_rate=0.02)\n",
    "    \n",
    "    # 5) SJM-BL dynamic allocation\n",
    "    sjm_test_states = sjm_model.predict(X_test)\n",
    "    pv_sjmbl, _ = regime_based_bl_backtest(df_test, sjm_test_states, sjm_mu, prior_cov, tau=0.05, risk_free_rate=0.02)\n",
    "    \n",
    "    # Collect performance metrics for each strategy.\n",
    "    perf = {\n",
    "        \"EW\": performance_metrics(pv_ew),\n",
    "        \"IV\": performance_metrics(pv_iv),\n",
    "        \"MVO\": performance_metrics(pv_mvo),\n",
    "        \"HMM-BL\": performance_metrics(pv_hmmbl),\n",
    "        \"SJM-BL\": performance_metrics(pv_sjmbl)\n",
    "    }\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenario_1study(T_sim=5000):\n",
    "    \"\"\"\n",
    "    Simulate data for each state process and run allocations.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    df1 = simulate_1state_data(T_sim)\n",
    "    df2, states2 = simulate_2state_data(T_sim)\n",
    "    df3, states3 = simulate_3state_data(T_sim)\n",
    "    \n",
    "    results[\"1state\"] = run_allocation(df1, None, scenario_name=\"1state\")\n",
    "    results[\"2state\"] = run_allocation(df2, states2, scenario_name=\"2state\")\n",
    "    results[\"3state\"] = run_allocation(df3, states3, scenario_name=\"3state\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Main execution: Run simulation and output performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EfficientFrontier.max_sharpe() got an unexpected keyword argument 'solver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     sim_results \u001b[38;5;241m=\u001b[39m run_scenario_1study(T_sim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res \u001b[38;5;129;01min\u001b[39;00m sim_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Performance Metrics ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m, in \u001b[0;36mrun_scenario_1study\u001b[1;34m(T_sim)\u001b[0m\n\u001b[0;32m      7\u001b[0m df2, states2 \u001b[38;5;241m=\u001b[39m simulate_2state_data(T_sim)\n\u001b[0;32m      8\u001b[0m df3, states3 \u001b[38;5;241m=\u001b[39m simulate_3state_data(T_sim)\n\u001b[1;32m---> 10\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_allocation(df1, \u001b[38;5;28;01mNone\u001b[39;00m, scenario_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_allocation(df2, states2, scenario_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_allocation(df3, states3, scenario_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 49\u001b[0m, in \u001b[0;36mrun_allocation\u001b[1;34m(df, states, scenario_name)\u001b[0m\n\u001b[0;32m     46\u001b[0m pv_iv \u001b[38;5;241m=\u001b[39m backtest_portfolio(df_test, w_iv)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 3) Static MVO using PyPortfolioOpt\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m w_mvo \u001b[38;5;241m=\u001b[39m static_mvo_allocation(df_test, risk_free_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m)\n\u001b[0;32m     50\u001b[0m w_mvo_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([w_mvo[asset] \u001b[38;5;28;01mfor\u001b[39;00m asset \u001b[38;5;129;01min\u001b[39;00m assets])\n\u001b[0;32m     51\u001b[0m pv_mvo \u001b[38;5;241m=\u001b[39m backtest_portfolio(df_test, w_mvo_arr)\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mstatic_mvo_allocation\u001b[1;34m(returns, risk_free_rate)\u001b[0m\n\u001b[0;32m      5\u001b[0m ef \u001b[38;5;241m=\u001b[39m EfficientFrontier(mu, S_reg, weight_bounds\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Force a different solver:\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m weights \u001b[38;5;241m=\u001b[39m ef\u001b[38;5;241m.\u001b[39mmax_sharpe(risk_free_rate\u001b[38;5;241m=\u001b[39mrisk_free_rate, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mECOS\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ef\u001b[38;5;241m.\u001b[39mclean_weights()\n",
      "\u001b[1;31mTypeError\u001b[0m: EfficientFrontier.max_sharpe() got an unexpected keyword argument 'solver'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sim_results = run_scenario_1study(T_sim=5000)\n",
    "    for key, res in sim_results.items():\n",
    "        print(f\"=== {key.upper()} Performance Metrics ===\")\n",
    "        print(pd.DataFrame(res).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
