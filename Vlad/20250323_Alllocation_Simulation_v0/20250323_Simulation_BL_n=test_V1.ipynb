{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SJM-BL Simulation Study with Parallel Monte Carlo\n",
    "\n",
    "This code runs multiple Monte Carlo simulations of the 1-state, 2-state, and 3-state processes, computes performance metrics for each run, and then uses a Wilcoxon test to compare SJM-BL against all other strategies.\n",
    "\n",
    "#### 1.0 Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wilcoxon\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Hidden Markov Model utilities\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# PyPortfolioOpt\n",
    "from pypfopt.black_litterman import BlackLittermanModel\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "\n",
    "# Sparse Jump Model utilities\n",
    "from jumpmodels.sparse_jump import SparseJumpModel\n",
    "from jumpmodels.preprocess import StandardScalerPD, DataClipperStd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Simulation\n",
    "\n",
    "#### 2.1 Simulating the 1-state data\n",
    "We are simulating 6 fictional assets which are representing the 6 factors in our framework\n",
    "- **1-State:** A single regime with Student‑t returns.\n",
    "- **2-State:** A two-regime (bull/bear) HMM with state-dependent parameters.\n",
    "- **3-State:** A three-regime HMM with specified means and volatilitie\n",
    "\n",
    "All assets have the same expected return and volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETS = [\"Value\", \"Growth\", \"LowVol\", \"Size\", \"Momentum\", \"Quality\"]\n",
    "N_ASSETS = len(ASSETS)\n",
    "CONST_RET = 0.000461  # Hypothetical constant daily return used\n",
    "RISK_FREE_RATE = 0.02 / 252\n",
    "TRANSACTION_COST = 0.0005\n",
    "BL_TAU = 0.1  # Black-Litterman tau parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_1state_data(num_days, seed=None):\n",
    "    \"\"\"\n",
    "    Simulate a single-state (Student-t) process for all assets.\n",
    "    Each call uses its own RNG to ensure new draws if seed changes.\n",
    "    \"\"\"\n",
    "    local_rng = np.random.default_rng(seed)\n",
    "\n",
    "    mu = 0.000461\n",
    "    sig = 0.008388\n",
    "    dof = 5\n",
    "\n",
    "    corr = np.full((N_ASSETS, N_ASSETS), 0.185)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    cov = np.outer(np.full(N_ASSETS, sig), np.full(N_ASSETS, sig)) * corr\n",
    "\n",
    "    z = local_rng.multivariate_normal(mean=np.zeros(N_ASSETS), cov=cov, size=num_days)\n",
    "    chi = local_rng.chisquare(dof, size=num_days)\n",
    "    factor = np.sqrt(dof / chi)[:, None]  # scaling for t distribution\n",
    "    rets = mu + z * factor\n",
    "    return pd.DataFrame(rets, columns=ASSETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Simulating 2-state data\n",
    "\n",
    "This function simulates a 2-state HMM (bull/bear) with state‐dependent Student‑t returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_2state_data(num_days, seed=None):\n",
    "    \"\"\"\n",
    "    2-state HMM-like simulation. We keep local_rng to vary the data each time.\n",
    "    \"\"\"\n",
    "    local_rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Transition matrix\n",
    "    transmat = np.array([\n",
    "        [0.9976, 0.0024],\n",
    "        [0.0232, 0.9768]\n",
    "    ])\n",
    "    states = np.zeros(num_days, dtype=int)\n",
    "    states[0] = local_rng.integers(2)\n",
    "\n",
    "    for t in range(1, num_days):\n",
    "        states[t] = local_rng.choice(2, p=transmat[states[t - 1]])\n",
    "\n",
    "    mu_dict = {0: 0.0006, 1: -0.000881}\n",
    "    sig_dict = {0: 0.00757, 1: 0.0163}\n",
    "    corr = np.full((N_ASSETS, N_ASSETS), 0.185)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "    rets = np.zeros((num_days, N_ASSETS))\n",
    "    dof = 5\n",
    "    for t in range(num_days):\n",
    "        s = states[t]\n",
    "        mu_s = np.full(N_ASSETS, mu_dict[s])\n",
    "        sig_s = np.full(N_ASSETS, sig_dict[s])\n",
    "        cov_s = np.outer(sig_s, sig_s) * corr\n",
    "\n",
    "        z = local_rng.multivariate_normal(mean=np.zeros(N_ASSETS), cov=cov_s)\n",
    "        chi = local_rng.chisquare(dof)\n",
    "        factor = np.sqrt(dof / chi)\n",
    "        rets[t] = mu_s + factor * z\n",
    "\n",
    "    return pd.DataFrame(rets, columns=ASSETS), states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Simulating 3-state data\n",
    "\n",
    "We are simulating 6 fictional assets which are representing the 6 factors in our framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_3state_data(num_days, seed=None):\n",
    "    \"\"\"\n",
    "    3-state HMM-like simulation. Each call uses local_rng so that different seeds\n",
    "    produce different data.\n",
    "    \"\"\"\n",
    "    local_rng = np.random.default_rng(seed)\n",
    "\n",
    "    transmat = np.array([\n",
    "        [0.9989, 0.0004, 0.0007],\n",
    "        [0.0089, 0.9904, 0.0007],\n",
    "        [0.0089, 0.0004, 0.9907]\n",
    "    ])\n",
    "    states = np.zeros(num_days, dtype=int)\n",
    "    states[0] = local_rng.integers(3)\n",
    "\n",
    "    for t in range(1, num_days):\n",
    "        states[t] = local_rng.choice(3, p=transmat[states[t - 1]])\n",
    "\n",
    "    mu_list = [0.0008, 0.0, -0.003586]\n",
    "    sig_list = [0.0070, 0.0050, 0.01897]\n",
    "    corr = np.full((N_ASSETS, N_ASSETS), 0.185)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "    rets = np.zeros((num_days, N_ASSETS))\n",
    "    dof = 5\n",
    "    for t in range(num_days):\n",
    "        s = states[t]\n",
    "        mu_s = np.full(N_ASSETS, mu_list[s])\n",
    "        sig_s = np.full(N_ASSETS, sig_list[s])\n",
    "        cov_s = np.outer(sig_s, sig_s) * corr\n",
    "\n",
    "        z = local_rng.multivariate_normal(mean=np.zeros(N_ASSETS), cov=cov_s)\n",
    "        chi = local_rng.chisquare(dof)\n",
    "        factor = np.sqrt(dof / chi)\n",
    "        rets[t] = mu_s + factor * z\n",
    "\n",
    "    return pd.DataFrame(rets, columns=ASSETS), states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Training Regime Models\n",
    "\n",
    "#### 3.1 Training HMM using kmeans clustering initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mle(observations, n_components=2, init_type='default', seed=None):\n",
    "    model = GaussianHMM(n_components=n_components, covariance_type='diag',\n",
    "                        n_iter=100, random_state=seed)\n",
    "\n",
    "    if init_type == 'default':\n",
    "        model.startprob_ = np.array([1.0, 0.0])\n",
    "        model.transmat_ = np.array([[0.9, 0.1],\n",
    "                                    [0.1, 0.9]])\n",
    "        model.means_ = np.zeros((n_components, observations.shape[1]))\n",
    "        model.covars_ = np.full((n_components, observations.shape[1]), 1e-2)\n",
    "        # Disable re-initialization of parameters\n",
    "        model.init_params = ''\n",
    "    elif init_type == 'kmeans':\n",
    "        km = KMeans(n_clusters=n_components, n_init=10, random_state=seed)\n",
    "        labels = km.fit_predict(observations)\n",
    "        means, covars = [], []\n",
    "        for i in range(n_components):\n",
    "            obs_i = observations[labels == i]\n",
    "            means.append(np.mean(obs_i, axis=0))\n",
    "            covars.append(np.var(obs_i, axis=0) + 1e-2)\n",
    "        model.startprob_ = np.ones(n_components) / n_components\n",
    "        model.transmat_ = np.ones((n_components, n_components)) / n_components\n",
    "        model.means_ = np.array(means)\n",
    "        model.covars_ = np.array(covars)\n",
    "        model.init_params = ''\n",
    "\n",
    "    model.fit(observations)\n",
    "    pred_states = model.predict(observations)\n",
    "    return model, pred_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mle_default(observations, seed=None):\n",
    "    return run_mle(observations, init_type='default', seed=seed)\n",
    "\n",
    "\n",
    "def run_mle_kmeans(observations, seed=None):\n",
    "    return run_mle(observations, init_type='kmeans', seed=seed)\n",
    "\n",
    "\n",
    "def train_hmm_single_asset_default(series, n_components=2, random_state=42):\n",
    "    X = series.values.reshape(-1, 1)\n",
    "    model, _ = run_mle_default(X, seed=random_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_hmm_single_asset_kmeans(series, n_components=2, random_state=42):\n",
    "    X = series.values.reshape(-1, 1)\n",
    "    model, _ = run_mle_kmeans(X, seed=random_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Training Sparse Jump model with max_feats=9 and lambda=40\n",
    "##### 3.2.1 Defining feature selection framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_features_1d(y, window_len):\n",
    "    T = len(y)\n",
    "    feats = np.zeros((T, 9))\n",
    "    half = (window_len - 1) // 2\n",
    "\n",
    "    for t in range(T):\n",
    "        feats[t, 0] = y[t]  # current\n",
    "        feats[t, 1] = abs(y[t] - y[t - 1]) if t > 0 else 0.0\n",
    "        feats[t, 2] = abs(y[t + 1] - y[t]) if t < T - 1 else 0.0\n",
    "\n",
    "        left_c = max(0, t - half)\n",
    "        right_c = min(T, t + half + 1)\n",
    "        window_c = y[left_c:right_c]\n",
    "        feats[t, 3] = np.mean(window_c)\n",
    "        feats[t, 4] = np.std(window_c)\n",
    "\n",
    "        left_l = max(0, t - window_len)\n",
    "        window_l = y[left_l:t]\n",
    "        feats[t, 5] = np.mean(window_l) if len(window_l) > 0 else 0.0\n",
    "        feats[t, 6] = np.std(window_l) if len(window_l) > 0 else 0.0\n",
    "\n",
    "        window_r = y[t:t + window_len]\n",
    "        feats[t, 7] = np.mean(window_r) if len(window_r) > 0 else 0.0\n",
    "        feats[t, 8] = np.std(window_r) if len(window_r) > 0 else 0.0\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def combine_features_1d(y, window_list=[5, 13]):\n",
    "    feat_list = []\n",
    "    for w in window_list:\n",
    "        feat_list.append(compute_temporal_features_1d(y, w))\n",
    "    return np.hstack(feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sjm_single_asset(series, n_components=2, max_feats=9, lam=40.0, random_state=42):\n",
    "    y = series.values\n",
    "    X_raw = combine_features_1d(y)\n",
    "    clipper = DataClipperStd(mul=3.0)\n",
    "    scaler = StandardScalerPD()\n",
    "    X_clipped = clipper.fit_transform(pd.DataFrame(X_raw))\n",
    "    X_scaled = scaler.fit_transform(X_clipped)\n",
    "    X_arr = X_scaled.values\n",
    "\n",
    "    sjm = SparseJumpModel(\n",
    "        n_components=n_components,\n",
    "        max_feats=max_feats,\n",
    "        jump_penalty=lam,\n",
    "        cont=False,\n",
    "        max_iter=20,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    sjm.fit(X_arr)\n",
    "    return sjm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Allocation simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Allocation workhorse functions\n",
    "In this code we create the in which we fit the following models (each done in a seperate for loop such that we can store the relevant data such as return, weights, etc. in seperate dfs):\n",
    "1. Equal weigted\n",
    "2. Inverse volatility weighted\n",
    "3. Mean-Variance-Optimal static portfolio\n",
    "4. Hidden Markov Model Black Litterman where infered states are the identified regimes\n",
    "5. Sparse Jump Model Black Litterman where infered states are the identified regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_portfolio(returns, weights):\n",
    "    \"\"\"\n",
    "    Given a static weight vector 'weights' and a DataFrame of returns,\n",
    "    compute the portfolio value series over time (starting from 1.0).\n",
    "    \"\"\"\n",
    "    T = len(returns)\n",
    "    portfolio_vals = np.zeros(T)\n",
    "    portfolio_vals[0] = 1.0\n",
    "    for t in range(T - 1):\n",
    "        ret_t = returns.iloc[t].values\n",
    "        portfolio_vals[t + 1] = portfolio_vals[t] * (1.0 + np.dot(weights, ret_t))\n",
    "    return portfolio_vals\n",
    "\n",
    "\n",
    "def equal_weight_allocation(n_assets):\n",
    "    return np.ones(n_assets) / n_assets\n",
    "\n",
    "\n",
    "def inverse_vol_weights(returns):\n",
    "    stds = returns.std(axis=0).values + 1e-12\n",
    "    w = 1.0 / stds\n",
    "    return w / w.sum()\n",
    "\n",
    "\n",
    "def static_mvo_allocation(returns):\n",
    "    \"\"\"\n",
    "    Example static MVO using PyPortfolioOpt (just for illustration).\n",
    "    \"\"\"\n",
    "    from pypfopt import expected_returns\n",
    "    mu = pd.Series(CONST_RET, index=returns.columns)  # constant mu\n",
    "    cov = risk_models.sample_cov(returns)\n",
    "\n",
    "    ef = EfficientFrontier(mu, cov, weight_bounds=(0, 1), solver=\"SCS\")\n",
    "    ef_weights = ef.max_sharpe(risk_free_rate=RISK_FREE_RATE)\n",
    "    return ef.clean_weights()\n",
    "\n",
    "\n",
    "def black_litterman_allocation(view_vector, prior_cov):\n",
    "    \"\"\"\n",
    "    Given a 'view_vector' (dict of {asset: expected return}) and a prior covariance,\n",
    "    run Black-Litterman to obtain final weights.\n",
    "    \"\"\"\n",
    "    pi = pd.Series(CONST_RET, index=prior_cov.columns)\n",
    "    viewdict = {asset: v for asset, v in zip(ASSETS, view_vector)}\n",
    "\n",
    "    bl = BlackLittermanModel(\n",
    "        cov_matrix=prior_cov,\n",
    "        pi=pi,\n",
    "        absolute_views=viewdict,\n",
    "        tau=BL_TAU,\n",
    "        risk_aversion=1\n",
    "    )\n",
    "    bl_rets = bl.bl_returns()  \n",
    "    bl_cov = bl.bl_cov()\n",
    "\n",
    "    ef = EfficientFrontier(bl_rets, bl_cov, weight_bounds=(0, 1), solver=\"SCS\")\n",
    "    if max(bl_rets) <= RISK_FREE_RATE:\n",
    "        ef_weights = ef.min_volatility()\n",
    "    else:\n",
    "        ef_weights = ef.max_sharpe(risk_free_rate=RISK_FREE_RATE)\n",
    "\n",
    "    clean_weights = ef.clean_weights()\n",
    "    w_array = np.array([clean_weights[a] for a in prior_cov.columns])\n",
    "    return w_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Performance metric evaluation:\n",
    "Here we divide the performance metric into. We assume 250 data points to be 1 year off trading:\n",
    "1. Return-Based Metrics \n",
    "\n",
    "Annualized Return: Average return per year. \n",
    "\n",
    "Cumulative Return: Total portfolio growth over time. \n",
    "\n",
    "2. Risk-Based Metrics \n",
    "\n",
    "Volatility: Standard deviation of returns. \n",
    "\n",
    "Downside Deviation: Measures negative return fluctuations. \n",
    "\n",
    "Max Drawdown (MDD): Largest portfolio decline from peak to trough. \n",
    "\n",
    "3. Risk-Adjusted Metrics \n",
    "\n",
    "Sharpe Ratio: Return per unit of total risk. \n",
    "\n",
    "Sortino Ratio: Return per unit of downside risk. \n",
    "\n",
    "Calmar Ratio: Return relative to max drawdown. \n",
    "\n",
    "4. Portfolio Stability & Adaptation \n",
    "\n",
    "Turnover Rate: Measures frequency of asset reallocation. \n",
    "\n",
    "\n",
    "We further split the performance three seperate tables with 1-state process, 2-state process, 3-state process\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_metrics(portfolio_vals, weight_history=None, annual_factor=250):\n",
    "    \"\"\"\n",
    "    Compute standard performance metrics for a given portfolio value series.\n",
    "    \"\"\"\n",
    "    pv = np.asarray(portfolio_vals)\n",
    "    rets = np.diff(pv) / pv[:-1]\n",
    "\n",
    "    ann_ret = rets.mean() * annual_factor\n",
    "    cum_ret = pv[-1] / pv[0] - 1\n",
    "    ann_vol = rets.std() * np.sqrt(annual_factor)\n",
    "\n",
    "    negative_rets = rets[rets < 0]\n",
    "    ddev = negative_rets.std() * np.sqrt(annual_factor) if len(negative_rets) > 0 else 0.0\n",
    "    max_dd = (pv / np.maximum.accumulate(pv) - 1).min()\n",
    "\n",
    "    sharpe = ann_ret / (ann_vol + 1e-12)\n",
    "    sortino = ann_ret / ddev if ddev > 1e-12 else np.nan\n",
    "    calmar = ann_ret / abs(max_dd) if max_dd < 0 else np.nan\n",
    "\n",
    "    if weight_history is not None and len(weight_history) > 1:\n",
    "        turnover_list = []\n",
    "        for t in range(1, len(weight_history)):\n",
    "            turnover_list.append(np.sum(np.abs(weight_history[t] - weight_history[t - 1])))\n",
    "        avg_turnover = np.mean(turnover_list)\n",
    "    else:\n",
    "        avg_turnover = 0.0\n",
    "\n",
    "    return {\n",
    "        \"Annualized Return\": ann_ret,\n",
    "        \"Cumulative Return\": cum_ret,\n",
    "        \"Volatility\": ann_vol,\n",
    "        \"Downside Deviation\": ddev,\n",
    "        \"Max Drawdown\": max_dd,\n",
    "        \"Sharpe Ratio\": sharpe,\n",
    "        \"Sortino Ratio\": sortino,\n",
    "        \"Calmar Ratio\": calmar,\n",
    "        \"Turnover Rate\": avg_turnover,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.0 Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regime_means_single_asset(asset_series, regime_assignments):\n",
    "    \"\"\"\n",
    "    For a single asset series and its assigned states, return a dictionary\n",
    "    of {regime_label: mean_of_that_regime}.\n",
    "    \"\"\"\n",
    "    unique_states = np.unique(regime_assignments)\n",
    "    regime_means = {}\n",
    "    for s in unique_states:\n",
    "        regime_means[s] = asset_series[regime_assignments == s].mean()\n",
    "    return regime_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.0 Regime Based Asset Allocaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regime_based_bl_backtest(df_test, states_test, regime_means_list, prior_cov, train_means_per_asset):\n",
    "    \"\"\"\n",
    "    Daily rebalancing approach:\n",
    "      For each day t, identify the regime for each asset i -> states_test[t,i].\n",
    "      Then use the corresponding regime mean as the 'view' for that asset.\n",
    "      Finally run black-litterman allocation to get daily weights.\n",
    "    \"\"\"\n",
    "    T_test = len(df_test)\n",
    "    portfolio_vals = np.zeros(T_test)\n",
    "    portfolio_vals[0] = 1.0\n",
    "\n",
    "    weight_history = np.zeros((T_test, N_ASSETS))\n",
    "\n",
    "    # Start with some initial weight (e.g. equal)\n",
    "    w_prev = equal_weight_allocation(N_ASSETS)\n",
    "    weight_history[0] = w_prev\n",
    "\n",
    "    # Step through each day\n",
    "    for t in range(T_test - 1):\n",
    "        view_vector = []\n",
    "        for i in range(N_ASSETS):\n",
    "            current_regime = states_test[t, i]\n",
    "            # Just pick the regime-specific mean\n",
    "            view_val = regime_means_list[i][current_regime]\n",
    "            view_vector.append(view_val)\n",
    "\n",
    "        # Generate new weights from BL\n",
    "        w_bl = black_litterman_allocation(view_vector, prior_cov)\n",
    "\n",
    "        # Compute daily growth\n",
    "        ret_t = df_test.iloc[t].values\n",
    "        portfolio_vals[t + 1] = portfolio_vals[t] * (1.0 + np.dot(w_prev, ret_t))\n",
    "\n",
    "        # Store new weight\n",
    "        weight_history[t + 1] = w_bl\n",
    "        w_prev = w_bl\n",
    "\n",
    "    return portfolio_vals, weight_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.0 Wrapper to run full allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allocation(df):\n",
    "    \"\"\"\n",
    "    Splits df into train/test. Trains HMM and SJM per asset.\n",
    "    Then runs the 6 strategies:\n",
    "      1) Equal Weight\n",
    "      2) Inverse Vol\n",
    "      3) Static MVO\n",
    "      4) HMM-BL (Default)\n",
    "      5) HMM-BL (Kmeans)\n",
    "      6) SJM-BL\n",
    "    Returns a dict of performance metrics for each strategy.\n",
    "    \"\"\"\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    df_train = df.iloc[:split_idx]\n",
    "    df_test = df.iloc[split_idx:]\n",
    "    prior_cov = df_train.cov()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Train models per asset\n",
    "    # -------------------------------\n",
    "    hmm_models_default = []\n",
    "    hmm_models_kmeans = []\n",
    "    sjm_models = []\n",
    "\n",
    "    hmm_states_default_train = np.zeros((split_idx, N_ASSETS), dtype=int)\n",
    "    hmm_states_kmeans_train = np.zeros((split_idx, N_ASSETS), dtype=int)\n",
    "    sjm_states_train = np.zeros((split_idx, N_ASSETS), dtype=int)\n",
    "\n",
    "    for i, asset in enumerate(ASSETS):\n",
    "        series_train = df_train[asset]\n",
    "\n",
    "        # 1) HMM default\n",
    "        hmm_mod_default = train_hmm_single_asset_default(series_train)\n",
    "        hmm_mod_states_default = hmm_mod_default.predict(series_train.values.reshape(-1, 1))\n",
    "        hmm_models_default.append(hmm_mod_default)\n",
    "        hmm_states_default_train[:, i] = hmm_mod_states_default\n",
    "\n",
    "        # 2) HMM kmeans\n",
    "        hmm_mod_kmeans = train_hmm_single_asset_kmeans(series_train)\n",
    "        hmm_mod_states_kmeans = hmm_mod_kmeans.predict(series_train.values.reshape(-1, 1))\n",
    "        hmm_models_kmeans.append(hmm_mod_kmeans)\n",
    "        hmm_states_kmeans_train[:, i] = hmm_mod_states_kmeans\n",
    "\n",
    "        # 3) SJM\n",
    "        sjm_mod = train_sjm_single_asset(series_train, n_components=2, lam=80.0)\n",
    "        X_raw = combine_features_1d(series_train.values)\n",
    "        clipper = DataClipperStd(mul=3.0)\n",
    "        scaler = StandardScalerPD()\n",
    "        X_test_clipped = clipper.fit_transform(pd.DataFrame(X_raw))\n",
    "        X_test_scaled = scaler.fit_transform(X_test_clipped)\n",
    "        sjm_states = sjm_mod.predict(X_test_scaled.values)\n",
    "        sjm_models.append(sjm_mod)\n",
    "        sjm_states_train[:, i] = sjm_states\n",
    "\n",
    "    # -------------------------------\n",
    "    # Compute in-sample regime means\n",
    "    # -------------------------------\n",
    "    hmm_regime_means_default = []\n",
    "    hmm_regime_means_kmeans = []\n",
    "    sjm_regime_means = []\n",
    "    train_means_per_asset = []\n",
    "\n",
    "    for i in range(N_ASSETS):\n",
    "        asset_train = df_train.iloc[:, i]\n",
    "        train_means_per_asset.append(asset_train.mean())\n",
    "\n",
    "        hmm_regime_means_default.append(\n",
    "            get_regime_means_single_asset(asset_train, hmm_states_default_train[:, i])\n",
    "        )\n",
    "        hmm_regime_means_kmeans.append(\n",
    "            get_regime_means_single_asset(asset_train, hmm_states_kmeans_train[:, i])\n",
    "        )\n",
    "        sjm_regime_means.append(\n",
    "            get_regime_means_single_asset(asset_train, sjm_states_train[:, i])\n",
    "        )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Predict test states\n",
    "    # -------------------------------\n",
    "    T_test = len(df_test)\n",
    "    hmm_states_default_test = np.zeros((T_test, N_ASSETS), dtype=int)\n",
    "    hmm_states_kmeans_test = np.zeros((T_test, N_ASSETS), dtype=int)\n",
    "    sjm_states_test = np.zeros((T_test, N_ASSETS), dtype=int)\n",
    "\n",
    "    for i, asset in enumerate(ASSETS):\n",
    "        asset_series_test = df_test[asset].values.reshape(-1, 1)\n",
    "        hmm_states_default_test[:, i] = hmm_models_default[i].predict(asset_series_test)\n",
    "        hmm_states_kmeans_test[:, i] = hmm_models_kmeans[i].predict(asset_series_test)\n",
    "\n",
    "        X_test_raw = combine_features_1d(df_test[asset].values)\n",
    "        clipper = DataClipperStd(mul=3.0)\n",
    "        scaler = StandardScalerPD()\n",
    "        X_test_clipped = clipper.fit_transform(pd.DataFrame(X_test_raw))\n",
    "        X_test_scaled = scaler.fit_transform(X_test_clipped)\n",
    "        sjm_states_test[:, i] = sjm_models[i].predict(X_test_scaled.values)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 6 Strategies\n",
    "    # -------------------------------\n",
    "    # 1) Equal-Weight\n",
    "    w_ew = equal_weight_allocation(N_ASSETS)\n",
    "    pv_ew = backtest_portfolio(df_test, w_ew)\n",
    "    w_hist_ew = np.tile(w_ew, (T_test, 1))\n",
    "\n",
    "    # 2) Inverse Vol\n",
    "    w_iv = inverse_vol_weights(df_test)\n",
    "    pv_iv = backtest_portfolio(df_test, w_iv)\n",
    "    w_hist_iv = np.tile(w_iv, (T_test, 1))\n",
    "\n",
    "    # 3) Static MVO\n",
    "    w_mvo_dict = static_mvo_allocation(df_train)\n",
    "    w_mvo_arr = np.array([w_mvo_dict[a] for a in ASSETS])\n",
    "    pv_mvo = backtest_portfolio(df_test, w_mvo_arr)\n",
    "    w_hist_mvo = np.tile(w_mvo_arr, (T_test, 1))\n",
    "\n",
    "    # 4) HMM-BL (Default)\n",
    "    pv_hmmbl_default, w_hmmbl_default = regime_based_bl_backtest(\n",
    "        df_test, hmm_states_default_test,\n",
    "        hmm_regime_means_default,\n",
    "        prior_cov,\n",
    "        train_means_per_asset\n",
    "    )\n",
    "\n",
    "    # 5) HMM-BL (K-Means)\n",
    "    pv_hmmbl_kmeans, w_hmmbl_kmeans = regime_based_bl_backtest(\n",
    "        df_test, hmm_states_kmeans_test,\n",
    "        hmm_regime_means_kmeans,\n",
    "        prior_cov,\n",
    "        train_means_per_asset\n",
    "    )\n",
    "\n",
    "    # 6) SJM-BL\n",
    "    pv_sjmbl, w_sjmbl = regime_based_bl_backtest(\n",
    "        df_test, sjm_states_test,\n",
    "        sjm_regime_means,\n",
    "        prior_cov,\n",
    "        train_means_per_asset\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Performance for each strategy\n",
    "    # -------------------------------\n",
    "    perf = {\n",
    "        \"EW\": compute_performance_metrics(pv_ew, w_hist_ew),\n",
    "        \"IV\": compute_performance_metrics(pv_iv, w_hist_iv),\n",
    "        \"MVO\": compute_performance_metrics(pv_mvo, w_hist_mvo),\n",
    "        \"HMM-BL-Default\": compute_performance_metrics(pv_hmmbl_default, w_hmmbl_default),\n",
    "        \"HMM-BL-KMeans\": compute_performance_metrics(pv_hmmbl_kmeans, w_hmmbl_kmeans),\n",
    "        \"SJM-BL\": compute_performance_metrics(pv_sjmbl, w_sjmbl)\n",
    "    }\n",
    "\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.0 Full scenario: 1-state, 2-state, 3-state runs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenario_123(T_sim=1000, seed1=None, seed2=None, seed3=None):\n",
    "    \"\"\"\n",
    "    Simulate & run 1-state, 2-state, 3-state data sets.\n",
    "    We pass different seeds for each scenario so each is truly random.\n",
    "    \"\"\"\n",
    "    # 1-state\n",
    "    df1_full = simulate_1state_data(T_sim, seed=seed1)\n",
    "    perf_1 = run_allocation(df1_full)\n",
    "\n",
    "    # 2-state\n",
    "    df2_full, _ = simulate_2state_data(T_sim, seed=seed2)\n",
    "    perf_2 = run_allocation(df2_full)\n",
    "\n",
    "    # 3-state\n",
    "    df3_full, _ = simulate_3state_data(T_sim, seed=seed3)\n",
    "    perf_3 = run_allocation(df3_full)\n",
    "\n",
    "    return {\n",
    "        \"1state\": perf_1,\n",
    "        \"2state\": perf_2,\n",
    "        \"3state\": perf_3\n",
    "    }\n",
    "\n",
    "\n",
    "def single_monte_carlo_run(run_id, T_sim=1000):\n",
    "    \"\"\"\n",
    "    A single replication of the full scenario: 1-state, 2-state, and 3-state study.\n",
    "    Each scenario uses a unique seed for variety.\n",
    "    \"\"\"\n",
    "    print(f\"Running simulation {run_id}...\")\n",
    "\n",
    "    # Example: generate 3 different seeds for the 3 scenarios\n",
    "    # so each scenario draws different random data on each run.\n",
    "    seed_for_1state = run_id * 1000 + 11\n",
    "    seed_for_2state = run_id * 1000 + 22\n",
    "    seed_for_3state = run_id * 1000 + 33\n",
    "\n",
    "    results = run_scenario_123(\n",
    "        T_sim=T_sim,\n",
    "        seed1=seed_for_1state,\n",
    "        seed2=seed_for_2state,\n",
    "        seed3=seed_for_3state\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_monte_carlo_study(n_runs=10, T_sim=1000):\n",
    "    \"\"\"\n",
    "    Run n_runs Monte Carlo replications in parallel. Collect all performance metrics\n",
    "    for each scenario & strategy, then do Wilcoxon tests on Sharpe Ratios.\n",
    "    \"\"\"\n",
    "    n_cores = multiprocessing.cpu_count()\n",
    "    print(f\"detected {n_cores} cores\")\n",
    "\n",
    "    # Parallel or single-thread as desired. If you want fewer cores, set n_jobs accordingly.\n",
    "    all_results = Parallel(n_jobs=n_cores)(\n",
    "        delayed(single_monte_carlo_run)(i + 1, T_sim) for i in range(n_runs)\n",
    "    )\n",
    "\n",
    "    # Strategies we compare\n",
    "    strategies = [\"EW\", \"IV\", \"MVO\", \"HMM-BL-Default\", \"HMM-BL-KMeans\", \"SJM-BL\"]\n",
    "    scenarios = [\"1state\", \"2state\", \"3state\"]\n",
    "\n",
    "    # -----------------------------------\n",
    "    #  Collect Sharpe for Wilcoxon\n",
    "    # -----------------------------------\n",
    "    sharpe_data = {sc: {st: [] for st in strategies} for sc in scenarios}\n",
    "\n",
    "    # Also store entire metric distributions for average stats\n",
    "    # We'll do e.g. metric_data[sc][st][\"Annualized Return\"] = [run1, run2, ...]\n",
    "    all_metrics = {}\n",
    "    for sc in scenarios:\n",
    "        all_metrics[sc] = {}\n",
    "        for st in strategies:\n",
    "            all_metrics[sc][st] = {\n",
    "                \"Annualized Return\": [],\n",
    "                \"Cumulative Return\": [],\n",
    "                \"Volatility\": [],\n",
    "                \"Downside Deviation\": [],\n",
    "                \"Max Drawdown\": [],\n",
    "                \"Sharpe Ratio\": [],\n",
    "                \"Sortino Ratio\": [],\n",
    "                \"Calmar Ratio\": [],\n",
    "                \"Turnover Rate\": [],\n",
    "            }\n",
    "\n",
    "    for run_res in all_results:\n",
    "        # run_res is a dict: {\"1state\": {...}, \"2state\": {...}, \"3state\": {...}}\n",
    "        for sc in scenarios:\n",
    "            for st in strategies:\n",
    "                # Pull out the dictionary of metrics for that scenario & strategy\n",
    "                metrics_dict = run_res[sc][st]\n",
    "                # Keep track of Sharpe for Wilcoxon\n",
    "                sharpe_data[sc][st].append(metrics_dict[\"Sharpe Ratio\"])\n",
    "                # Keep track of all metrics\n",
    "                for mkey in all_metrics[sc][st]:\n",
    "                    all_metrics[sc][st][mkey].append(metrics_dict[mkey])\n",
    "\n",
    "    # -----------------------------------\n",
    "    #  Print/Collect Wilcoxon results\n",
    "    # -----------------------------------\n",
    "    print(\"\\n==== Wilcoxon Tests (SJM-BL vs. others, Sharpe Ratio) ====\")\n",
    "    wilcoxon_rows = []\n",
    "    for sc in scenarios:\n",
    "        sjm_sharpes = sharpe_data[sc][\"SJM-BL\"]\n",
    "        for st in strategies:\n",
    "            if st == \"SJM-BL\":\n",
    "                continue\n",
    "            other_sharpes = sharpe_data[sc][st]\n",
    "            stat, pval = wilcoxon(sjm_sharpes, other_sharpes, alternative='two-sided')\n",
    "            print(f\"{sc} | SJM-BL vs {st}: Wilcoxon stat={stat:.4f}, p={pval:.4g}\")\n",
    "\n",
    "            wilcoxon_rows.append({\n",
    "                \"Scenario\": sc,\n",
    "                \"Comparison\": f\"SJM-BL vs {st}\",\n",
    "                \"Statistic\": stat,\n",
    "                \"p-value\": pval\n",
    "            })\n",
    "\n",
    "    df_wilcoxon = pd.DataFrame(wilcoxon_rows)\n",
    "    print(\"\\nWilcoxon Results Table:\")\n",
    "    print(df_wilcoxon.to_string(index=False))\n",
    "\n",
    "    # -----------------------------------\n",
    "    #  Compute & Print Average Metrics\n",
    "    # -----------------------------------\n",
    "    print(\"\\n==== Average Performance Metrics (across all runs) ====\")\n",
    "    for sc in scenarios:\n",
    "        rows = []\n",
    "        for st in strategies:\n",
    "            # compute mean of each metric across runs\n",
    "            metric_means = {}\n",
    "            for mkey, vals in all_metrics[sc][st].items():\n",
    "                metric_means[mkey] = np.mean(vals)\n",
    "            row = {\"Strategy\": st}\n",
    "            row.update(metric_means)\n",
    "            rows.append(row)\n",
    "\n",
    "        df_avg = pd.DataFrame(rows)\n",
    "        df_avg.set_index(\"Strategy\", inplace=True)\n",
    "        print(f\"\\n--- {sc.upper()} ---\")\n",
    "        print(df_avg.to_string())\n",
    "\n",
    "    return sharpe_data, all_metrics, all_results, df_wilcoxon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.0 Main execution: Run simulation and output performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected 8 cores\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example: run 5 replications\n",
    "    n_simulations = 10\n",
    "    T_sim = 5000\n",
    "\n",
    "    # Run parallel simulation\n",
    "    sharpe_data, all_metrics, all_runs, df_wilcoxon = run_monte_carlo_study(\n",
    "        n_runs=n_simulations,\n",
    "        T_sim=T_sim\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.0 Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
