{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM vs kmeans vs Jump vs Sparse Jump Simulation study (Gaussian)\n",
    "\n",
    "This script illustrates how to replicate a simulation study using the \"temporal features\" approach from Nystrup et al. (2020). We simulate data from a 2-state Gaussian HMM (univariate), then transform it into features, and finally apply:\n",
    "1) HMM (from hmmlearn)\n",
    "2) Jump Model\n",
    "3) Sparse Jump Model\n",
    "\n",
    "We'll demonstrate how to tune hyperparameters (penalty parameters, etc.) for the jump-based models.\n",
    "In a real scenario, you may want more robust approaches (e.g., cross-validation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from jumpmodels.sparse_jump import SparseJumpModel    # Sparse JM class\n",
    "from jumpmodels.jump import JumpModel   \n",
    "from scipy import stats\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Simulation & Utility Functions\n",
    "\n",
    "We simulate a univariate 2-state Gaussian HMM. We then define functions for:\n",
    "  - Aligning predicted labels with true labels (Hungarian algorithm).\n",
    "  - Computing balanced accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(T, random_state=None):\n",
    "    \"\"\"\n",
    "    Simulate data from a 2-state Gaussian HMM (univariate).\n",
    "    We'll define the parameters similarly to the \"daily\" scale example:\n",
    "        mu1 = 0.0006, sigma1 = 0.0078\n",
    "        mu2 = -0.0008, sigma2 = 0.0174\n",
    "        transmat = [[0.9629, 0.0371],\n",
    "                    [0.2101, 0.7899]]\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # True parameters\n",
    "    mu1, mu2 = 0.0006, -0.0008\n",
    "    sigma1, sigma2 = 0.0078, 0.0174\n",
    "    transmat = np.array([[0.9629, 0.0371],\n",
    "                         [0.2101, 0.7899]])\n",
    "\n",
    "    # Compute stationary distribution for initialization\n",
    "    eigvals, eigvecs = np.linalg.eig(transmat.T)\n",
    "    stat = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n",
    "    stat = stat[:, 0]\n",
    "    stat = stat / np.sum(stat)\n",
    "\n",
    "    # Generate state sequence\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    states[0] = rng.choice([0, 1], p=stat)\n",
    "    for t in range(1, T):\n",
    "        states[t] = rng.choice([0, 1], p=transmat[states[t - 1]])\n",
    "\n",
    "    # Generate observations from state-dependent Gaussians\n",
    "    y = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        if states[t] == 0:\n",
    "            y[t] = rng.normal(mu1, sigma1)\n",
    "        else:\n",
    "        else:\n",
    "            y[t] = rng.normal(mu2, sigma2)\n",
    "\n",
    "    return y, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Alligning Predicted labels using Hungarian Algorithm"
   ]
  },
  {
   "cell_type": "code",l,
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Align predicted labels with true labels using the Hungarian algorithm.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "    mapping = {col: row for row, col in zip(col_ind, row_ind)}\n",
    "    aligned = np.array([mapping[x] for x in pred_labels])\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Function for calculating the BAC"
   ]
  },
  {
   "cell_type": "code",l,
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bac(true_states, pred_states):\n",
    "    \"\"\"\n",
    "    Compute Balanced Accuracy (BAC) after aligning predicted state labels.\n",
    "    \"\"\"\n",
    "    aligned_pred = align_labels(true_states, pred_states)\n",
    "    return balanced_accuracy_score(true_states, aligned_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Model Wrappers\n",
    "\n",
    "We'll define functions to:\n",
    "  - Fit a standard HMM (hmmlearn)\n",
    "  - Grid search for the best Jump Model penalty\n",
    "  - Grid search for the best Sparse Jump Model penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Fit a GaussianHMM with two different initilizations"
   ]
  },
  {
   "cell_type": "code",l,
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mle(observations, n_components=2, init_type='default'):\n",
    "    \"\"\"\n",
    "    Fit a GaussianHMM to the observations using the EM algorithm.\n",
    "    \n",
    "    Two initialization strategies are supported:\n",
    "    - 'default': Uses fixed starting values (e.g., starting in state 0, with preset transition probabilities,\n",
    "      means, and variances).\n",
    "    - 'kmeans': Uses K-means clustering on the observations to estimate initial means and variances.\n",
    "    \n",
    "    After initialization, the model parameters are updated using the EM algorithm.\n",
    "    \n",
    "    Returns:\n",
    "        model: The fitted GaussianHMM model.\n",
    "        states_est: The decoded state sequence from the model.\n",
    "    \"\"\"\n",
    "    # Create the HMM model object with specified number of states and diagonal covariance matrices\n",
    "    model = GaussianHMM(n_components=n_components, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    \n",
    "    if init_type == 'default':\n",
    "        # Fixed initialization: manually set starting probabilities, transition matrix, means, and variances.\n",
    "        model.startprob_ = np.array([1.0, 0.0])\n",
    "        model.transmat_ = np.array([[0.9, 0.1],\n",
    "                                    [0.1, 0.9]])\n",
    "        model.means_ = np.array([[0.0], [0.0]])\n",
    "        model.covars_ = np.array([[0.01], [0.01]])\n",
    "    elif init_type == 'kmeans':\n",
    "        # Use K-means to estimate initial cluster centers from the 1D observations.\n",
    "        kmeans = KMeans(n_clusters=n_components, random_state=42).fit(observations.reshape(-1, 1))\n",
    "        labels = kmeans.labels_\n",
    "        means = []\n",
    "        covars = []\n",
    "        # Compute the mean and variance for each cluster\n",
    "        for i in range(n_components):\n",
    "            obs_i = observations[labels == i]\n",
    "            means.append(np.mean(obs_i))\n",
    "            var_ = np.var(obs_i) if len(obs_i) > 0 else 0.01\n",
    "            covars.append(var_)\n",
    "        # Set uniform starting probabilities and transition probabilities\n",
    "        model.startprob_ = np.ones(n_components) / n_components\n",
    "        model.transmat_ = np.ones((n_components, n_components)) / n_components\n",
    "        model.means_ = np.array(means).reshape(-1, 1)\n",
    "        model.covars_ = np.array(covars).reshape(-1, 1)\n",
    "    \n",
    "    # Specify which parameters to update: transitions (t), means (m), and covariances (c)\n",
    "    model.init_params = 'tmc'\n",
    "    # Fit the model using the EM algorithm\n",
    "    model.fit(observations.reshape(-1, 1))\n",
    "    # Decode the hidden state sequence using the Viterbi algorithm\n",
    "    logprob, states_est = model.decode(observations.reshape(-1, 1))\n",
    "    return model, states_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fitting a jump model with fixed hyperparameters"
   ]
  },
  {
   "cell_type": "code",l,
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_jump_model(Z, n_components=2, lambda_=100.0, random_state=None):\n",
    "    \"\"\"\n",
    "    Fit a JumpModel to feature matrix Z using a fixed penalty parameter (lambda_).\n",
    "    Returns the predicted state labels.\n",
    "    \"\"\"\n",
    "    model = JumpModel(\n",
    "        n_components=n_components,\n",
    "        jump_penalty=lambda_,\n",
    "        cont=False,       # discrete states\n",
    "        max_iter=10,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(Z)\n",
    "    return model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fitting a sparse jump model with fixed hyperparameters"
   ]
  },
  {
   "cell_type": "code",l,
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sparse_jump_model(Z, n_components=2, lambda_=10.0, max_feats=10, random_state=None):\n",
    "    \"\"\"\n",
    "    Fit a SparseJumpModel to feature matrix Z using fixed penalty (lambda_)\n",
    "    and a fixed max_feats parameter.\n",
    "    Returns the predicted state labels.\n",
    "    \"\"\"\n",
    "    model = SparseJumpModel(\n",
    "        n_components=n_components,\n",
    "        jump_penalty=lambda_,\n",
    "        cont=False,\n",
    "        max_feats=max_feats,\n",
    "        max_iter=10,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(Z)\n",
    "    return model.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Features from Algorithm 2\n",
    "\n",
    "Given a univariate time series `y`, the function will return the following features:\n",
    "\n",
    "1. **Observation:** \\( y[t] \\)  \n",
    "2. **Left absolute change:** \\(\\left| y[t] - y[t-1] \\right|\\)  \n",
    "3. **Right absolute change:** \\(\\left| y[t+1] - y[t] \\right|\\)  \n",
    "4. **Centered local mean:** \\(\\text{mean}(y[t-(l-1)/2 : t+(l-1)/2])\\)  \n",
    "5. **Centered local std**  \n",
    "6. **Left local mean**  \n",
    "7. **Left local std**  \n",
    "8. **Right local mean**  \n",
    "9. **Right local std**"
   ]
  },
  {
   "cell_type": "code",l,
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_features(y, l):\n",
    "    \"\"\"\n",
    "    Compute the 9 features from Algorithm 2 for window length l.\n",
    "    y is assumed 1D, shape (T,).\n",
    "    Returns array of shape (T, 9).\n",
    "    \"\"\"\n",
    "    T = len(y)\n",
    "    feats = np.zeros((T, 9), dtype=float)\n",
    "    \n",
    "    half = (l - 1) // 2\n",
    "    for t in range(T):\n",
    "        # 1) observation\n",
    "        feats[t, 0] = y[t]\n",
    "\n",
    "        # 2) left absolute change\n",
    "        if t > 0:\n",
    "            feats[t, 1] = abs(y[t] - y[t - 1])\n",
    "        else:\n",
    "            feats[t, 1] = 0.0\n",
    "\n",
    "        # 3) right absolute change\n",
    "        if t < T - 1:\n",
    "            feats[t, 2] = abs(y[t + 1] - y[t])\n",
    "        else:\n",
    "            feats[t, 2] = 0.0\n",
    "\n",
    "        # 4) & 5) centered local mean & std\n",
    "        left_c = max(0, t - half)\n",
    "        right_c = min(T, t + half + 1)\n",
    "        window_c = y[left_c:right_c]\n",
    "        feats[t, 3] = np.mean(window_c)\n",
    "        feats[t, 4] = np.std(window_c)\n",
    "\n",
    "        # 6) & 7) left local mean & std\n",
    "        left_l = max(0, t - l)\n",
    "        right_l = t\n",
    "        window_l = y[left_l:right_l]\n",
    "        if len(window_l) > 0:\n",
    "            feats[t, 5] = np.mean(window_l)\n",
    "            feats[t, 6] = np.std(window_l)\n",
    "        else:\n",
    "            feats[t, 5] = 0.0\n",
    "            feats[t, 6] = 0.0\n",
    "\n",
    "        # 8) & 9) right local mean & std\n",
    "        left_r = t\n",
    "        right_r = min(T, t + l)\n",
    "        window_r = y[left_r:right_r]\n",
    "        if len(window_r) > 0:\n",
    "            feats[t, 7] = np.mean(window_r)\n",
    "            feats[t, 8] = np.std(window_r)\n",
    "        else:\n",
    "            feats[t, 7] = 0.0\n",
    "            feats[t, 8] = 0.0\n",
    "\n",
    "    return feats\n",
    "\n",
    "def combine_features(y, l_list=[5, 13]):\n",
    "    \"\"\"\n",
    "    Convenience function: For each window length in l_list,\n",
    "    compute the 9 features and horizontally stack them.\n",
    "    Returns shape (T, 9*len(l_list)).\n",
    "    \"\"\"\n",
    "    feat_list = []\n",
    "    for l in l_list:\n",
    "        f = compute_temporal_features(y, l)\n",
    "        feat_list.append(f)\n",
    "    return np.hstack(feat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Main execution"
   ]
  },
  {
   "cell_type": "code",l,
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the code to see whether this works\n",
    "\n",
    "def run_one_simulation(seed, T=500, lambda_jump=100.0, lambda_sjump=10.0, max_feats=10, init_type='default'):\n",
    "    \"\"\"\n",
    "    Single simulation run:\n",
    "      1) Simulate data (y, true_states)\n",
    "      2) Build features Z\n",
    "      3) Fit MLE (HMM) with chosen initialization\n",
    "      4) Fit JumpModel (fixed lambda)\n",
    "      5) Fit SparseJumpModel (fixed lambda, max_feats)\n",
    "      6) Return Balanced Accuracy for each\n",
    "    \"\"\"\n",
    "    # 1) Simulate\n",
    "    y, true_states = simulate_data(T, random_state=seed)\n",
    "\n",
    "    # 2) Features\n",
    "    Z = combine_features(y, l_list=[5, 13])\n",
    "\n",
    "    # 3) MLE\n",
    "    model_mle, states_est_mle = run_mle(y, n_components=2, init_type=init_type)\n",
    "    bac_mle = calculate_bac(true_states, states_est_mle)  # Corrected function name\n",
    "\n",
    "    # 4) JumpModel\n",
    "    pred_jump = run_jump_model(Z, n_components=2, lambda_=lambda_jump, random_state=seed)\n",
    "    bac_jump = calculate_bac(true_states, pred_jump)  # Corrected function name\n",
    "\n",
    "    # 5) SparseJumpModel\n",
    "    pred_sjump = run_sparse_jump_model(Z, n_components=2, lambda_=lambda_sjump, max_feats=max_feats, random_state=seed)\n",
    "    bac_sjump = calculate_bac(true_states, pred_sjump)  # Corrected function name\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"MLE_BAC\": bac_mle,\n",
    "        \"Jump_BAC\": bac_jump,\n",
    "        \"SparseJump_BAC\": bac_sjump\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",l,
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 30 simulations on 8 cores...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GaussianHMM' is not defined",
     "output_type": "error",the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
     "traceback": [nit_params' contains 'm'\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 'init_params' contains 't'\n",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/mg/4q2pl8qx47b43dbs_tncwchh0000gn/T/ipykernel_11860/1972348863.py\", line 20, in run_one_simulation\n  File \"/var/folders/mg/4q2pl8qx47b43dbs_tncwchh0000gn/T/ipykernel_11860/3631951308.py\", line 17, in run_mle\nNameError: name 'GaussianHMM' is not defined\n\"\"\"",g initialization because 'init_params' contains 't'\n",
      "\nThe above exception was the direct cause of the following exception:\n",nit_params' contains 'm'\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_simulations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m simulations on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m seeds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(n_simulations)\n\u001b[0;32m---> 20\u001b[0m results_list \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_cores)(\n\u001b[1;32m     21\u001b[0m     delayed(run_one_simulation)(\n\u001b[1;32m     22\u001b[0m         seed\u001b[38;5;241m=\u001b[39ms,\n\u001b[1;32m     23\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n\u001b[1;32m     24\u001b[0m         lambda_jump\u001b[38;5;241m=\u001b[39mlambda_jump,\n\u001b[1;32m     25\u001b[0m         lambda_sjump\u001b[38;5;241m=\u001b[39mlambda_sjump,\n\u001b[1;32m     26\u001b[0m         max_feats\u001b[38;5;241m=\u001b[39mmax_feats,\n\u001b[1;32m     27\u001b[0m         init_type\u001b[38;5;241m=\u001b[39mhmm_init\n\u001b[1;32m     28\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m seeds\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Convert results to DataFrame\u001b[39;00m\n\u001b[1;32m     32\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results_list)\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m-> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",during initialization because 'init_params' contains 'm'\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GaussianHMM' is not defined""Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
     ] "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
    } "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n",
   ],ough the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n",
   "source": [en though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n",
    "\n",en though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n",
    "\n",bute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n",
    "# Runnign the full simulation\n",ibute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n",
    "if __name__ == \"__main__\":\n",set, it will be overwritten during initialization because 'init_params' contains 'c'\n",
    "    # Adjust simulation settings here\n",he 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n",
    "    T = 500\n",  Current: 1623.8853499822849 is not greater than 1624.087511312289. Delta is -0.20216133000417358\n",
    "    n_simulations = 30\n",del is not converging.  Current: 1628.0074585322157 is not greater than 1628.1870381907513. Delta is -0.17957965853565838\n",
    "\n",86 is not greater than 1594.642742519587. Delta is -0.029528248828455617\n",
    "    # Fixed hyperparameters for jump-based models\n", Current: 1621.2445636043185 is not greater than 1621.2732894512405. Delta is -0.028725846922043274\n"
    "    lambda_jump = 100.0\n",
    "    lambda_sjump = 10.0\n",
    "    max_feats = 10\n",
    "\n",
    "    # HMM initialization type ('default' or 'kmeans')\n",' is not defined",
    "    hmm_init = 'default'\n",put_type": "error",
    "\n",
    "    # Parallel simulation\n",------------------------------------------\u001b[0m",
    "    num_cores = multiprocessing.cpu_count()\n",t recent call last)",
    "    print(f\"Running {n_simulations} simulations on {num_cores} cores...\")\n",001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/mg/4q2pl8qx47b43dbs_tncwchh0000gn/T/ipykernel_11860/1972348863.py\", line 21, in run_one_simulation\nNameError: name 'compute_bac' is not defined\n\"\"\"",
    "\n",ause of the following exception:\n",
    "    seeds = np.arange(n_simulations)\n",               Traceback (most recent call last)",
    "    results_list = Parallel(n_jobs=num_cores)(\n",[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_simulations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m simulations on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m seeds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(n_simulations)\n\u001b[0;32m---> 20\u001b[0m results_list \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_cores)(\n\u001b[1;32m     21\u001b[0m     delayed(run_one_simulation)(\n\u001b[1;32m     22\u001b[0m         seed\u001b[38;5;241m=\u001b[39ms,\n\u001b[1;32m     23\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n\u001b[1;32m     24\u001b[0m         lambda_jump\u001b[38;5;241m=\u001b[39mlambda_jump,\n\u001b[1;32m     25\u001b[0m         lambda_sjump\u001b[38;5;241m=\u001b[39mlambda_sjump,\n\u001b[1;32m     26\u001b[0m         max_feats\u001b[38;5;241m=\u001b[39mmax_feats,\n\u001b[1;32m     27\u001b[0m         init_type\u001b[38;5;241m=\u001b[39mhmm_init\n\u001b[1;32m     28\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m seeds\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Convert results to DataFrame\u001b[39;00m\n\u001b[1;32m     32\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results_list)\n",
    "        delayed(run_one_simulation)(\n",sktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
    "            seed=s,\n",/Desktop/git/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
    "            T=T,\n",a3/lib/python3.12/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
    "            lambda_jump=lambda_jump,\n",/lib/python3.12/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
    "            lambda_sjump=lambda_sjump,\n",conda3/lib/python3.12/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
    "            max_feats=max_feats,\n",naconda3/lib/python3.12/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
    "            init_type=hmm_init\n",b[0m: name 'compute_bac' is not defined"
    "        ) for s in seeds\n",
    "    )\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "\n",
    "    # Summaries\n",
    "    mle_mean = df_results[\"MLE_BAC\"].mean()\n",
    "    mle_std = df_results[\"MLE_BAC\"].std()\n",# Adjust simulation settings here\n",
    "\n",
    "    jump_mean = df_results[\"Jump_BAC\"].mean()\n",
    "    jump_std = df_results[\"Jump_BAC\"].std()\n",
    "\n",
    "    sjump_mean = df_results[\"SparseJump_BAC\"].mean()\n",
    "    sjump_std = df_results[\"SparseJump_BAC\"].std()\n",lambda_sjump = 32.0\n",
    "\n",
    "    print(\"\\n=== Results Summary ===\")\n",
    "    print(f\"HMM (MLE)    : {mle_mean:.3f} ± {mle_std:.3f}\")\n",
    "    print(f\"Jump Model   : {jump_mean:.3f} ± {jump_std:.3f}\")\n",
    "    print(f\"Sparse Jump  : {sjump_mean:.3f} ± {sjump_std:.3f}\")\n",
    "\n",
    "    print(\"\\nSample of raw results:\")\n",.cpu_count()\n",
    "    print(df_results.head(10))""    print(f\"Running {n_simulations} simulations on {num_cores} cores...\")\n",
   ]"\n",
  }, "    seeds = np.arange(n_simulations)\n",
  { Parallel(n_jobs=num_cores)(\n",
   "cell_type": "code",e_simulation)(\n",
   "execution_count": null,eed=s,\n",
   "metadata": {},T=T,\n",
   "outputs": [],  lambda_jump=lambda_jump,\n",
   "source": [] "            lambda_sjump=lambda_sjump,\n",
  } "            max_feats=max_feats,\n",
 ],   init_type=hmm_init\n",
 "metadata": {r s in seeds\n",
  "kernelspec": {
   "display_name": "base",
   "language": "python",esults to DataFrame\n",
   "name": "python3""    df_results = pd.DataFrame(results_list)\n",
  },
  "language_info": {,
   "codemirror_mode": {_results[\"MLE_BAC\"].mean()\n",
    "name": "ipython", = df_results[\"MLE_BAC\"].std()\n",
    "version": 3\n",
   },ults[\"Jump_BAC\"].mean()\n",
   "file_extension": ".py",\"Jump_BAC\"].std()\n",
   "mimetype": "text/x-python",
   "name": "python","SparseJump_BAC\"].mean()\n",
   "nbconvert_exporter": "python",\"SparseJump_BAC\"].std()\n",
   "pygments_lexer": "ipython3",
   "version": "3.12.2" "    print(\"\\n=== Results Summary ===\")\n",
  } "    print(f\"HMM (MLE)    : {mle_mean:.3f} ± {mle_std:.3f}\")\n",
 },f\"Jump Model   : {jump_mean:.3f} ± {jump_std:.3f}\")\n",
 "nbformat": 4,arse Jump  : {sjump_mean:.3f} ± {sjump_std:.3f}\")\n",
 "nbformat_minor": 2   "\n",
}    "    print(\"\\nSample of raw results:\")\n",

    "    print(df_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
