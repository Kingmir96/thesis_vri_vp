{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e1539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running simulations for lambda = 10...\n",
      "\n",
      "Strategy: EW (lambda = 10)\n",
      "Annualized Return: Mean = 0.1210, Std = 0.0462\n",
      "Cumulative Return: Mean = 0.6283, Std = 0.2900\n",
      "Volatility: Mean = 0.0788, Std = 0.0044\n",
      "Downside Deviation: Mean = 0.0472, Std = 0.0043\n",
      "Max Drawdown: Mean = -0.0898, Std = 0.0438\n",
      "Sharpe Ratio: Mean = 1.5536, Std = 0.6207\n",
      "Sortino Ratio: Mean = 2.6409, Std = 1.1181\n",
      "Calmar Ratio: Mean = 1.7174, Std = 0.9165\n",
      "Turnover Rate: Mean = 0.0000, Std = 0.0000\n",
      "\n",
      "Strategy: SJM-BL (lambda = 10)\n",
      "Annualized Return: Mean = 0.1384, Std = 0.0472\n",
      "Cumulative Return: Mean = 0.7452, Std = 0.3026\n",
      "Volatility: Mean = 0.0799, Std = 0.0048\n",
      "Downside Deviation: Mean = 0.0478, Std = 0.0042\n",
      "Max Drawdown: Mean = -0.0758, Std = 0.0226\n",
      "Sharpe Ratio: Mean = 1.7473, Std = 0.6169\n",
      "Sortino Ratio: Mean = 2.9680, Std = 1.1061\n",
      "Calmar Ratio: Mean = 2.0474, Std = 0.9007\n",
      "Turnover Rate: Mean = 0.0037, Std = 0.0011\n",
      "\n",
      "Running simulations for lambda = 25...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 501\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lam \u001b[38;5;129;01min\u001b[39;00m lambda_grid:\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning simulations for lambda = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 501\u001b[0m     metrics, _ \u001b[38;5;241m=\u001b[39m run_monte_carlo_for_lambda(\n\u001b[0;32m    502\u001b[0m         n_runs\u001b[38;5;241m=\u001b[39mn_simulations,\n\u001b[0;32m    503\u001b[0m         T_sim\u001b[38;5;241m=\u001b[39mT_sim,\n\u001b[0;32m    504\u001b[0m         lam_sjm\u001b[38;5;241m=\u001b[39mlam\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    506\u001b[0m     summary_results[lam] \u001b[38;5;241m=\u001b[39m metrics\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# Print summary stats\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 468\u001b[0m, in \u001b[0;36mrun_monte_carlo_for_lambda\u001b[1;34m(n_runs, T_sim, lam_sjm, risk_free_rate, transaction_cost, bl_tau)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03mRuns multiple simulations in parallel for a given lambda,\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03maggregates results.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Use up to 6 cores\u001b[39;00m\n\u001b[1;32m--> 468\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)(\n\u001b[0;32m    469\u001b[0m     delayed(single_simulation_run)(\n\u001b[0;32m    470\u001b[0m         i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, T_sim, lam_sjm, risk_free_rate, transaction_cost, bl_tau\n\u001b[0;32m    471\u001b[0m     )\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_runs)\n\u001b[0;32m    473\u001b[0m )\n\u001b[0;32m    475\u001b[0m strategies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEW\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSJM-BL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    476\u001b[0m metric_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnualized Return\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCumulative Return\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVolatility\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownside Deviation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Drawdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSharpe Ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSortino Ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalmar Ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTurnover Rate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m ]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# PyPortfolioOpt\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt.black_litterman import BlackLittermanModel\n",
    "\n",
    "# For the SJM (Sparse Jump Model)\n",
    "from jumpmodels.sparse_jump import SparseJumpModel\n",
    "from jumpmodels.preprocess import StandardScalerPD, DataClipperStd\n",
    "\n",
    "ASSETS = [\"Value\", \"Growth\", \"LowVol\", \"Size\", \"Momentum\", \"Quality\"]\n",
    "N_ASSETS = len(ASSETS)\n",
    "\n",
    "# --- 1. Data simulation (2-state only) ---\n",
    "def simulate_2state_data(num_days, seed=None):\n",
    "    \"\"\"\n",
    "    Simulates a returns DataFrame with shape (num_days, N_ASSETS),\n",
    "    plus a separate matrix of hidden states. Returns (DataFrame, states).\n",
    "    \"\"\"\n",
    "    np_rng = np.random.default_rng(seed)\n",
    "    transmat = np.array([\n",
    "        [0.9976, 0.0024],\n",
    "        [0.0232, 0.9768]\n",
    "    ])\n",
    "    mu_dict  = {0: 0.0006,   1: -0.000881}\n",
    "    sig_dict = {0: 0.00757,  1: 0.0163}\n",
    "    base_corr = np.full((N_ASSETS, N_ASSETS), 0.185)\n",
    "    np.fill_diagonal(base_corr, 1.0)\n",
    "\n",
    "    # Edge check\n",
    "    if num_days < 1:\n",
    "        raise ValueError(\"num_days must be >= 1.\")\n",
    "\n",
    "    # Simulate each asset's hidden states\n",
    "    all_states = np.zeros((num_days, N_ASSETS), dtype=int)\n",
    "    for i in range(N_ASSETS):\n",
    "        s = np.zeros(num_days, dtype=int)\n",
    "        s[0] = np_rng.integers(2)\n",
    "        for t in range(1, num_days):\n",
    "            s[t] = np_rng.choice(2, p=transmat[s[t - 1]])\n",
    "        all_states[:, i] = s\n",
    "\n",
    "    # Generate returns\n",
    "    rets = np.zeros((num_days, N_ASSETS))\n",
    "    for t in range(num_days):\n",
    "        mu_vec  = np.zeros(N_ASSETS)\n",
    "        sig_vec = np.zeros(N_ASSETS)\n",
    "        for i in range(N_ASSETS):\n",
    "            curr_state = all_states[t, i]\n",
    "            mu_vec[i]  = mu_dict[curr_state]\n",
    "            sig_vec[i] = sig_dict[curr_state]\n",
    "        cov_t = np.outer(sig_vec, sig_vec) * base_corr\n",
    "        rets[t] = np_rng.multivariate_normal(mean=mu_vec, cov=cov_t)\n",
    "\n",
    "    return pd.DataFrame(rets, columns=ASSETS), all_states\n",
    "\n",
    "\n",
    "# --- 2. SJM functions ---\n",
    "def compute_sjm_features(factor_ser: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create SJM features (downside dev, differences of dev, etc.).\n",
    "    \"\"\"\n",
    "    def ewm_downside_dev(returns: pd.Series, halflife: float) -> pd.Series:\n",
    "        negative_returns = returns.clip(upper=0)\n",
    "        neg_sq = negative_returns ** 2\n",
    "        ewm_mean = neg_sq.ewm(halflife=halflife, adjust=False).mean()\n",
    "        return np.sqrt(ewm_mean)\n",
    "\n",
    "    dd20 = ewm_downside_dev(factor_ser, halflife=20)\n",
    "    dd60 = ewm_downside_dev(factor_ser, halflife=60)\n",
    "    dd120 = ewm_downside_dev(factor_ser, halflife=120)\n",
    "\n",
    "    feats = {\n",
    "        \"DD_hl20\": dd20,\n",
    "        \"DD20_minus_DD60\": dd20 - dd60,\n",
    "        \"DD60_minus_DD120\": dd60 - dd120,\n",
    "        \"Return_hl120\": factor_ser.ewm(halflife=120, adjust=False).mean(),\n",
    "    }\n",
    "    df_feats = pd.DataFrame(feats).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    return df_feats\n",
    "\n",
    "def train_sjm_single_asset(series, n_components=2, max_feats=4, lam=50, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains a SparseJumpModel on one asset's returns and returns\n",
    "    the fitted model plus the transformations.\n",
    "    \"\"\"\n",
    "    # Quick check to ensure enough data\n",
    "    if len(series) < 2:\n",
    "        raise ValueError(\"Not enough data points to train the SJM on a single asset.\")\n",
    "\n",
    "    feats_df = compute_sjm_features(series)\n",
    "    clipper = DataClipperStd(mul=3.0)\n",
    "    scaler  = StandardScalerPD()\n",
    "    X_clipped = clipper.fit_transform(feats_df)\n",
    "    X_scaled  = scaler.fit_transform(X_clipped)\n",
    "    X_arr = X_scaled.values\n",
    "\n",
    "    sjm = SparseJumpModel(\n",
    "        n_components=n_components,\n",
    "        max_feats=max_feats,\n",
    "        jump_penalty=lam,\n",
    "        cont=False,\n",
    "        max_iter=20,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    sjm.fit(X_arr)\n",
    "    return sjm, clipper, scaler\n",
    "\n",
    "def get_regime_means_stds_single_asset(asset_series, regime_assignments):\n",
    "    \"\"\"\n",
    "    Computes the mean and std for each regime in 'regime_assignments'.\n",
    "    \"\"\"\n",
    "    unique_states = np.unique(regime_assignments)\n",
    "    regime_means = {}\n",
    "    regime_stds  = {}\n",
    "    for s in unique_states:\n",
    "        data_in_s = asset_series[regime_assignments == s]\n",
    "        if len(data_in_s) > 0:\n",
    "            regime_means[s] = data_in_s.mean()\n",
    "            regime_stds[s]  = data_in_s.std()\n",
    "        else:\n",
    "            # Fallback if no points in that regime\n",
    "            regime_means[s] = asset_series.mean()\n",
    "            regime_stds[s]  = asset_series.std()\n",
    "    return regime_means, regime_stds\n",
    "\n",
    "\n",
    "# --- 3. Black-Litterman helper functions ---\n",
    "def build_equal_unconditional_prior(df_train):\n",
    "    \"\"\"\n",
    "    Returns a simple unconditional prior vector and covariance matrix\n",
    "    based on the single-state simulation assumptions.\n",
    "    \"\"\"\n",
    "    SIM_MEAN_1STATE = 0.000461\n",
    "    SIM_SIG_1STATE  = 0.008388\n",
    "    TRUE_CORR       = 0.185\n",
    "\n",
    "    n_assets = df_train.shape[1]\n",
    "    uniform_corr = np.full((n_assets, n_assets), TRUE_CORR)\n",
    "    np.fill_diagonal(uniform_corr, 1.0)\n",
    "    cov_flat = (SIM_SIG_1STATE**2) * uniform_corr\n",
    "\n",
    "    assets = df_train.columns\n",
    "    pi_series = pd.Series(np.full(n_assets, SIM_MEAN_1STATE), index=assets)\n",
    "    cov_df = pd.DataFrame(cov_flat, index=assets, columns=assets)\n",
    "    return pi_series, cov_df\n",
    "\n",
    "def get_rolling_cov(full_returns, current_index, halflife=126):\n",
    "    \"\"\"\n",
    "    Returns an exponentially weighted covariance matrix up to row 'current_index'.\n",
    "    \"\"\"\n",
    "    sub_df = full_returns.iloc[:current_index]\n",
    "    if len(sub_df) < 2:\n",
    "        # Fallback to identity if too little data\n",
    "        n_ = sub_df.shape[1]\n",
    "        return np.eye(n_)\n",
    "    span_equiv = (2 / (1 - np.exp(-np.log(2) / halflife))) - 1\n",
    "    cov_est = risk_models.exp_cov(sub_df, span=span_equiv, returns_data=True)\n",
    "    return cov_est\n",
    "\n",
    "def regime_based_bl_backtest_flatprior(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    states_test,\n",
    "    init_state,\n",
    "    regime_means_list,\n",
    "    transaction_cost=0.0007,\n",
    "    risk_free_rate=0.02/252,\n",
    "    bl_tau=0.05,\n",
    "    halflife=126\n",
    "):\n",
    "    \"\"\"\n",
    "    A BL backtest that rebalances only when regime changes.\n",
    "    \"\"\"\n",
    "    T_test = len(df_test)\n",
    "    assets = df_test.columns\n",
    "    n_assets = len(assets)\n",
    "    combined_df = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Build an unconditional prior (not used heavily except for structure)\n",
    "    flat_pi, _ = build_equal_unconditional_prior(df_train)\n",
    "\n",
    "    portfolio_vals = np.zeros(T_test)\n",
    "    if T_test == 0:\n",
    "        # If no test data, just return empty arrays\n",
    "        return portfolio_vals, np.zeros((0, n_assets)), []\n",
    "\n",
    "    portfolio_vals[0] = 1.0\n",
    "    weight_history = np.zeros((T_test, n_assets))\n",
    "\n",
    "    # Start with equal weights\n",
    "    w_prev = np.ones(n_assets) / n_assets\n",
    "    weight_history[0] = w_prev\n",
    "\n",
    "    fallback_records = []\n",
    "\n",
    "    for t in range(1, T_test):\n",
    "        # Growth from previous day\n",
    "        ret_t_minus_1 = df_test.iloc[t - 1].values\n",
    "        gross_growth = portfolio_vals[t - 1] * (1.0 + np.dot(w_prev, ret_t_minus_1))\n",
    "\n",
    "        # Check regime change\n",
    "        if t == 1:\n",
    "            current_states = init_state\n",
    "            do_rebalance = True\n",
    "        else:\n",
    "            current_states = states_test[t - 1]\n",
    "            do_rebalance = np.any(states_test[t - 1] != states_test[t - 2])\n",
    "\n",
    "        if do_rebalance:\n",
    "            # Build absolute views from regime_means\n",
    "            view_vector = np.zeros(n_assets)\n",
    "            for i in range(n_assets):\n",
    "                view_vector[i] = regime_means_list[i].get(current_states[i], 0.0)\n",
    "\n",
    "            # Rolling cov up to (train + t)\n",
    "            global_index = len(df_train) + t\n",
    "            cov_t = get_rolling_cov(combined_df, global_index, halflife=halflife)\n",
    "\n",
    "            # Build the BL model\n",
    "            bl = BlackLittermanModel(\n",
    "                cov_matrix=cov_t,\n",
    "                pi=\"equal\",\n",
    "                absolute_views=dict(zip(assets, view_vector)),\n",
    "                tau=bl_tau,\n",
    "                risk_aversion=2.5\n",
    "            )\n",
    "            # Posterior returns\n",
    "            bl_rets = bl.bl_returns()\n",
    "\n",
    "            # Solve the EF with max_sharpe\n",
    "            ef = EfficientFrontier(bl_rets, cov_t, weight_bounds=(0, 1), solver=\"SCS\")\n",
    "            try:\n",
    "                w_dict = ef.max_sharpe(risk_free_rate=risk_free_rate)\n",
    "            except ValueError as e:\n",
    "                # Fallback to equal-weight if solver fails\n",
    "                fallback_records.append({\n",
    "                    \"time\": t,\n",
    "                    \"predicted_regime\": current_states.copy(),\n",
    "                    \"absolute_views\": dict(zip(assets, view_vector)),\n",
    "                    \"posterior_views\": bl_rets.copy() if isinstance(bl_rets, np.ndarray) else bl_rets,\n",
    "                })\n",
    "                w_dict = {a: 1.0 / n_assets for a in assets}\n",
    "            w_array = np.array([w_dict[a] for a in assets])\n",
    "        else:\n",
    "            w_array = w_prev.copy()\n",
    "\n",
    "        # Trading cost\n",
    "        traded_fraction = np.sum(np.abs(w_array - w_prev))\n",
    "        cost = gross_growth * traded_fraction * transaction_cost\n",
    "\n",
    "        # New portfolio value\n",
    "        portfolio_vals[t] = gross_growth - cost\n",
    "        weight_history[t] = w_array\n",
    "        w_prev = w_array\n",
    "\n",
    "    return portfolio_vals, weight_history, fallback_records\n",
    "\n",
    "\n",
    "# --- 4. Backtesting & performance ---\n",
    "def backtest_portfolio(returns, weights, transaction_cost=0.0007):\n",
    "    \"\"\"\n",
    "    Simple hold & rebal once at t=0. Deduct cost on initial notional only.\n",
    "    \"\"\"\n",
    "    T = len(returns)\n",
    "    if T == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    portfolio_vals = np.zeros(T)\n",
    "    # Initial cost\n",
    "    cost_init = np.sum(np.abs(weights)) * transaction_cost\n",
    "    portfolio_vals[0] = 1.0 - cost_init\n",
    "\n",
    "    for t in range(T - 1):\n",
    "        ret_t = returns.iloc[t].values\n",
    "        portfolio_vals[t + 1] = portfolio_vals[t] * (1.0 + np.dot(weights, ret_t))\n",
    "\n",
    "    return portfolio_vals\n",
    "\n",
    "def compute_performance_metrics(portfolio_vals, weight_history=None, annual_factor=250):\n",
    "    \"\"\"\n",
    "    Calculates standard portfolio metrics (Sharpe, Sortino, etc.).\n",
    "    \"\"\"\n",
    "    pv = np.asarray(portfolio_vals)\n",
    "    if len(pv) < 2:\n",
    "        # Not enough data points to compute real metrics\n",
    "        return {\n",
    "            \"Annualized Return\": np.nan,\n",
    "            \"Cumulative Return\": np.nan,\n",
    "            \"Volatility\": np.nan,\n",
    "            \"Downside Deviation\": np.nan,\n",
    "            \"Max Drawdown\": np.nan,\n",
    "            \"Sharpe Ratio\": np.nan,\n",
    "            \"Sortino Ratio\": np.nan,\n",
    "            \"Calmar Ratio\": np.nan,\n",
    "            \"Turnover Rate\": np.nan,\n",
    "        }\n",
    "\n",
    "    rets = np.diff(pv) / pv[:-1]\n",
    "    ann_ret = rets.mean() * annual_factor\n",
    "    cum_ret = pv[-1] / pv[0] - 1\n",
    "    ann_vol = rets.std() * np.sqrt(annual_factor)\n",
    "\n",
    "    negative_rets = rets[rets < 0]\n",
    "    ddev = (negative_rets.std() * np.sqrt(annual_factor)) if len(negative_rets) > 0 else 0.0\n",
    "\n",
    "    max_dd = (pv / np.maximum.accumulate(pv) - 1).min()\n",
    "    sharpe = ann_ret / (ann_vol + 1e-12)\n",
    "    sortino = ann_ret / ddev if ddev > 1e-12 else np.nan\n",
    "    calmar  = ann_ret / abs(max_dd) if max_dd < 0 else np.nan\n",
    "\n",
    "    # Average turnover\n",
    "    if weight_history is not None and len(weight_history) > 1:\n",
    "        turnovers = []\n",
    "        for t in range(1, len(weight_history)):\n",
    "            turnovers.append(np.sum(np.abs(weight_history[t] - weight_history[t-1])))\n",
    "        avg_turnover = np.mean(turnovers)\n",
    "    else:\n",
    "        avg_turnover = 0.0\n",
    "\n",
    "    return {\n",
    "        \"Annualized Return\": ann_ret,\n",
    "        \"Cumulative Return\": cum_ret,\n",
    "        \"Volatility\": ann_vol,\n",
    "        \"Downside Deviation\": ddev,\n",
    "        \"Max Drawdown\": max_dd,\n",
    "        \"Sharpe Ratio\": sharpe,\n",
    "        \"Sortino Ratio\": sortino,\n",
    "        \"Calmar Ratio\": calmar,\n",
    "        \"Turnover Rate\": avg_turnover,\n",
    "    }\n",
    "\n",
    "def equal_weight_allocation(n_assets):\n",
    "    \"\"\" Returns an equal-weight allocation vector. \"\"\"\n",
    "    return np.ones(n_assets) / n_assets\n",
    "\n",
    "\n",
    "# --- 5. Allocation simulation using only EW and SJM-BL ---\n",
    "def run_allocation_simulation(\n",
    "    df,\n",
    "    lam_sjm=50,\n",
    "    risk_free_rate=0.02/252,\n",
    "    transaction_cost=0.0007,\n",
    "    bl_tau=0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits df 80/20, trains SJM, predicts states, then runs two strategies:\n",
    "    1) Equal-weight\n",
    "    2) SJM-BL\n",
    "    Returns performance dict and fallback events.\n",
    "    \"\"\"\n",
    "    # 1) Check the columns\n",
    "    missing_cols = [col for col in ASSETS if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"DataFrame is missing required columns: {missing_cols}\")\n",
    "\n",
    "    # 2) Split check\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    if split_idx < 1 or (len(df) - split_idx) < 1:\n",
    "        raise ValueError(\"Not enough data in df for an 80/20 train-test split.\")\n",
    "\n",
    "    df_train = df.iloc[:split_idx]\n",
    "    df_test  = df.iloc[split_idx:]\n",
    "    T_test = len(df_test)\n",
    "\n",
    "    # Train SJM for each asset\n",
    "    sjm_models = []\n",
    "    sjm_clippers = []\n",
    "    sjm_scalers = []\n",
    "    sjm_states_train = np.zeros((split_idx, N_ASSETS), dtype=int)\n",
    "    for i, asset in enumerate(ASSETS):\n",
    "        series_train = df_train[asset]\n",
    "        sjm_mod, sjm_clip, sjm_scale = train_sjm_single_asset(\n",
    "            series_train,\n",
    "            n_components=2,\n",
    "            max_feats=4,\n",
    "            lam=lam_sjm\n",
    "        )\n",
    "        feats_train = compute_sjm_features(series_train)\n",
    "        feats_train = feats_train.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "        X_train_clip = sjm_clip.transform(feats_train)\n",
    "        X_train_scl  = sjm_scale.transform(X_train_clip)\n",
    "        st_sjm = sjm_mod.predict(X_train_scl)\n",
    "\n",
    "        sjm_models.append(sjm_mod)\n",
    "        sjm_clippers.append(sjm_clip)\n",
    "        sjm_scalers.append(sjm_scale)\n",
    "        sjm_states_train[:, i] = st_sjm\n",
    "\n",
    "    # Compute regime means from training data\n",
    "    sjm_regime_means = []\n",
    "    for i in range(N_ASSETS):\n",
    "        asset_train = df_train.iloc[:, i]\n",
    "        m_sjm, _ = get_regime_means_stds_single_asset(asset_train, sjm_states_train[:, i])\n",
    "        sjm_regime_means.append(m_sjm)\n",
    "\n",
    "    # Predict SJM states for the test period (using an expanding window approach)\n",
    "    sjm_states_test = np.zeros((T_test, N_ASSETS), dtype=int)\n",
    "    full_series = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "    for i, asset in enumerate(ASSETS):\n",
    "        asset_full = full_series[asset]\n",
    "        feats_full = compute_sjm_features(asset_full)\n",
    "        feats_full = feats_full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "        X_full_clip = sjm_clippers[i].transform(feats_full)\n",
    "        X_full_scl  = sjm_scalers[i].transform(X_full_clip)\n",
    "\n",
    "        for t in range(T_test):\n",
    "            end_idx = split_idx + t + 1  # end index in the full data\n",
    "            partial_X = X_full_scl[:end_idx]\n",
    "            partial_states = sjm_models[i].predict(partial_X)\n",
    "            # Use the last predicted state for time t without negative indexing\n",
    "            last_index = len(partial_states) - 1\n",
    "            sjm_states_test[t, i] = partial_states[last_index]\n",
    "\n",
    "    # 1) Equal Weight strategy\n",
    "    w_ew = equal_weight_allocation(N_ASSETS)\n",
    "    pv_ew = backtest_portfolio(df_test, w_ew, transaction_cost=transaction_cost)\n",
    "    w_hist_ew = np.tile(w_ew, (T_test, 1))\n",
    "\n",
    "    # 2) SJM-BL strategy\n",
    "    pv_sjmbl, w_sjmbl, fallback_sjmbl = regime_based_bl_backtest_flatprior(\n",
    "        df_train,\n",
    "        df_test,\n",
    "        sjm_states_test,\n",
    "        sjm_states_train[split_idx - 1, :],  # use the last training row explicitly\n",
    "        sjm_regime_means,\n",
    "        transaction_cost=transaction_cost,\n",
    "        risk_free_rate=risk_free_rate,\n",
    "        bl_tau=bl_tau,\n",
    "        halflife=126\n",
    "    )\n",
    "\n",
    "    perf = {\n",
    "        \"EW\": compute_performance_metrics(pv_ew, w_hist_ew),\n",
    "        \"SJM-BL\": compute_performance_metrics(pv_sjmbl, w_sjmbl)\n",
    "    }\n",
    "    fallback_events = {\"SJM-BL\": fallback_sjmbl}\n",
    "    return perf, fallback_events\n",
    "\n",
    "\n",
    "\n",
    "# --- 6. Monte Carlo simulation for a given lambda ---\n",
    "def single_simulation_run(run_id, T_sim=1000, lam_sjm=50, risk_free_rate=0.02/252,\n",
    "                          transaction_cost=0.0007, bl_tau=0.05):\n",
    "    \"\"\"\n",
    "    Creates a 2-state dataset, runs the two strategies (EW & SJM-BL),\n",
    "    returns the performance results.\n",
    "    \"\"\"\n",
    "    seed = run_id * 1000 + 100  # reproducibility\n",
    "    df_sim, _ = simulate_2state_data(T_sim, seed=seed)\n",
    "    return run_allocation_simulation(df_sim, lam_sjm, risk_free_rate, transaction_cost, bl_tau)\n",
    "\n",
    "def run_monte_carlo_for_lambda(n_runs=8, T_sim=1000, lam_sjm=50, risk_free_rate=0.02/252,\n",
    "                               transaction_cost=0.0007, bl_tau=0.05):\n",
    "    \"\"\"\n",
    "    Runs multiple simulations in parallel for a given lambda,\n",
    "    aggregates results.\n",
    "    \"\"\"\n",
    "    # Use up to 6 cores\n",
    "    results = Parallel(n_jobs=6)(\n",
    "        delayed(single_simulation_run)(\n",
    "            i+1, T_sim, lam_sjm, risk_free_rate, transaction_cost, bl_tau\n",
    "        )\n",
    "        for i in range(n_runs)\n",
    "    )\n",
    "\n",
    "    strategies = [\"EW\", \"SJM-BL\"]\n",
    "    metric_keys = [\n",
    "        \"Annualized Return\", \"Cumulative Return\", \"Volatility\",\n",
    "        \"Downside Deviation\", \"Max Drawdown\", \"Sharpe Ratio\",\n",
    "        \"Sortino Ratio\", \"Calmar Ratio\", \"Turnover Rate\"\n",
    "    ]\n",
    "    all_metrics = {\n",
    "        st: {m: [] for m in metric_keys} for st in strategies\n",
    "    }\n",
    "\n",
    "    # Collect performance metrics\n",
    "    for perf, _ in results:\n",
    "        for st in strategies:\n",
    "            for mkey in metric_keys:\n",
    "                all_metrics[st][mkey].append(perf[st][mkey])\n",
    "\n",
    "    return all_metrics, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lambda_grid = [10, 25, 50, 75, 100, 125]  # Grid of lambda values\n",
    "    T_sim = 5000\n",
    "    n_simulations = 8\n",
    "\n",
    "    summary_results = {}\n",
    "    for lam in lambda_grid:\n",
    "        print(f\"\\nRunning simulations for lambda = {lam}...\")\n",
    "        metrics, _ = run_monte_carlo_for_lambda(\n",
    "            n_runs=n_simulations,\n",
    "            T_sim=T_sim,\n",
    "            lam_sjm=lam\n",
    "        )\n",
    "        summary_results[lam] = metrics\n",
    "\n",
    "        # Print summary stats\n",
    "        for strategy, mdict in metrics.items():\n",
    "            print(f\"\\nStrategy: {strategy} (lambda = {lam})\")\n",
    "            for mkey, values in mdict.items():\n",
    "                mean_val = np.nanmean(values)\n",
    "                std_val  = np.nanstd(values)\n",
    "                print(f\"{mkey}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
