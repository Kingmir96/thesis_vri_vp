{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SJM-BL Simulation study (scenario 1)\n",
    "### 1.0 Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For HMM, MLE\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import wilcoxon\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Import your jumpmodels\n",
    "from jumpmodels.sparse_jump import SparseJumpModel\n",
    "from jumpmodels.jump import JumpModel\n",
    "from jumpmodels.preprocess import StandardScalerPD, DataClipperStd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Importing Portfolio Packages\n",
    "from pypfopt.black_litterman import BlackLittermanModel, market_implied_risk_aversion\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import base_optimizer, expected_returns, risk_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Simulation\n",
    "\n",
    "#### 2.1 Simulating the 1-state data\n",
    "We are simulating 6 fictional assets which are representing the 6 factors in our framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Asset\" universe\n",
    "assets = [\"Value\", \"Growth\", \"LowVol\", \"Size\", \"Momentum\", \"Quality\"]\n",
    "n_assets = len(assets)\n",
    "rng = np.random.default_rng(42)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_1state_data(T):\n",
    "    \"\"\"\n",
    "    1-state model: each day, returns ~ Student-t with \n",
    "      mean = 0.000461, stdev = 0.008388, and pairwise correlation = 0.185.\n",
    "    Vectorized implementation.\n",
    "    \"\"\"\n",
    "    mu = 0.000461\n",
    "    sig = 0.008388\n",
    "    dof = 5\n",
    "    # Create correlation matrix with off-diagonals = 0.185\n",
    "    corr = np.full((n_assets, n_assets), 0.185)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    Cov = (sig * np.ones(n_assets))[:, None] @ (sig * np.ones(n_assets))[None, :] * corr\n",
    "\n",
    "    # Generate all multivariate normal draws at once\n",
    "    z = rng.multivariate_normal(mean=np.zeros(n_assets), cov=Cov, size=T)\n",
    "    # Generate T chi-square values and compute scaling factors\n",
    "    chi = rng.chisquare(dof, size=T)\n",
    "    factor = np.sqrt(dof / chi)\n",
    "    # Broadcast factor to each asset dimension\n",
    "    rets = mu + z * factor[:, np.newaxis]\n",
    "    \n",
    "    return pd.DataFrame(rets, columns=assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Simulating 2-state data\n",
    "\n",
    "This function simulates a 2-state HMM (bull/bear) with state‐dependent Student‑t returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_2state_data(T):\n",
    "    \"\"\"\n",
    "    2-state HMM with daily bull/bear parameters:\n",
    "      Bull: mu=0.0006, sigma=0.00757\n",
    "      Bear: mu=-0.000881, sigma=0.0163\n",
    "    Transition matrix:\n",
    "      [0.9976, 0.0024]\n",
    "      [0.0232, 0.9768]\n",
    "    \"\"\"\n",
    "    transmat = np.array([[0.9976, 0.0024],\n",
    "                          [0.0232, 0.9768]])\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    states[0] = rng.integers(2)\n",
    "    for t in range(1, T):\n",
    "        states[t] = rng.choice(2, p=transmat[states[t - 1]])\n",
    "\n",
    "    mu_dict = {0: 0.0006, 1: -0.000881}\n",
    "    sig_dict = {0: 0.00757, 1: 0.0163}\n",
    "    # Create common correlation matrix\n",
    "    corr = np.full((n_assets, n_assets), 0.185)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    \n",
    "    rets = np.zeros((T, n_assets))\n",
    "    dof = 5\n",
    "    for t in range(T):\n",
    "        s = states[t]\n",
    "        mu_s = np.full(n_assets, mu_dict[s])\n",
    "        sig_s = np.full(n_assets, sig_dict[s])\n",
    "        Cov_s = np.outer(sig_s, sig_s) * corr\n",
    "        z = rng.multivariate_normal(mean=np.zeros(n_assets), cov=Cov_s)\n",
    "        chi = rng.chisquare(dof)\n",
    "        factor = np.sqrt(dof / chi)\n",
    "        rets[t] = mu_s + factor * z\n",
    "\n",
    "    return pd.DataFrame(rets, columns=assets), states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Simulating 3-state data\n",
    "\n",
    "We are simulating 6 fictional assets which are representing the 6 factors in our framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a 3-state HMM with states 0,1,2, stationaries ~ (0.89,0.04,0.07), means ~ (0.0005862,0.0,-0.0008672) etc.\n",
    "def simulate_3state_data(T):\n",
    "    \"\"\"\n",
    "    3-state HMM with approximate stationary distribution (0.89, 0.04, 0.07)\n",
    "    Means: [0.0005862, 0.0, -0.0008672]\n",
    "    Stdevs: [0.0075313, 0.0135351, 0.0163387]\n",
    "    \"\"\"\n",
    "    transmat = np.array([[0.9950, 0.004335, 0.000665],\n",
    "                         [0.01667, 0.95, 0.03333],\n",
    "                         [0.00652, 0.04348, 0.9500]])\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    states[0] = rng.integers(3)\n",
    "    for t in range(1, T):\n",
    "        states[t] = rng.choice(3, p=transmat[states[t - 1]])\n",
    "\n",
    "    mu_list = [0.0005862, 0.0, -0.0008672]\n",
    "    sig_list = [0.0075313, 0.0135351, 0.0163387]\n",
    "    corr = np.full((n_assets, n_assets), 0.185)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    \n",
    "    rets = np.zeros((T, n_assets))\n",
    "    dof = 5\n",
    "    for t in range(T):\n",
    "        s = states[t]\n",
    "        mu_s = np.full(n_assets, mu_list[s])\n",
    "        sig_s = np.full(n_assets, sig_list[s])\n",
    "        Cov_s = np.outer(sig_s, sig_s) * corr\n",
    "        z = rng.multivariate_normal(mean=np.zeros(n_assets), cov=Cov_s)\n",
    "        chi = rng.chisquare(dof)\n",
    "        factor = np.sqrt(dof / chi)\n",
    "        rets[t] = mu_s + factor * z\n",
    "\n",
    "    return pd.DataFrame(rets, columns=assets), states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Training Regime Models\n",
    "\n",
    "#### 3.1 Training HMM model with k-means clustering initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmm_kmeans(X, n_components=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Fit a GaussianHMM with k-means initialization.\n",
    "    Returns the trained model and the predicted states.\n",
    "    \"\"\"\n",
    "    model = GaussianHMM(n_components=n_components, covariance_type=\"diag\",\n",
    "                        n_iter=100, random_state=random_state)\n",
    "    # Initialize with KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10, random_state=random_state)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    means, covars = [], []\n",
    "    for i in range(n_components):\n",
    "        obs_i = X[labels == i]\n",
    "        means.append(obs_i.mean(axis=0))\n",
    "        covars.append(obs_i.var(axis=0) + 1e-2)\n",
    "    model.startprob_ = np.ones(n_components) / n_components\n",
    "    model.transmat_  = np.ones((n_components, n_components)) / n_components\n",
    "    model.means_     = np.array(means)\n",
    "    model.covars_    = np.array(covars)\n",
    "    model.init_params = 'tmc'\n",
    "    \n",
    "    model.fit(X)\n",
    "    pred_states = model.predict(X)\n",
    "    return model, pred_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Training Sparse Jump model with max_feats=9 and lambda=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sjm(X, max_feats=9.0, lam=80.0, n_components=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a SparseJumpModel with specified hyperparameters.\n",
    "    \"\"\"\n",
    "    sjm = SparseJumpModel(n_components=n_components,\n",
    "                          max_feats=max_feats,\n",
    "                          jump_penalty=lam,\n",
    "                          cont=False,\n",
    "                          max_iter=20,          # coordinate-descent steps\n",
    "                          random_state=random_state)\n",
    "    sjm.fit(X)\n",
    "    return sjm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Allocation simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Allocation workhorse functions\n",
    "In this code we create the in which we fit the following models (each done in a seperate for loop such that we can store the relevant data such as return, weights, etc. in seperate dfs):\n",
    "1. Equal weigted\n",
    "2. Inverse volatility weighted\n",
    "3. Mean-Variance-Optimal static portfolio\n",
    "4. Hidden Markov Model Black Litterman where infered states are the identified regimes\n",
    "5. Sparse Jump Model Black Litterman where infered states are the identified regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_mvo_allocation(returns, risk_free_rate=0.02):\n",
    "    \"\"\"\n",
    "    Calculate static MVO portfolio using EfficientFrontier (max Sharpe).\n",
    "    \"\"\"\n",
    "    mu = expected_returns.mean_historical_return(returns)\n",
    "    S = risk_models.sample_cov(returns)\n",
    "    ef = EfficientFrontier(mu, S, weight_bounds=(0, 1))\n",
    "    weights = ef.max_sharpe(risk_free_rate=risk_free_rate)\n",
    "    return ef.clean_weights()\n",
    "\n",
    "def inverse_vol_weights(returns):\n",
    "    stds = returns.std(axis=0).values\n",
    "    inv = 1.0 / (stds + 1e-12)\n",
    "    return inv / inv.sum()\n",
    "\n",
    "def backtest_portfolio(returns, weights):\n",
    "    T = len(returns)\n",
    "    pv = np.zeros(T)\n",
    "    pv[0] = 1.0\n",
    "    for t in range(T - 1):\n",
    "        r = returns.iloc[t].values\n",
    "        pv[t + 1] = pv[t] * (1.0 + np.dot(weights, r))\n",
    "    return pv\n",
    "\n",
    "def bl_allocation(view_vector, prior_cov, tau=0.05, risk_free_rate=0.02):\n",
    "    \"\"\"\n",
    "    Calculate Black-Litterman weights given a view vector.\n",
    "    \"\"\"\n",
    "    viewdict = {asset: view for asset, view in zip(assets, view_vector)}\n",
    "    bl = BlackLittermanModel(cov_matrix=prior_cov, absolute_views=viewdict, tau=tau, risk_aversion=1)\n",
    "    weights = bl.bl_weights()  # returns an OrderedDict\n",
    "    # Convert to numpy array in the asset order\n",
    "    return np.array([weights[asset] for asset in assets])\n",
    "\n",
    "def regime_based_bl_backtest(returns, states, regime_mu_dict, prior_cov, tau=0.05, risk_free_rate=0.02):\n",
    "    \"\"\"\n",
    "    Perform BL dynamic allocation:\n",
    "      - When a regime change occurs, update weights using the view from regime_mu_dict.\n",
    "      - Otherwise, carry over previous weights.\n",
    "    \"\"\"\n",
    "    T = len(returns)\n",
    "    n = returns.shape[1]\n",
    "    pv = np.zeros(T)\n",
    "    pv[0] = 1.0\n",
    "    w_hist = np.zeros((T, n))\n",
    "    # Start with an equal-weight portfolio\n",
    "    w_hist[0] = np.ones(n) / n\n",
    "    \n",
    "    for t in range(T - 1):\n",
    "        r = returns.iloc[t].values\n",
    "        pv[t + 1] = pv[t] * (1.0 + np.dot(w_hist[t], r))\n",
    "        # Update weights if the regime changes\n",
    "        if t == 0 or (states[t] != states[t - 1]):\n",
    "            Q = regime_mu_dict[states[t]]\n",
    "            w_new = bl_allocation(Q, prior_cov, tau=tau, risk_free_rate=risk_free_rate)\n",
    "        else:\n",
    "            w_new = w_hist[t]\n",
    "        w_hist[t + 1] = w_new\n",
    "    # Final day update\n",
    "    r_last = returns.iloc[-1].values\n",
    "    pv[-1] = pv[-2] * (1.0 + np.dot(w_hist[-2], r_last))\n",
    "    return pv, w_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Performance metric evaluation:\n",
    "Here we divide the performance metric into. We assume 250 data points to be 1 year off trading:\n",
    "1. Return-Based Metrics \n",
    "\n",
    "Annualized Return: Average return per year. \n",
    "\n",
    "Cumulative Return: Total portfolio growth over time. \n",
    "\n",
    "2. Risk-Based Metrics \n",
    "\n",
    "Volatility: Standard deviation of returns. \n",
    "\n",
    "Downside Deviation: Measures negative return fluctuations. \n",
    "\n",
    "Max Drawdown (MDD): Largest portfolio decline from peak to trough. \n",
    "\n",
    "3. Risk-Adjusted Metrics \n",
    "\n",
    "Sharpe Ratio: Return per unit of total risk. \n",
    "\n",
    "Sortino Ratio: Return per unit of downside risk. \n",
    "\n",
    "Calmar Ratio: Return relative to max drawdown. \n",
    "\n",
    "4. Portfolio Stability & Adaptation \n",
    "\n",
    "Turnover Rate: Measures frequency of asset reallocation. \n",
    "\n",
    "\n",
    "We further split the performance three seperate tables with 1-state process, 2-state process, 3-state process\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(pv, annual_factor=250):\n",
    "    \"\"\"\n",
    "    Calculate various performance metrics given the portfolio value series.\n",
    "    \"\"\"\n",
    "    pv = np.array(pv)\n",
    "    rets = np.diff(pv) / pv[:-1]\n",
    "    ann_ret = rets.mean() * annual_factor\n",
    "    cum_ret = pv[-1] / pv[0] - 1\n",
    "    ann_vol = rets.std() * np.sqrt(annual_factor)\n",
    "    sharpe = ann_ret / (ann_vol + 1e-12)\n",
    "    running_max = np.maximum.accumulate(pv)\n",
    "    drawdown = (pv - running_max) / running_max\n",
    "    max_dd = drawdown.min()\n",
    "    return {\"AnnRet\": ann_ret, \"CumRet\": cum_ret, \"AnnVol\": ann_vol, \"Sharpe\": sharpe, \"MaxDD\": max_dd}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.0 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allocation(df, states=None, scenario_name=\"\"):\n",
    "    # Split data: first 80% for training, remaining 20% for testing\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    df_train = df.iloc[:split_idx]\n",
    "    df_test = df.iloc[split_idx:]\n",
    "    \n",
    "    # Preprocessing: fit clipper and scaler on training data; transform both sets.\n",
    "    clipper = DataClipperStd(mul=3.0)\n",
    "    scaler = StandardScalerPD()\n",
    "    df_train_clipped = clipper.fit_transform(df_train)\n",
    "    df_train_scaled = scaler.fit_transform(df_train_clipped)\n",
    "    X_train = df_train_scaled.values\n",
    "    \n",
    "    df_test_clipped = clipper.transform(df_test)\n",
    "    df_test_scaled = scaler.transform(df_test_clipped)\n",
    "    X_test = df_test_scaled.values\n",
    "    \n",
    "    # Fit regime models on training set\n",
    "    hmm_model, hmm_states_train = train_hmm_kmeans(X_train, n_components=2, random_state=42)\n",
    "    sjm_model = train_sjm(X_train, max_feats=9.0, lam=80.0, n_components=2, random_state=42)\n",
    "    \n",
    "    # Compute regime average returns from original (unscaled) training data for BL views.\n",
    "    def get_regime_means(df_orig, labels):\n",
    "        regs = {}\n",
    "        for lab in np.unique(labels):\n",
    "            idx = np.where(labels == lab)[0]\n",
    "            regs[lab] = df_orig.iloc[idx].mean(axis=0).values\n",
    "        return regs\n",
    "\n",
    "    hmm_mu = get_regime_means(df_train, hmm_states_train)\n",
    "    sjm_mu = get_regime_means(df_train, sjm_model.predict(X_train))\n",
    "    \n",
    "    # Define baseline priors from training data\n",
    "    prior_mu = df_train.mean(axis=0).values\n",
    "    prior_cov = df_train.cov().values\n",
    "    \n",
    "    # Allocations are computed on the test set.\n",
    "    # 1) Equal Weighted (EW)\n",
    "    w_ew = np.ones(n_assets) / n_assets\n",
    "    pv_ew = backtest_portfolio(df_test, w_ew)\n",
    "    # 2) Inverse Volatility (IV)\n",
    "    w_iv = inverse_vol_weights(df_test)\n",
    "    pv_iv = backtest_portfolio(df_test, w_iv)\n",
    "    # 3) Static MVO using PyPortfolioOpt\n",
    "    w_mvo = static_mvo_allocation(df_test, risk_free_rate=0.02)\n",
    "    w_mvo_arr = np.array([w_mvo[asset] for asset in assets])\n",
    "    pv_mvo = backtest_portfolio(df_test, w_mvo_arr)\n",
    "    # 4) HMM-BL dynamic allocation: predict test regimes using trained HMM\n",
    "    hmm_test_states = hmm_model.predict(X_test)\n",
    "    pv_hmmbl, _ = regime_based_bl_backtest(df_test, hmm_test_states, hmm_mu, prior_cov, tau=0.05, risk_free_rate=0.02)\n",
    "    # 5) SJM-BL dynamic allocation: predict test regimes using trained SJM\n",
    "    sjm_test_states = sjm_model.predict(X_test)\n",
    "    pv_sjmbl, _ = regime_based_bl_backtest(df_test, sjm_test_states, sjm_mu, prior_cov, tau=0.05, risk_free_rate=0.02)\n",
    "    \n",
    "    perf = {\n",
    "        \"EW\": performance_metrics(pv_ew),\n",
    "        \"IV\": performance_metrics(pv_iv),\n",
    "        \"MVO\": performance_metrics(pv_mvo),\n",
    "        \"HMM-BL\": performance_metrics(pv_hmmbl),\n",
    "        \"SJM-BL\": performance_metrics(pv_sjmbl)\n",
    "    }\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenario_1study(T_sim=1000):\n",
    "    results = {}\n",
    "    # Simulate data for each process:\n",
    "    df1 = simulate_1state_data(T_sim)\n",
    "    df2, states2 = simulate_2state_data(T_sim)\n",
    "    df3, states3 = simulate_3state_data(T_sim)\n",
    "    \n",
    "    results[\"1state\"] = run_allocation(df1, None, scenario_name=\"1state\")\n",
    "    results[\"2state\"] = run_allocation(df2, states2, scenario_name=\"2state\")\n",
    "    results[\"3state\"] = run_allocation(df3, states3, scenario_name=\"3state\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Main execution: Run simulation and output performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR in LDL_factor: Error in KKT matrix LDL factorization when computing the nonzero elements. The problem seems to be non-convex\n",
      "ERROR in osqp_setup: KKT matrix factorization.\n",
      "The problem seems to be non-convex.\n"
     ]
    },
    {
     "ename": "SolverError",
     "evalue": "Workspace allocation error!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cvxpy\\reductions\\solvers\\qp_solvers\\osqp_qpif.py:102\u001b[0m, in \u001b[0;36mOSQP.solve_via_data\u001b[1;34m(self, data, warm_start, verbose, solver_opts, solver_cache)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     solver\u001b[38;5;241m.\u001b[39msetup(P, q, A, lA, uA, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msolver_opts)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\osqp\\interface.py:37\u001b[0m, in \u001b[0;36mOSQP.setup\u001b[1;34m(self, P, q, A, l, u, **settings)\u001b[0m\n\u001b[0;32m     36\u001b[0m unpacked_data, settings \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mprepare_data(P, q, A, l, u, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39msetup(\u001b[38;5;241m*\u001b[39munpacked_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings)\n",
      "\u001b[1;31mValueError\u001b[0m: Workspace allocation error!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSolverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     sim_results \u001b[38;5;241m=\u001b[39m run_scenario_1study(T_sim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res \u001b[38;5;129;01min\u001b[39;00m sim_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Performance Metrics ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 8\u001b[0m, in \u001b[0;36mrun_scenario_1study\u001b[1;34m(T_sim)\u001b[0m\n\u001b[0;32m      5\u001b[0m df2, states2 \u001b[38;5;241m=\u001b[39m simulate_2state_data(T_sim)\n\u001b[0;32m      6\u001b[0m df3, states3 \u001b[38;5;241m=\u001b[39m simulate_3state_data(T_sim)\n\u001b[1;32m----> 8\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_allocation(df1, \u001b[38;5;28;01mNone\u001b[39;00m, scenario_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_allocation(df2, states2, scenario_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_allocation(df3, states3, scenario_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 45\u001b[0m, in \u001b[0;36mrun_allocation\u001b[1;34m(df, states, scenario_name)\u001b[0m\n\u001b[0;32m     43\u001b[0m pv_iv \u001b[38;5;241m=\u001b[39m backtest_portfolio(df_test, w_iv)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 3) Static MVO using PyPortfolioOpt\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m w_mvo \u001b[38;5;241m=\u001b[39m static_mvo_allocation(df_test, risk_free_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m)\n\u001b[0;32m     46\u001b[0m w_mvo_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([w_mvo[asset] \u001b[38;5;28;01mfor\u001b[39;00m asset \u001b[38;5;129;01min\u001b[39;00m assets])\n\u001b[0;32m     47\u001b[0m pv_mvo \u001b[38;5;241m=\u001b[39m backtest_portfolio(df_test, w_mvo_arr)\n",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m, in \u001b[0;36mstatic_mvo_allocation\u001b[1;34m(returns, risk_free_rate)\u001b[0m\n\u001b[0;32m      6\u001b[0m S \u001b[38;5;241m=\u001b[39m risk_models\u001b[38;5;241m.\u001b[39msample_cov(returns)\n\u001b[0;32m      7\u001b[0m ef \u001b[38;5;241m=\u001b[39m EfficientFrontier(mu, S, weight_bounds\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 8\u001b[0m weights \u001b[38;5;241m=\u001b[39m ef\u001b[38;5;241m.\u001b[39mmax_sharpe(risk_free_rate\u001b[38;5;241m=\u001b[39mrisk_free_rate)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ef\u001b[38;5;241m.\u001b[39mclean_weights()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pypfopt\\efficient_frontier\\efficient_frontier.py:290\u001b[0m, in \u001b[0;36mEfficientFrontier.max_sharpe\u001b[1;34m(self, risk_free_rate)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Transformed max_sharpe convex problem:\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraints \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    285\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_returns \u001b[38;5;241m-\u001b[39m risk_free_rate)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_w \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    286\u001b[0m     cp\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_w) \u001b[38;5;241m==\u001b[39m k,\n\u001b[0;32m    287\u001b[0m     k \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    288\u001b[0m ] \u001b[38;5;241m+\u001b[39m new_constraints\n\u001b[1;32m--> 290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_cvxpy_opt_problem()\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# Inverse-transform\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_w\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m/\u001b[39m k\u001b[38;5;241m.\u001b[39mvalue)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m16\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pypfopt\\base_optimizer.py:307\u001b[0m, in \u001b[0;36mBaseConvexOptimizer._solve_cvxpy_opt_problem\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constr_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_constraint_ids:\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInstantiationError(\n\u001b[0;32m    304\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe constraints were changed after the initial optimization. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease create a new instance instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m             )\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opt\u001b[38;5;241m.\u001b[39msolve(\n\u001b[0;32m    308\u001b[0m         solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solver, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solver_options\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, cp\u001b[38;5;241m.\u001b[39mDCPError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mOptimizationError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cvxpy\\problems\\problem.py:600\u001b[0m, in \u001b[0;36mProblem.solve\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    598\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver_path\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please choose one.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_solver_path(solve_func,solver_path, args, kwargs)\n\u001b[1;32m--> 600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solve_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cvxpy\\problems\\problem.py:1183\u001b[0m, in \u001b[0;36mProblem._solve\u001b[1;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver_verbose \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m verbose):\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_NUM_SOLVER_STR)\n\u001b[1;32m-> 1183\u001b[0m solution \u001b[38;5;241m=\u001b[39m solving_chain\u001b[38;5;241m.\u001b[39msolve_via_data(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m, data, warm_start, solver_verbose, kwargs)\n\u001b[0;32m   1185\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:524\u001b[0m, in \u001b[0;36mSolvingChain.solve_via_data\u001b[1;34m(self, problem, data, warm_start, verbose, solver_opts)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msolve_via_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, problem, data, warm_start: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    489\u001b[0m                    solver_opts\u001b[38;5;241m=\u001b[39m{}):\n\u001b[0;32m    490\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Solves the problem using the data output by the an apply invocation.\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m    The semantics are:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03m        a Solution object.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39msolve_via_data(data, warm_start, verbose,\n\u001b[0;32m    525\u001b[0m                                       solver_opts, problem\u001b[38;5;241m.\u001b[39m_solver_cache)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cvxpy\\reductions\\solvers\\qp_solvers\\osqp_qpif.py:104\u001b[0m, in \u001b[0;36mOSQP.solve_via_data\u001b[1;34m(self, data, warm_start, verbose, solver_opts, solver_cache)\u001b[0m\n\u001b[0;32m    102\u001b[0m         solver\u001b[38;5;241m.\u001b[39msetup(P, q, A, lA, uA, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msolver_opts)\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SolverError(e)\n\u001b[0;32m    106\u001b[0m results \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39msolve()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mSolverError\u001b[0m: Workspace allocation error!"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sim_results = run_scenario_1study(T_sim=1000)\n",
    "    for key, res in sim_results.items():\n",
    "        print(f\"=== {key.upper()} Performance Metrics ===\")\n",
    "        print(pd.DataFrame(res).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
