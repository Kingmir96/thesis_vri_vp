{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7646fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from jumpmodels.utils import filter_date_range\n",
    "from jumpmodels.preprocess import StandardScalerPD, DataClipperStd\n",
    "from jumpmodels.sparse_jump import SparseJumpModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pypfopt.black_litterman import BlackLittermanModel, market_implied_prior_returns\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pypfopt import exceptions \n",
    "from scipy.optimize import brentq\n",
    "import cvxpy as cp\n",
    "from scipy import stats\n",
    "\n",
    "#sys.path.append('/Users/victor/Documents/thesis_vri_vp/vic_new')         # for mac\n",
    "sys.path.append('C:\\\\Users\\\\victo\\\\git_new\\\\thesis_vri_vp\\\\vic_new')      # for windows\n",
    "# sys.path.append('/Users/vlad/Desktop/git/Masters-Thesis-VRI-VP/vic_new')         # for mac vlad\n",
    "from feature_set_v2 import MergedDataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e65d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REFIT_FREQ        = \"ME\"        \n",
    "MIN_TRAINING_YEARS= 8\n",
    "MAX_TRAINING_YEARS= 12\n",
    "INITIAL_TRAIN_START = \"2002-05-31\"\n",
    "test_start        = \"2017-01-01\"\n",
    "\n",
    "\n",
    "cv_choice = \"bayes\"\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "base_dir   = os.path.abspath(os.path.join(script_dir, \"..\", \"..\", \"..\",\"..\", \"..\",\"..\"))\n",
    "data_dir   = os.path.join(base_dir, \"data_new\")\n",
    "\n",
    "factor_file = os.path.join(data_dir, \"1estimation_index_returns.csv\")\n",
    "market_file = os.path.join(data_dir, \"1macro_data.csv\")\n",
    "etf_file    = os.path.join(data_dir, \"2trading_etf_returns_aligned.csv\")\n",
    "\n",
    "factors = [\"iwf\", \"mtum\", \"qual\", \"size\", \"usmv\", \"vlue\"]  \n",
    "\n",
    "\n",
    "bayes_df   = pd.read_parquet(\"Riskoff_CV_output.parquet\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4281c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_map = {\n",
    "\n",
    "    \"bayes\":   bayes_df,\n",
    "\n",
    "}\n",
    "cv_df = df_map[cv_choice]\n",
    "\n",
    "\n",
    "\n",
    "saved_hyperparams = {}\n",
    "for fac in factors:\n",
    "    sub = cv_df[cv_df[\"factor\"] == fac].sort_values(\"date\")\n",
    "    saved_hyperparams[fac] = [\n",
    "        {\n",
    "            \"date\":      row[\"date\"],\n",
    "            \"new_lambda\": row[\"best_lambda\"],\n",
    "            \"new_kappa\":  row[\"best_kappa\"]\n",
    "        }\n",
    "        for _, row in sub.iterrows()\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for iwf\n",
      "Loading data for mtum\n",
      "Loading data for qual\n",
      "Loading data for size\n",
      "Loading data for usmv\n",
      "Loading data for vlue\n"
     ]
    }
   ],
   "source": [
    "\n",
    "factor_data_dict  = {}\n",
    "factor_returns_ls = []\n",
    "\n",
    "for fac in factors:\n",
    "    print(f\"Loading data for {fac}\")\n",
    "    data = MergedDataLoader(\n",
    "        factor_file=factor_file,\n",
    "        market_file=market_file,\n",
    "        ver=\"v2\",\n",
    "        factor_col=fac\n",
    "    ).load()\n",
    "\n",
    "    common_idx = (data.X.index\n",
    "                  .intersection(data.ret_ser.index)\n",
    "                  .intersection(data.market_ser.index))\n",
    "\n",
    "    X_full        = data.X.loc[common_idx]\n",
    "    fac_ret_full  = data.ret_ser.loc[common_idx]\n",
    "    mkt_ret_full  = data.market_ser.loc[common_idx]\n",
    "    active_ret    = fac_ret_full - mkt_ret_full\n",
    "\n",
    "    factor_data_dict[fac] = {\n",
    "        \"X\"        : X_full,\n",
    "        \"fac_ret\"  : fac_ret_full,\n",
    "        \"mkt_ret\"  : mkt_ret_full,\n",
    "        \"active_ret\": active_ret,\n",
    "    }\n",
    "    factor_returns_ls.append(fac_ret_full)\n",
    "\n",
    "all_market_ret = mkt_ret_full\n",
    "\n",
    "\n",
    "full_factors_df = pd.concat(factor_returns_ls, axis=1).dropna()\n",
    "full_df = pd.concat([full_factors_df, all_market_ret], axis=1).dropna()\n",
    "full_df.columns = factors + [\"Market\"]\n",
    "\n",
    "\n",
    "etf_df   = pd.read_csv(etf_file, index_col=0, parse_dates=True).dropna().sort_index()\n",
    "rf_ser   = etf_df[\"rf\"]\n",
    "full_df  = pd.concat([full_df, rf_ser], axis=1).dropna()\n",
    "full_df.columns = factors + [\"Market\", \"rf\"]\n",
    "\n",
    "\n",
    "test_slice = full_df.loc[test_start:]\n",
    "test_index = test_slice.index.sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VIEWS_FILE = \"views2.pkl\"\n",
    "FORCE_REBUILD = False \n",
    "\n",
    "def _fit_one_factor(fac, refit_date, test_dates_chunk,\n",
    "                    factor_data_dict, hyperparams,\n",
    "                    min_years, max_years, init_start):\n",
    "\n",
    "\n",
    "    def get_train_window(current_date, full_data):\n",
    "        train_end  = current_date\n",
    "        train_start= max(train_end - pd.DateOffset(years=max_years),\n",
    "                         pd.to_datetime(init_start))\n",
    "        if (train_end - train_start) < pd.Timedelta(days=365.25*min_years):\n",
    "            train_start = train_end - pd.DateOffset(years=min_years)\n",
    "        idx = full_data.index\n",
    "        subset = idx[(idx >= train_start) & (idx <= train_end)]\n",
    "        start_date, end_date = subset.min(), subset.max()\n",
    "        return start_date, end_date \n",
    "\n",
    "\n",
    "    fac_data = factor_data_dict[fac]\n",
    "    X   = fac_data[\"X\"]\n",
    "    ret = fac_data[\"fac_ret\"]\n",
    "    act = fac_data[\"active_ret\"]\n",
    "\n",
    "    lam = hyperparams[\"new_lambda\"]\n",
    "    kp  = hyperparams[\"new_kappa\"]\n",
    "    train_start, train_end = get_train_window(refit_date, X)\n",
    "\n",
    "\n",
    "    clipper = DataClipperStd(mul=3.0)\n",
    "    scaler  = StandardScaler()\n",
    "    X_train = scaler.fit_transform(clipper.fit_transform(\n",
    "                 filter_date_range(X, train_start, train_end)))\n",
    "    active_train = filter_date_range(act, train_start, train_end)\n",
    "\n",
    "\n",
    "    sjm = SparseJumpModel(n_components=2,\n",
    "                          max_feats=int(kp**2),\n",
    "                          jump_penalty=lam)\n",
    "    \n",
    "    train_idx = filter_date_range(X, train_start, train_end).index\n",
    "    X_train_df = pd.DataFrame(X_train, index=train_idx, columns=X.columns)\n",
    "    sjm.fit(X_train_df, ret_ser=active_train, sort_by=\"cumret\")\n",
    "\n",
    "    ret_train = filter_date_range(ret, train_start, train_end)\n",
    "\n",
    "\n",
    "    train_states = sjm.predict(X_train_df)\n",
    "    abs_ret = {}\n",
    "    for st in range(2):\n",
    "        st_idx = (train_states==st)\n",
    "        abs_ret[st] = ret_train.loc[st_idx].mean() * 252\n",
    "\n",
    "\n",
    "    states = {}\n",
    "    for day in test_dates_chunk:\n",
    "        X_hist = X.loc[:day]             \n",
    "        temp_clipper = DataClipperStd(mul=3.0)\n",
    "        X_hist_clip  = temp_clipper.fit_transform(X_hist)\n",
    "\n",
    "        temp_scaler  = StandardScaler()\n",
    "        _ = temp_scaler.fit_transform(X_hist_clip)  \n",
    "\n",
    "        if day in X.index:\n",
    "            X_day_clip   = temp_clipper.transform(X.loc[[day]])\n",
    "            X_day_scaled = temp_scaler.transform(X_day_clip)\n",
    "            states[day]  = sjm.predict_online(\n",
    "                pd.DataFrame(X_day_scaled,\n",
    "                            index=[day],\n",
    "                            columns=X.columns)).iloc[0]\n",
    "\n",
    "\n",
    "    out = pd.DataFrame({\"state\": pd.Series(states)},\n",
    "                       index=list(states.keys()))\n",
    "    out[\"ann_abs_ret\"] = out[\"state\"].map(abs_ret)\n",
    "    return fac, out\n",
    "\n",
    "def build_factor_views(factor_data_dict, saved_hyperparams, factors,\n",
    "                       test_index,\n",
    "                       refit_freq=\"ME\", min_years=8, max_years=12,\n",
    "                       init_start=\"2002-05-31\"):\n",
    "\n",
    "    views = {f:[] for f in factors}\n",
    "    refit_dates = (test_index.to_series()\n",
    "                   .resample(refit_freq)\n",
    "                   .last()\n",
    "                   .dropna())\n",
    "\n",
    "    for j, refit_date in enumerate(refit_dates):\n",
    "        if j < len(refit_dates)-1:\n",
    "            next_refit = refit_dates.iloc[j+1]\n",
    "        else:\n",
    "            next_refit = test_index[-1]\n",
    "        test_mask = (test_index>refit_date)&(test_index<=next_refit)\n",
    "        test_chunk = test_index[test_mask]\n",
    "\n",
    "\n",
    "        jobs = []\n",
    "        for fac in factors:\n",
    "\n",
    "            hp_hist = [h for h in saved_hyperparams[fac]\n",
    "                       if pd.to_datetime(h[\"date\"])<=refit_date]\n",
    "            if not hp_hist: continue\n",
    "            hp = hp_hist[-1]\n",
    "            jobs.append(delayed(_fit_one_factor)(\n",
    "                fac, refit_date, test_chunk,\n",
    "                factor_data_dict, hp,\n",
    "                min_years, max_years, init_start))\n",
    "        for fac, df in Parallel(n_jobs=-1)(jobs):\n",
    "            views[fac].append(df)\n",
    "\n",
    "\n",
    "    for fac in factors:\n",
    "        views[fac] = (pd.concat(views[fac])\n",
    "                      .sort_index()\n",
    "                      .loc[:,[\"state\",\"ann_abs_ret\"]])\n",
    "    return views\n",
    "\n",
    "   \n",
    "if FORCE_REBUILD or not os.path.exists(VIEWS_FILE):\n",
    "    factor_views = build_factor_views(factor_data_dict, saved_hyperparams, factors, \n",
    "                                      test_index,\n",
    "                                      refit_freq=REFIT_FREQ, \n",
    "                                      min_years=8, max_years=12, init_start=\"2002-05-31\")\n",
    "    with open(VIEWS_FILE, \"wb\") as f:\n",
    "        pickle.dump(factor_views, f)\n",
    "else:\n",
    "    with open(VIEWS_FILE, \"rb\") as f:\n",
    "        factor_views = pickle.load(f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
