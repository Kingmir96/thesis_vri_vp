{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7646fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from jumpmodels.utils import filter_date_range\n",
    "from jumpmodels.preprocess import StandardScalerPD, DataClipperStd\n",
    "from jumpmodels.sparse_jump import SparseJumpModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pypfopt.black_litterman import BlackLittermanModel, market_implied_prior_returns\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pypfopt import exceptions \n",
    "from scipy.optimize import brentq\n",
    "import cvxpy as cp\n",
    "from scipy import stats\n",
    "\n",
    "#sys.path.append('/Users/victor/Documents/thesis_vri_vp/vic_new')         # for mac\n",
    "sys.path.append('C:\\\\Users\\\\victo\\\\git_new\\\\thesis_vri_vp\\\\vic_new')      # for windows\n",
    "# sys.path.append('/Users/vlad/Desktop/git/Masters-Thesis-VRI-VP/vic_new')         # for mac vlad\n",
    "from feature_set_v2 import MergedDataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e65d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REFIT_FREQ        = \"ME\"        \n",
    "MIN_TRAINING_YEARS= 8\n",
    "MAX_TRAINING_YEARS= 12\n",
    "INITIAL_TRAIN_START = \"2002-05-31\"\n",
    "test_start        = \"2017-01-01\"\n",
    "\n",
    "\n",
    "cv_choice = \"bayes\"\n",
    "\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "base_dir   = os.path.abspath(os.path.join(script_dir, \"..\", \"..\", \"..\",\"..\", \"..\",\"..\",\"..\",\"..\"))\n",
    "data_dir   = os.path.join(base_dir, \"data_new\")\n",
    "\n",
    "factor_file = os.path.join(data_dir, \"1estimation_index_returns.csv\")\n",
    "market_file = os.path.join(data_dir, \"1macro_data.csv\")\n",
    "etf_file    = os.path.join(data_dir, \"2trading_etf_returns_aligned.csv\")\n",
    "\n",
    "factors = [\"iwf\", \"mtum\", \"qual\", \"size\", \"usmv\", \"vlue\"]  \n",
    "\n",
    "\n",
    "bayes_df   = pd.read_parquet(\"Main_CV_output.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4281c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_map = {\n",
    "\n",
    "    \"bayes\":   bayes_df,\n",
    "\n",
    "}\n",
    "cv_df = df_map[cv_choice]\n",
    "\n",
    "\n",
    "saved_hyperparams = {}\n",
    "for fac in factors:\n",
    "    sub = cv_df[cv_df[\"factor\"] == fac].sort_values(\"date\")\n",
    "    saved_hyperparams[fac] = [\n",
    "        {\n",
    "            \"date\":      row[\"date\"],\n",
    "            \"new_lambda\": row[\"best_lambda\"],\n",
    "            \"new_kappa\":  row[\"best_kappa\"]\n",
    "        }\n",
    "        for _, row in sub.iterrows()\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for iwf\n",
      "Loading data for mtum\n",
      "Loading data for qual\n",
      "Loading data for size\n",
      "Loading data for usmv\n",
      "Loading data for vlue\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "factor_data_dict  = {}\n",
    "factor_returns_ls = []\n",
    "\n",
    "for fac in factors:\n",
    "    print(f\"Loading data for {fac}\")\n",
    "    data = MergedDataLoader(\n",
    "        factor_file=factor_file,\n",
    "        market_file=market_file,\n",
    "        ver=\"v2\",\n",
    "        factor_col=fac\n",
    "    ).load()\n",
    "\n",
    "    common_idx = (data.X.index\n",
    "                  .intersection(data.ret_ser.index)\n",
    "                  .intersection(data.market_ser.index))\n",
    "\n",
    "    X_full        = data.X.loc[common_idx]\n",
    "    fac_ret_full  = data.ret_ser.loc[common_idx]\n",
    "    mkt_ret_full  = data.market_ser.loc[common_idx]\n",
    "    active_ret    = fac_ret_full - mkt_ret_full\n",
    "\n",
    "    factor_data_dict[fac] = {\n",
    "        \"X\"        : X_full,\n",
    "        \"fac_ret\"  : fac_ret_full,\n",
    "        \"mkt_ret\"  : mkt_ret_full,\n",
    "        \"active_ret\": active_ret,\n",
    "    }\n",
    "    factor_returns_ls.append(fac_ret_full)\n",
    "\n",
    "\n",
    "all_market_ret = mkt_ret_full\n",
    "\n",
    "\n",
    "full_factors_df = pd.concat(factor_returns_ls, axis=1).dropna()\n",
    "full_df = pd.concat([full_factors_df, all_market_ret], axis=1).dropna()\n",
    "full_df.columns = factors + [\"Market\"]\n",
    "\n",
    "\n",
    "etf_df   = pd.read_csv(etf_file, index_col=0, parse_dates=True).dropna().sort_index()\n",
    "rf_ser   = etf_df[\"rf\"]\n",
    "full_df  = pd.concat([full_df, rf_ser], axis=1).dropna()\n",
    "full_df.columns = factors + [\"Market\", \"rf\"]\n",
    "\n",
    "\n",
    "test_slice = full_df.loc[test_start:]\n",
    "test_index = test_slice.index.sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VIEWS_FILE = \"views.pkl\" \n",
    "FORCE_REBUILD = False \n",
    "\n",
    "def _fit_one_factor(fac, refit_date, test_dates_chunk,\n",
    "                    factor_data_dict, hyperparams,\n",
    "                    min_years, max_years, init_start):\n",
    "\n",
    "\n",
    "    def get_train_window(current_date, full_data):\n",
    "        train_end  = current_date\n",
    "        train_start= max(train_end - pd.DateOffset(years=max_years),\n",
    "                         pd.to_datetime(init_start))\n",
    "        if (train_end - train_start) < pd.Timedelta(days=365.25*min_years):\n",
    "            train_start = train_end - pd.DateOffset(years=min_years)\n",
    "        idx = full_data.index\n",
    "        subset = idx[(idx >= train_start) & (idx <= train_end)]\n",
    "        start_date, end_date = subset.min(), subset.max()\n",
    "        return start_date, end_date \n",
    "\n",
    "\n",
    "    fac_data = factor_data_dict[fac]\n",
    "    X   = fac_data[\"X\"]\n",
    "    ret = fac_data[\"fac_ret\"]\n",
    "    act = fac_data[\"active_ret\"]\n",
    "\n",
    "    lam = hyperparams[\"new_lambda\"]\n",
    "    kp  = hyperparams[\"new_kappa\"]\n",
    "    train_start, train_end = get_train_window(refit_date, X)\n",
    "\n",
    "\n",
    "    clipper = DataClipperStd(mul=3.0)\n",
    "    scaler  = StandardScaler()\n",
    "    X_train = scaler.fit_transform(clipper.fit_transform(\n",
    "                 filter_date_range(X, train_start, train_end)))\n",
    "    active_train = filter_date_range(act, train_start, train_end)\n",
    "\n",
    "\n",
    "    sjm = SparseJumpModel(n_components=2,\n",
    "                          max_feats=int(kp**2),\n",
    "                          jump_penalty=lam)\n",
    "    \n",
    "    train_idx = filter_date_range(X, train_start, train_end).index\n",
    "    X_train_df = pd.DataFrame(X_train, index=train_idx, columns=X.columns)\n",
    "    sjm.fit(X_train_df, ret_ser=active_train, sort_by=\"cumret\")\n",
    "\n",
    "    ret_train = filter_date_range(ret, train_start, train_end)\n",
    "\n",
    "\n",
    "    train_states = sjm.predict(X_train_df)\n",
    "    abs_ret = {}\n",
    "    act_ret = {}\n",
    "    for st in range(2):\n",
    "        st_idx = (train_states==st)\n",
    "        abs_ret[st] = ret_train.loc[st_idx].mean() * 252\n",
    "        act_ret[st] = active_train.loc[st_idx].mean() * 252 \n",
    "\n",
    "\n",
    "    states = {}\n",
    "    for day in test_dates_chunk:\n",
    "        X_hist = X.loc[:day]                        \n",
    "        temp_clipper = DataClipperStd(mul=3.0)\n",
    "        X_hist_clip  = temp_clipper.fit_transform(X_hist)\n",
    "\n",
    "        temp_scaler  = StandardScaler()\n",
    "        _ = temp_scaler.fit_transform(X_hist_clip)   \n",
    "\n",
    "        if day in X.index:\n",
    "            X_day_clip   = temp_clipper.transform(X.loc[[day]])\n",
    "            X_day_scaled = temp_scaler.transform(X_day_clip)\n",
    "            states[day]  = sjm.predict_online(\n",
    "                pd.DataFrame(X_day_scaled,\n",
    "                            index=[day],\n",
    "                            columns=X.columns)).iloc[0]\n",
    "\n",
    "\n",
    "    out = pd.DataFrame({\"state\": pd.Series(states)},\n",
    "                       index=list(states.keys()))\n",
    "\n",
    "\n",
    "    out[\"ann_abs_ret\"] = out[\"state\"].map(abs_ret)\n",
    "\n",
    "\n",
    "    out[\"s_0_act\"] = sjm.ret_[0]\n",
    "    out[\"s_1_act\"] = sjm.ret_[1]\n",
    "\n",
    "\n",
    "    out[\"s_0_abs_ret\"] = abs_ret.get(0, np.nan)\n",
    "    out[\"s_1_abs_ret\"] = abs_ret.get(1, np.nan)\n",
    "\n",
    "    return fac, out\n",
    "\n",
    "\n",
    "def build_factor_views(factor_data_dict, saved_hyperparams, factors,\n",
    "                       test_index,\n",
    "                       refit_freq=\"ME\", min_years=8, max_years=12,\n",
    "                       init_start=\"2002-05-31\"):\n",
    "\n",
    "    views = {f:[] for f in factors}\n",
    "    refit_dates = (test_index.to_series()\n",
    "                   .resample(refit_freq)\n",
    "                   .last()\n",
    "                   .dropna())\n",
    "\n",
    "    for j, refit_date in enumerate(refit_dates):\n",
    "        if j < len(refit_dates)-1:\n",
    "            next_refit = refit_dates.iloc[j+1]\n",
    "        else:\n",
    "            next_refit = test_index[-1]\n",
    "        test_mask = (test_index>refit_date)&(test_index<=next_refit)\n",
    "        test_chunk = test_index[test_mask]\n",
    "\n",
    "\n",
    "        jobs = []\n",
    "        for fac in factors:\n",
    "\n",
    "            hp_hist = [h for h in saved_hyperparams[fac]\n",
    "                       if pd.to_datetime(h[\"date\"])<=refit_date]\n",
    "            if not hp_hist: continue\n",
    "            hp = hp_hist[-1]\n",
    "            jobs.append(delayed(_fit_one_factor)(\n",
    "                fac, refit_date, test_chunk,\n",
    "                factor_data_dict, hp,\n",
    "                min_years, max_years, init_start))\n",
    "        for fac, df in Parallel(n_jobs=-1)(jobs):\n",
    "            views[fac].append(df)\n",
    "\n",
    "\n",
    "    for fac in factors:\n",
    "        views[fac] = (\n",
    "            pd.concat(views[fac])\n",
    "            .sort_index()\n",
    "            .loc[:, [\"state\",\n",
    "                    \"ann_abs_ret\",\n",
    "                    \"s_0_abs_ret\",\n",
    "                    \"s_1_abs_ret\",\n",
    "                    \"s_0_act\",\n",
    "                    \"s_1_act\"]]\n",
    "        )\n",
    "    return views\n",
    "\n",
    "\n",
    "   \n",
    "if FORCE_REBUILD or not os.path.exists(VIEWS_FILE):\n",
    "    factor_views = build_factor_views(factor_data_dict, saved_hyperparams, factors, \n",
    "                                      test_index,\n",
    "                                      refit_freq=REFIT_FREQ, \n",
    "                                      min_years=8, max_years=12, init_start=\"2002-05-31\")\n",
    "    with open(VIEWS_FILE, \"wb\") as f:\n",
    "        pickle.dump(factor_views, f)\n",
    "else:\n",
    "    with open(VIEWS_FILE, \"rb\") as f:\n",
    "        factor_views = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fac, df in factor_views.items():\n",
    "\n",
    "    df[[\"s_0_act\", \"s_1_act\"]] = df[[\"s_0_act\", \"s_1_act\"]] * 252\n",
    "\n",
    "\n",
    "    df[[\"s_0_act\", \"s_1_act\"]] = df[[\"s_0_act\", \"s_1_act\"]].clip(lower=-0.05, upper=0.05)\n",
    "\n",
    "\n",
    "    mask = df[\"s_0_act\"] < df[\"s_1_act\"]\n",
    "    df.loc[mask, [\"s_0_act\", \"s_1_act\"]] = df.loc[mask, [\"s_1_act\", \"s_0_act\"]].values\n",
    "\n",
    "\n",
    "    df[\"active_ret\"] = np.where(df[\"state\"] == 0, df[\"s_0_act\"], df[\"s_1_act\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f91d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VIEWS_FILE2 = \"views2.pkl\" \n",
    "FORCE_REBUILD = False \n",
    "\n",
    "def _fit_one_factor(fac, refit_date, test_dates_chunk,\n",
    "                    factor_data_dict, hyperparams,\n",
    "                    min_years, max_years, init_start):\n",
    "\n",
    "\n",
    "    def get_train_window(current_date, full_data):\n",
    "        train_end  = current_date\n",
    "        train_start= max(train_end - pd.DateOffset(years=max_years),\n",
    "                         pd.to_datetime(init_start))\n",
    "        if (train_end - train_start) < pd.Timedelta(days=365.25*min_years):\n",
    "            train_start = train_end - pd.DateOffset(years=min_years)\n",
    "        idx = full_data.index\n",
    "        subset = idx[(idx >= train_start) & (idx <= train_end)]\n",
    "        start_date, end_date = subset.min(), subset.max()\n",
    "        return start_date, end_date \n",
    "\n",
    "\n",
    "    fac_data = factor_data_dict[fac]\n",
    "    X   = fac_data[\"X\"]\n",
    "    ret = fac_data[\"fac_ret\"]\n",
    "    act = fac_data[\"active_ret\"]\n",
    "\n",
    "    lam = hyperparams[\"new_lambda\"]\n",
    "    kp  = hyperparams[\"new_kappa\"]\n",
    "    train_start, train_end = get_train_window(refit_date, X)\n",
    "\n",
    "\n",
    "    clipper = DataClipperStd(mul=3.0)\n",
    "    scaler  = StandardScaler()\n",
    "    X_train = scaler.fit_transform(clipper.fit_transform(\n",
    "                 filter_date_range(X, train_start, train_end)))\n",
    "    active_train = filter_date_range(act, train_start, train_end)\n",
    "\n",
    "\n",
    "    sjm = SparseJumpModel(n_components=2,\n",
    "                          max_feats=int(kp**2),\n",
    "                          jump_penalty=lam)\n",
    "    \n",
    "    train_idx = filter_date_range(X, train_start, train_end).index\n",
    "    X_train_df = pd.DataFrame(X_train, index=train_idx, columns=X.columns)\n",
    "    sjm.fit(X_train_df, ret_ser=active_train, sort_by=\"cumret\")\n",
    "\n",
    "    ret_train = filter_date_range(ret, train_start, train_end)\n",
    "\n",
    "\n",
    "    train_states = sjm.predict(X_train_df)\n",
    "    abs_ret = {}\n",
    "    for st in range(2):\n",
    "        st_idx = (train_states==st)\n",
    "        abs_ret[st] = ret_train.loc[st_idx].mean() * 252\n",
    "\n",
    "\n",
    "    states = {}\n",
    "    for day in test_dates_chunk:\n",
    "        X_hist = X.loc[:day]                         \n",
    "        temp_clipper = DataClipperStd(mul=3.0)\n",
    "        X_hist_clip  = temp_clipper.fit_transform(X_hist)\n",
    "\n",
    "        temp_scaler  = StandardScaler()\n",
    "        _ = temp_scaler.fit_transform(X_hist_clip)    \n",
    "\n",
    "        if day in X.index:\n",
    "            X_day_clip   = temp_clipper.transform(X.loc[[day]])\n",
    "            X_day_scaled = temp_scaler.transform(X_day_clip)\n",
    "            states[day]  = sjm.predict_online(\n",
    "                pd.DataFrame(X_day_scaled,\n",
    "                            index=[day],\n",
    "                            columns=X.columns)).iloc[0]\n",
    "\n",
    "\n",
    "    out = pd.DataFrame({\"state\": pd.Series(states)},\n",
    "                       index=list(states.keys()))\n",
    "    out[\"ann_abs_ret\"] = out[\"state\"].map(abs_ret)\n",
    "    return fac, out\n",
    "\n",
    "def build_factor_views(factor_data_dict, saved_hyperparams, factors,\n",
    "                       test_index,\n",
    "                       refit_freq=\"ME\", min_years=8, max_years=12,\n",
    "                       init_start=\"2002-05-31\"):\n",
    "\n",
    "    views = {f:[] for f in factors}\n",
    "    refit_dates = (test_index.to_series()\n",
    "                   .resample(refit_freq)\n",
    "                   .last()\n",
    "                   .dropna())\n",
    "\n",
    "    for j, refit_date in enumerate(refit_dates):\n",
    "        if j < len(refit_dates)-1:\n",
    "            next_refit = refit_dates.iloc[j+1]\n",
    "        else:\n",
    "            next_refit = test_index[-1]\n",
    "        test_mask = (test_index>refit_date)&(test_index<=next_refit)\n",
    "        test_chunk = test_index[test_mask]\n",
    "\n",
    "\n",
    "        jobs = []\n",
    "        for fac in factors:\n",
    "\n",
    "            hp_hist = [h for h in saved_hyperparams[fac]\n",
    "                       if pd.to_datetime(h[\"date\"])<=refit_date]\n",
    "            if not hp_hist: continue\n",
    "            hp = hp_hist[-1]\n",
    "            jobs.append(delayed(_fit_one_factor)(\n",
    "                fac, refit_date, test_chunk,\n",
    "                factor_data_dict, hp,\n",
    "                min_years, max_years, init_start))\n",
    "        for fac, df in Parallel(n_jobs=-1)(jobs):\n",
    "            views[fac].append(df)\n",
    "\n",
    "\n",
    "    for fac in factors:\n",
    "        views[fac] = (pd.concat(views[fac])\n",
    "                      .sort_index()\n",
    "                      .loc[:,[\"state\",\"ann_abs_ret\"]])\n",
    "    return views\n",
    "\n",
    "\n",
    "    \n",
    "if FORCE_REBUILD or not os.path.exists(VIEWS_FILE2):\n",
    "    factor_views_risk_off = build_factor_views(factor_data_dict, saved_hyperparams, factors, \n",
    "                                      test_index,\n",
    "                                      refit_freq=REFIT_FREQ, \n",
    "                                      min_years=8, max_years=12, init_start=\"2002-05-31\")\n",
    "    with open(VIEWS_FILE2, \"wb\") as f:\n",
    "        pickle.dump(factor_views_risk_off, f)\n",
    "else:\n",
    "    with open(VIEWS_FILE2, \"rb\") as f:\n",
    "        factor_views_risk_off = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt.exceptions import OptimizationError\n",
    "\n",
    "def ewm_covariance(returns, halflife=126, min_periods=60):\n",
    "    ewm_cov = returns.ewm(halflife=halflife,\n",
    "                          adjust=False,\n",
    "                          min_periods=min_periods).cov()\n",
    "    if returns.empty: return pd.DataFrame()\n",
    "    return ewm_cov.loc[returns.index[-1]]\n",
    "\n",
    "def detect_state_shifts(views, factors):\n",
    "\n",
    "    state_df = pd.concat({f: views[f][\"state\"] for f in factors}, axis=1)\n",
    "\n",
    "    return state_df.ne(state_df.shift()).any(axis=1)\n",
    "\n",
    "\n",
    "def make_relative_views(view_dict, assets, benchmark=\"Market\"):\n",
    "\n",
    "    if benchmark not in assets:\n",
    "        raise ValueError(f\"Benchmark {benchmark!r} not in trade universe\")\n",
    "\n",
    "    n = len(assets)\n",
    "    k = len(view_dict)\n",
    "    P = np.zeros((k, n))\n",
    "    Q = np.zeros(k)\n",
    "\n",
    "    for i, (fac, v) in enumerate(view_dict.items()):\n",
    "        if fac not in assets:\n",
    "            continue                    \n",
    "        P[i, assets.index(fac)]       = 1\n",
    "        P[i, assets.index(benchmark)] = -1\n",
    "        Q[i] = v             \n",
    "    return P, Q\n",
    "\n",
    "\n",
    "def bl_max_sharpe_te(cov_hist, pi, views, tau, delta,\n",
    "                     w_bmk, te_target, bounds, rf,\n",
    "                     use_bl_cov=False):\n",
    "\n",
    "\n",
    "\n",
    "    P, Q = make_relative_views(views, list(w_bmk.index), benchmark=\"Market\")\n",
    "    bl = BlackLittermanModel(cov_hist, pi=pi, tau=tau,\n",
    "                             delta=delta, P=P, Q=Q)\n",
    "    Sigma = bl.bl_cov() if use_bl_cov else cov_hist\n",
    "    mu    = bl.bl_returns()\n",
    "    n     = len(mu)\n",
    "    bmk_w = w_bmk.values\n",
    "    Σ     = Sigma.values\n",
    "\n",
    "\n",
    "    ef = EfficientFrontier(mu, Sigma)\n",
    "    ef.add_constraint(lambda w: cp.quad_form(w - bmk_w, Σ) <= te_target**2)\n",
    "\n",
    "    for i, (lo, hi) in enumerate(bounds):\n",
    "        ef.add_constraint(lambda w, i=i, lo=lo: w[i] >= lo)\n",
    "        ef.add_constraint(lambda w, i=i, hi=hi: w[i] <= hi)\n",
    "\n",
    "\n",
    "    try:\n",
    "        raw_w = ef.max_sharpe(risk_free_rate=rf)\n",
    "    except (OptimizationError, ValueError):\n",
    "        w_var = cp.Variable(n)\n",
    "        w_act = w_var - bmk_w\n",
    "        prob = cp.Problem(\n",
    "            cp.Maximize((mu.values - rf) @ w_var),\n",
    "            [\n",
    "                cp.sum(w_var) == 1,                                       \n",
    "                w_var >= np.array([lo for lo, hi in bounds]),\n",
    "                w_var <= np.array([hi for lo, hi in bounds]),\n",
    "                cp.quad_form(w_act, Σ) <= te_target**2\n",
    "            ]\n",
    "        )\n",
    "        prob.solve(solver=\"SCS\")\n",
    "        raw_w = dict(zip(w_bmk.index, w_var.value))\n",
    "\n",
    "    return pd.Series(raw_w, index=w_bmk.index)\n",
    "\n",
    "\n",
    "def run_bl_with_drift(views, returns_df, full_df, riskoff_views,\n",
    "                      shift_series=None,\n",
    "                      tau=0.05, delta=2.5,\n",
    "                      te_target=0.05,\n",
    "                      trade_market=True,\n",
    "                      use_bl_cov=False,\n",
    "                      allow_market_short=False,\n",
    "                      allow_factor_short=False,\n",
    "                      use_bl_prior=False,\n",
    "                      fallback_strategy=\"HOLD_RFR\",\n",
    "                      tcost=0.0007,\n",
    "                      initial_capital=1_000_000,\n",
    "                      shifts = 0):\n",
    "\n",
    "\n",
    "    views = {fac: df.shift(shifts) for fac, df in views.items()}\n",
    "    assets  = returns_df.columns.tolist()\n",
    "    factors = list(views.keys())\n",
    "\n",
    "\n",
    "    state_df = pd.concat({f: views[f][\"state\"] for f in factors}, axis=1)\n",
    "    per_factor_shifts = (\n",
    "        state_df.ne(state_df.shift())           \n",
    "        .reindex(returns_df.index, fill_value=False)\n",
    "    )\n",
    "    per_factor_shifts.iloc[0, :] = True        \n",
    "\n",
    "\n",
    "    prev_active_ret = {\n",
    "        fac: views[fac].loc[returns_df.index[0], \"active_ret\"]\n",
    "        for fac in factors\n",
    "    }\n",
    "\n",
    "    if trade_market:\n",
    "        trade_assets = [a for a in assets if a != \"rf\"]\n",
    "    else:\n",
    "        trade_assets = [a for a in assets if a not in {\"rf\", \"Market\"}]\n",
    "\n",
    "    cash_asset = \"rf\"\n",
    "\n",
    "\n",
    "    bounds = []\n",
    "    for a in trade_assets:\n",
    "        if a == \"Market\":\n",
    "            bounds.append((-.3, .3) if allow_market_short else (0, 0.3))\n",
    "        else:\n",
    "            bounds.append((-.3, .3) if allow_factor_short else (0, 0.3))\n",
    "\n",
    "\n",
    "    w_view   = pd.DataFrame(0.0, index=returns_df.index, columns=trade_assets + [cash_asset], dtype=float)\n",
    "    w_actual = pd.DataFrame(0.0, index=returns_df.index, columns=trade_assets + [cash_asset], dtype=float)\n",
    "\n",
    "\n",
    "    positions = pd.Series(0.0, index=trade_assets + [cash_asset])\n",
    "    capital   = initial_capital\n",
    "\n",
    "\n",
    "    rets = pd.Series(0.0, index=returns_df.index)\n",
    "\n",
    "    for i, t in enumerate(returns_df.index):\n",
    "        old_capital = capital \n",
    "        yesterday = t - pd.Timedelta(days=1)\n",
    "        if fallback_strategy==\"HOLD_RFR\" and yesterday in rf_dates:\n",
    "\n",
    "            w_view.loc[t, trade_assets] = 0.0\n",
    "            w_view.loc[t, cash_asset]   = 1.0\n",
    "            positions = capital * w_view.loc[t]\n",
    "            w_actual.loc[t] = positions / capital\n",
    "            rets.loc[t]     = 0.0 \n",
    "            continue\n",
    "\n",
    "\n",
    "        if i > 0:\n",
    "            day_ret = returns_df.loc[t, trade_assets + [cash_asset]]\n",
    "            daily_pnl = (positions[trade_assets] * day_ret[trade_assets]).sum() \\\n",
    "                        + positions[cash_asset] * day_ret[cash_asset]\n",
    "            \n",
    "\n",
    "            short_dollar = positions[trade_assets].clip(upper=0).abs().sum()\n",
    "            borrw_r_daily = .004 / 360 \n",
    "            borrow_cost  = short_dollar * borrw_r_daily\n",
    "\n",
    "\n",
    "            net_pnl     = daily_pnl - borrow_cost\n",
    "            capital    += net_pnl\n",
    "\n",
    "\n",
    "            rets.loc[t] = net_pnl / old_capital\n",
    "\n",
    "\n",
    "\n",
    "        do_rebalance = False\n",
    "        if i == 0:\n",
    "            do_rebalance = True\n",
    "        elif shift_series is not None and shift_series.loc[t]:\n",
    "            do_rebalance = True\n",
    "\n",
    "\n",
    "        if do_rebalance:\n",
    "            hist = full_df[trade_assets].loc[:t].iloc[:-1]\n",
    "            cov  = ewm_covariance(hist) * 252\n",
    "\n",
    "            if cov.empty or cov.isna().any().any():\n",
    "                w_view.loc[t, trade_assets] = 0.0\n",
    "                w_view.loc[t, cash_asset]   = 1.0  \n",
    "            else:\n",
    "\n",
    "                if use_bl_prior:\n",
    "                    market_caps  = {etf: 1.0 for etf in trade_assets}\n",
    "                    prior_for_bl = market_implied_prior_returns(market_caps, delta, cov)\n",
    "                else:\n",
    "                    prior_for_bl = \"equal\"\n",
    "\n",
    "                q = {}\n",
    "                for fac in factors:\n",
    "                    if per_factor_shifts.loc[t, fac]:\n",
    "                        q_val = views[fac].loc[t, \"active_ret\"]\n",
    "                    else:\n",
    "                        q_val = prev_active_ret[fac]\n",
    "                    q[fac] = q_val\n",
    "                    prev_active_ret[fac] = q_val\n",
    "                rf_annual = returns_df.loc[t, cash_asset] * 252\n",
    "\n",
    "\n",
    "                q_abs = {fac: riskoff_views[fac].loc[t, \"ann_abs_ret\"] for fac in factors}\n",
    "\n",
    "                bl0 = BlackLittermanModel(\n",
    "                    cov,\n",
    "                    pi=prior_for_bl,\n",
    "                    tau=tau,\n",
    "                    delta=delta,\n",
    "                    absolute_views=q_abs\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "                if (\n",
    "                    fallback_strategy != \"NO_FALLBACK\"\n",
    "                    and (bl0.bl_returns() <= rf_annual).all()\n",
    "                ):\n",
    "\n",
    "                    w_view.loc[t, trade_assets] = 0.0  \n",
    "                    if fallback_strategy == \"HOLD_RFR\":\n",
    "                        w_view.loc[t, cash_asset] = 1.0  \n",
    "                    elif fallback_strategy == \"SHORT_MARKET\" and \"Market\" in trade_assets:\n",
    "                        w_view.loc[t, \"Market\"]   = -1.0\n",
    "                        w_view.loc[t, cash_asset] = 1.0\n",
    "\n",
    "                else:\n",
    "\n",
    "                    w_opt = bl_max_sharpe_te(\n",
    "                        cov,\n",
    "                        pi=prior_for_bl,\n",
    "                        views=q,\n",
    "                        tau=tau,\n",
    "                        delta=delta,\n",
    "                        w_bmk=pd.Series(1 / len(trade_assets), index=trade_assets),\n",
    "                        te_target=te_target,\n",
    "                        bounds=bounds,\n",
    "                        rf=rf_annual,\n",
    "                        use_bl_cov=use_bl_cov\n",
    "                    )\n",
    "                    w_view.loc[t, trade_assets] = w_opt\n",
    "                    w_view.loc[t, cash_asset]   = 1 - w_opt.sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            new_positions = capital * w_view.loc[t]\n",
    "\n",
    "\n",
    "            if tcost > 0 and i > 0:\n",
    "                asset_trades = (new_positions[trade_assets] - positions[trade_assets]).abs().sum()\n",
    "                cost        = asset_trades * tcost\n",
    "                capital    -= cost\n",
    "                rets.loc[t] = (capital - old_capital) / old_capital\n",
    "\n",
    "            positions = new_positions\n",
    "\n",
    "        else:\n",
    "\n",
    "            if i > 0:\n",
    "                w_view.loc[t] = w_view.iloc[i - 1]\n",
    "\n",
    "\n",
    "        if capital > 0:\n",
    "            w_actual.loc[t] = positions / capital\n",
    "        else:\n",
    "            w_actual.loc[t] = 0.0\n",
    "\n",
    "    return w_view, w_actual, rets, shift_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15935178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/cvxpy/problems/problem.py:1504: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def annualized_sharpe(r):        \n",
    "    return (r.mean() / r.std()) * np.sqrt(252)\n",
    "\n",
    "def ann_turnover(w):\n",
    "    daily_turn = w.diff().abs().sum(axis=1).mean()\n",
    "    return daily_turn * 252\n",
    "\n",
    "trading_lag = 2\n",
    "\n",
    "test_df = full_df.loc[test_index]\n",
    "shift_days = detect_state_shifts(factor_views, factors)\n",
    "\n",
    "shift_days = shift_days.shift(trading_lag).reindex(test_df.index, fill_value=False)\n",
    "shift_days.iloc[0] = True\n",
    "\n",
    "cfgs = [\n",
    "\n",
    "     dict(label=\"Base model 4% TE, 0.25 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(0, 0.25),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=False, allow_factor_short=False, te_target=0.04, fallback_strategy=\"NO_FALLBACK\"),\n",
    "     dict(label=\"Base model 4% TE, 0.30 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(0, 0.30),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=False, allow_factor_short=False, te_target=0.04, fallback_strategy=\"NO_FALLBACK\"),        \n",
    "     dict(label=\"Base model 4% TE, 0.50 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(0, 0.5),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=False, allow_factor_short=False, te_target=0.04, fallback_strategy=\"NO_FALLBACK\"),\n",
    "     dict(label=\"Base model 4% TE, 1.00 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(0, 1),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=False, allow_factor_short=False, te_target=0.04, fallback_strategy=\"NO_FALLBACK\"),\n",
    "\n",
    "\n",
    " \n",
    "     dict(label=\"L/S 4% TE, 0.25 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(-0.25, 0.25),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=True, allow_factor_short=True, te_target=0.04, fallback_strategy=\"NO_FALLBACK\"),\n",
    "     dict(label=\"L/S 4% TE, 0.30 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(-0.30, 0.30),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=True, allow_factor_short=True, te_target=0.04, fallback_strategy=\"NO_FALLBACK\"),\n",
    "     dict(label=\"L/S 4% TE, 0.50 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(-0.5, 0.5),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=True, allow_factor_short=True, te_target=0.04, fallback_strategy=\"NO_FALLBACK\"),\n",
    "     dict(label=\"L/S 4% TE, 1.00 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(-1, 1),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=True, allow_factor_short=True, te_target=0.04, fallback_strategy=\"NO_FALLBACK\"),\n",
    "\n",
    "\n",
    "   \n",
    "     dict(label=\"Risk-off 4% TE, 0.25 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(0, 0.25),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=False, allow_factor_short=False, te_target=0.04, fallback_strategy=\"HOLD_RFR\"),\n",
    "     dict(label=\"Risk-off 4% TE, 0.30 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(0, 0.30),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=False, allow_factor_short=False, te_target=0.04, fallback_strategy=\"HOLD_RFR\"), \n",
    "     dict(label=\"Risk-off 4% TE, 0.50 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(0, 0.5),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=False, allow_factor_short=False, te_target=0.04, fallback_strategy=\"HOLD_RFR\"),\n",
    "     dict(label=\"Risk-off 4% TE, 1.00 bound\",  tau=0.05, delta=2.5,shift_series=shift_days, trade_market=True, bounds_override=(0, 1),\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=False, allow_factor_short=False, te_target=0.04, fallback_strategy=\"HOLD_RFR\"),     \n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "run_results = {}\n",
    "for c in cfgs:\n",
    "    label = c.pop(\"label\")\n",
    "    w_view, w_act, daily_returns, flags = run_bl_with_drift(\n",
    "        factor_views,  \n",
    "        test_df,       \n",
    "        full_df,       \n",
    "        riskoff_views=factor_views_risk_off,\n",
    "        shifts=trading_lag,\n",
    "        **c\n",
    "    )\n",
    "    \n",
    "    run_results[label] = {\n",
    "        \"returns\":        daily_returns,\n",
    "        \"weights_view\":   w_view,\n",
    "        \"weights_actual\": w_act,\n",
    "        \"trade_flags\":    flags,\n",
    "        \"cfg\":            c\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ec996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4341afa057b4d5486ef52893cdb50ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='Compare:', index=(0, 1), options=('Base model 4% TE, 0.25 bo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Strategy  Ann.Return  Ann.Ex.Return  Ann.Ex.Risk  Ex.Sharpe  MaxDD  Sortino  Calmar  InfoRatio  Turnover\n",
      "Base model 4% TE, 0.25 bound       0.123          0.099        0.189      0.595  0.325    0.558   0.380      0.564     5.180\n",
      "Base model 4% TE, 0.30 bound       0.123          0.099        0.188      0.596  0.317    0.561   0.389      0.537     7.172\n",
      "Base model 4% TE, 0.50 bound       0.117          0.093        0.187      0.568  0.312    0.536   0.375      0.306    10.596\n",
      "Base model 4% TE, 1.00 bound       0.108          0.084        0.188      0.524  0.325    0.500   0.333      0.084    12.603\n",
      "       L/S 4% TE, 0.25 bound       0.115          0.091        0.193      0.549  0.331    0.518   0.348      0.293     7.298\n",
      "       L/S 4% TE, 0.30 bound       0.116          0.091        0.191      0.555  0.322    0.524   0.360      0.271     9.967\n",
      "       L/S 4% TE, 0.50 bound       0.113          0.089        0.187      0.550  0.306    0.520   0.370      0.179    15.922\n",
      "       L/S 4% TE, 1.00 bound       0.103          0.079        0.186      0.502  0.308    0.478   0.334     -0.050    22.313\n",
      "  Risk-off 4% TE, 0.25 bound       0.086          0.063        0.119      0.570  0.176    0.575   0.491     -0.190     7.159\n",
      "  Risk-off 4% TE, 0.30 bound       0.086          0.062        0.119      0.565  0.172    0.570   0.497     -0.194     8.424\n",
      "  Risk-off 4% TE, 0.50 bound       0.082          0.058        0.120      0.534  0.167    0.535   0.491     -0.216    10.287\n",
      "  Risk-off 4% TE, 1.00 bound       0.080          0.056        0.120      0.517  0.167    0.524   0.479     -0.228    11.349\n",
      " Benchmark (Market Buy-Hold)       0.106          0.082        0.189      0.512  0.350    0.477   0.304      0.075     0.000\n",
      " Benchmark (Q-Rebalanced EW)       0.106          0.082        0.184      0.520  0.349    0.484   0.303        NaN     0.041\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import SelectMultiple, Dropdown, ToggleButtons, VBox, HBox, interact\n",
    "\n",
    "import importlib, jumpmodels.plot as jmplot\n",
    "importlib.reload(jmplot)\n",
    "from jumpmodels.plot import plot_cumret_compounded\n",
    "\n",
    "\n",
    "strategy_labels = {\n",
    "    \"Benchmark (Q-Rebalanced EW)\": \"Benchmark (Q-Rebalanced EW)\",\n",
    "    \"Benchmark (Market Buy-Hold)\": \"Benchmark (Market Buy-Hold)\",\n",
    "    \"Base model 1% TE\": \"Long-Only (1 per cent TE)\",\n",
    "    \"Base model 2% TE\": \"Long-Only (2 per cent TE)\",\n",
    "    \"Base model 3% TE\": \"Long-Only (3 per cent TE)\",\n",
    "    \"Base model 4% TE\": \"Long-Only (4 per cent TE)\",\n",
    "    \"L/S 1% TE\": \"Long-Short (1 per cent TE)\",\n",
    "    \"L/S 2% TE\": \"Long-Short (2 per cent TE)\",\n",
    "    \"L/S 3% TE\": \"Long-Short (3 per cent TE)\",\n",
    "    \"L/S 4% TE\": \"Long-Short (4 per cent TE)\",\n",
    "    \"Risk-off 1% TE\": \"Long-Only with Risk-Off (1 per cent TE)\",\n",
    "    \"Risk-off 2% TE\": \"Long-Only with Risk-Off (2 per cent TE)\",\n",
    "    \"Risk-off 3% TE\": \"Long-Only with Risk-Off (3 per cent TE)\",\n",
    "    \"Risk-off 4% TE\": \"Long-Only with Risk-Off (4 per cent TE)\",\n",
    "}\n",
    "\n",
    "run_results = {\n",
    "    strategy_labels.get(orig, orig): vals\n",
    "    for orig, vals in run_results.items()\n",
    "}\n",
    "\n",
    "factor_labels = {\n",
    "    \"iwf\":  \"IWF (Growth factor)\",\n",
    "    \"mtum\": \"MTUM (Momentum factor)\",\n",
    "    \"qual\": \"QUAL (Quality factor)\",\n",
    "    \"size\": \"SIZE (Size factor)\",\n",
    "    \"usmv\": \"USMV (Low-volatility factor)\",\n",
    "    \"vlue\": \"VLUE (Value factor)\",\n",
    "    \"Market\": \"Market\",\n",
    "    \"rf\":     \"Risk-free\"\n",
    "}\n",
    "\n",
    "\n",
    "full_idx = test_index\n",
    "\n",
    "\n",
    "for k in list(run_results):\n",
    "    if k.startswith(\"Benchmark (Q-Rebalanced EW)\"):\n",
    "        run_results.pop(k)\n",
    "\n",
    "\n",
    "TCOST = 0.0007                   \n",
    "df     = etf_df.loc[test_index.intersection(etf_df.index)].drop(columns=[\"rf\"])\n",
    "n      = df.shape[1]\n",
    "\n",
    "\n",
    "first  = df.index[0]\n",
    "qends  = df.index.to_series().resample(\"QE\").last().dropna().index\n",
    "if first not in qends:\n",
    "    qends = qends.insert(0, first)\n",
    "\n",
    "\n",
    "ew_view   = pd.DataFrame(index=df.index, columns=df.columns, dtype=float)   \n",
    "ew_actual = pd.DataFrame(index=df.index, columns=df.columns, dtype=float)   \n",
    "ew_rets   = pd.Series(index=df.index, dtype=float)\n",
    "\n",
    "prev_w     = np.zeros(n)      \n",
    "trade_days = []                \n",
    "\n",
    "\n",
    "for i in range(len(qends) - 1):\n",
    "    start, end = qends[i], qends[i + 1]\n",
    "    period = df.loc[(df.index > start) & (df.index <= end)]\n",
    "    if period.empty:\n",
    "        continue\n",
    "\n",
    "\n",
    "    first_day = period.index[0]\n",
    "    w_target  = np.ones(n) / n                    \n",
    "    tc        = TCOST * np.abs(w_target - prev_w).sum()\n",
    "    r0        = df.loc[first_day].values\n",
    "    ew_rets.loc[first_day] = w_target.dot(r0) - tc\n",
    "\n",
    "    ew_view.loc[first_day]   = w_target\n",
    "    ew_actual.loc[first_day] = w_target\n",
    "    trade_days.append(first_day)\n",
    "\n",
    "    w = w_target.copy()                            \n",
    "\n",
    "\n",
    "    for day in period.index[1:]:\n",
    "        r_i = df.loc[day].values\n",
    "        w   = w * (1 + r_i)              \n",
    "        w  /= w.sum()                          \n",
    "\n",
    "        ew_view.loc[day]   = w_target            \n",
    "        ew_actual.loc[day] = w              \n",
    "        ew_rets.loc[day]   = w.dot(r_i)\n",
    "\n",
    "    prev_w = w.copy()\n",
    "\n",
    "\n",
    "ew_view   = ew_view.ffill().fillna(1 / n)\n",
    "ew_actual = ew_actual.ffill().fillna(1 / n)\n",
    "ew_rets   = ew_rets.fillna(0)\n",
    "\n",
    "\n",
    "ew_flags = pd.Series(False, index=full_idx)\n",
    "ew_flags.loc[[d for d in trade_days if d in ew_flags.index]] = True\n",
    "\n",
    "\n",
    "run_results[\"Benchmark (Q-Rebalanced EW)\"] = {\n",
    "    \"returns\":        ew_rets.reindex(full_idx).fillna(0),\n",
    "    \"weights_view\":   ew_view.reindex(full_idx).ffill(),\n",
    "    \"weights_actual\": ew_actual.reindex(full_idx).ffill(),\n",
    "    \"trade_flags\":    ew_flags,\n",
    "}\n",
    "\n",
    "\n",
    "market_rets = etf_df[\"Market\"].loc[test_index.intersection(etf_df.index)]\n",
    "market_w    = pd.DataFrame(index=market_rets.index, columns=[\"Market\"])\n",
    "market_w[\"Market\"] = 1.0\n",
    "\n",
    "market_flags = pd.Series(False, index=full_idx)\n",
    "market_flags.iloc[0] = True\n",
    "\n",
    "run_results[\"Benchmark (Market Buy-Hold)\"] = {\n",
    "    \"returns\":        market_rets.reindex(full_idx).fillna(0),\n",
    "    \"weights_view\":   market_w.reindex(full_idx).ffill().fillna(0),\n",
    "    \"weights_actual\": market_w.reindex(full_idx).ffill().fillna(0),\n",
    "    \"trade_flags\":    market_flags,     \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for lab, res in run_results.items():\n",
    "\n",
    "    res[\"returns\"] = res[\"returns\"].reindex(full_idx).fillna(0)\n",
    "    \n",
    "\n",
    "    if \"weights_view\" in res:\n",
    "        res[\"weights_view\"] = res[\"weights_view\"].reindex(full_idx).ffill().fillna(0)\n",
    "    if \"weights_actual\" in res:\n",
    "        res[\"weights_actual\"] = res[\"weights_actual\"].reindex(full_idx).ffill().fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "active = pd.Index([])\n",
    "for cfg in run_results.values():\n",
    "\n",
    "    wdf = cfg.get(\"weights\")\n",
    "    if wdf is not None:\n",
    "        idx = wdf.drop(columns=[\"rf\"], errors=\"ignore\").dropna(how=\"all\").index\n",
    "        active = active.union(idx)\n",
    "\n",
    "    wv  = cfg.get(\"weights_view\")\n",
    "    wa  = cfg.get(\"weights_actual\")\n",
    "    if wv is not None:\n",
    "        idx = wv.drop(columns=[\"rf\"], errors=\"ignore\").dropna(how=\"all\").index\n",
    "        active = active.union(idx)\n",
    "    if wa is not None:\n",
    "        idx = wa.drop(columns=[\"rf\"], errors=\"ignore\").dropna(how=\"all\").index\n",
    "        active = active.union(idx)\n",
    "\n",
    "active = active.sort_values()\n",
    "ew_w = ew_actual.loc[ew_actual.index.isin(active)]\n",
    "\n",
    "\n",
    "labels = list(run_results.keys())\n",
    "\n",
    "cmp = SelectMultiple(\n",
    "    options=labels,\n",
    "    value=tuple(labels[:2]) if len(labels) >= 2 else tuple(labels),\n",
    "    description=\"Compare:\",\n",
    "    rows=min(8, len(labels)),\n",
    "    style={\"description_width\": \"70px\"}\n",
    ")\n",
    "wgt = Dropdown(\n",
    "    options=labels,\n",
    "    value=labels[0],\n",
    "    description=\"Weights:\",\n",
    "    style={\"description_width\": \"70px\"}\n",
    ")\n",
    "sign = ToggleButtons(\n",
    "    options=[(\"Both\", \"both\"), (\"Positive\", \"pos\"), (\"Negative\", \"neg\")],\n",
    "    value=\"both\",\n",
    "    description=\"Show:\",\n",
    "    style={\"description_width\": \"70px\"}\n",
    ")\n",
    "\n",
    "def sharpe(returns):\n",
    "    return returns.mean() / returns.std() * np.sqrt(252) if returns.std() != 0 else np.nan\n",
    "\n",
    "def _update(compare, weights_cfg, sign_filter):\n",
    "    if not compare:\n",
    "        print(\"Pick ≥1 config.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    benchmarks = []\n",
    "    for bench_name in [\"Benchmark (Q-Rebalanced EW)\", \"Benchmark (Market Buy-Hold)\"]:\n",
    "        if bench_name in run_results:\n",
    "            benchmarks.append(bench_name)\n",
    "\n",
    "\n",
    "    data = {}\n",
    "    for bmk in benchmarks:\n",
    "        data[bmk] = run_results[bmk][\"returns\"]\n",
    "    for lab in compare:\n",
    "        data[lab] = run_results[lab][\"returns\"]\n",
    "\n",
    "    ret_df = pd.DataFrame(data).reindex(full_idx, fill_value=0)\n",
    "\n",
    "\n",
    "    plot_cumret_compounded(ret_df)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    labels = [lab.replace(\"%\", \"%%\") for lab in labels]\n",
    "    plt.legend(handles, labels)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    res = run_results[weights_cfg]\n",
    "    wv = res.get(\"weights_view\")\n",
    "    wa = res.get(\"weights_actual\")\n",
    "\n",
    "\n",
    "    wv = wv.rename(columns=factor_labels)\n",
    "    wa = wa.rename(columns=factor_labels)\n",
    "\n",
    "\n",
    "    if wv is not None and wa is not None:\n",
    "\n",
    "        if sign_filter == \"pos\":\n",
    "            wv = wv.where(wv > 0, 0)\n",
    "            wa = wa.where(wa > 0, 0)\n",
    "        elif sign_filter == \"neg\":\n",
    "            wv = wv.where(wv < 0, 0)\n",
    "            wa = wa.where(wa < 0, 0)\n",
    "\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 6))\n",
    "        ax.stackplot(wv.index, wv.T.values, labels=wv.columns)\n",
    "        ax.set_ylabel(\"Weight\", fontsize=30)\n",
    "        ax.tick_params(axis='both', labelsize=30)\n",
    "\n",
    "\n",
    "        leg = ax.legend(\n",
    "            loc='center left',\n",
    "            bbox_to_anchor=(1.02, 0.5),  \n",
    "            fontsize=12,\n",
    "            frameon=True,\n",
    "            fancybox=True\n",
    "        )\n",
    "        leg.get_frame().set_facecolor('white')\n",
    "        leg.get_frame().set_edgecolor('black')\n",
    "        leg.get_frame().set_alpha(0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 6))\n",
    "        ax.stackplot(wa.index, wa.T.values, labels=wa.columns)\n",
    "        ax.set_ylabel(\"Weight\", fontsize=30)\n",
    "        ax.tick_params(axis='both', labelsize=30)\n",
    "\n",
    "\n",
    "        leg = ax.legend(\n",
    "            loc='center left',\n",
    "            bbox_to_anchor=(1.02, 0.5),\n",
    "            fontsize=12,\n",
    "            frameon=True,\n",
    "            fancybox=True\n",
    "        )\n",
    "        leg.get_frame().set_facecolor('white')\n",
    "        leg.get_frame().set_edgecolor('black')\n",
    "        leg.get_frame().set_alpha(0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ui = VBox([HBox([cmp, wgt, sign])])\n",
    "interact(_update, compare=cmp, weights_cfg=wgt, sign_filter=sign)\n",
    "\n",
    "\n",
    "\n",
    "def ann_return(r):\n",
    "    total = (1 + r).prod()\n",
    "    n = r.shape[0]\n",
    "    return total**(252/n) - 1 if total>0 else np.nan\n",
    "\n",
    "def ann_volatility(r):\n",
    "    return r.std() * np.sqrt(252)\n",
    "\n",
    "def max_drawdown(r):\n",
    "    cum = (1 + r).cumprod()\n",
    "    peak = cum.cummax()\n",
    "    dd = cum / peak - 1\n",
    "    return -dd.min()\n",
    "\n",
    "def sortino(r, mar=0.0):\n",
    "    downside = r[r < 0]\n",
    "    if downside.empty:\n",
    "        return np.nan\n",
    "    downside_deviation = np.sqrt((downside ** 2).mean())\n",
    "    return r.mean() / downside_deviation * np.sqrt(252) if downside_deviation > 0 else np.nan\n",
    "\n",
    "def calmar(r):\n",
    "    ar = ann_return(r)\n",
    "    dd = max_drawdown(r)\n",
    "    return ar / dd if dd>0 else np.nan\n",
    "\n",
    "def information_ratio(series, bench):\n",
    "    a, b = series.align(bench, join=\"inner\")\n",
    "    active = a - b\n",
    "    return sharpe(active)\n",
    "\n",
    "def total_turnover(weights):\n",
    "    w = weights.ffill().fillna(0)\n",
    "    daily_t = w.diff().abs().sum(axis=1)\n",
    "    return daily_t.mean() * 252\n",
    "\n",
    "\n",
    "\n",
    "def ann_turnover(weights, trade_flags=None):\n",
    "\n",
    "    w = weights.ffill().fillna(0)\n",
    "    daily = 0.5 * w.diff().abs().sum(1)          \n",
    "    if trade_flags is not None:\n",
    "        daily = daily.where(trade_flags, 0)     \n",
    "    return daily.resample(\"YE\").sum().mean()\n",
    "\n",
    "\n",
    "\n",
    "rf_daily = etf_df[\"rf\"].reindex(full_idx).fillna(0)\n",
    "rows = []\n",
    "for lab, res in run_results.items():\n",
    "    r = res[\"returns\"] \n",
    "    r_ex = r - rf_daily\n",
    "    wdf = res.get(\"weights_actual\", res.get(\"weights_view\", None))\n",
    "    flags = res[\"trade_flags\"]\n",
    "\n",
    "    row = {}\n",
    "    row[\"Strategy\"] = lab\n",
    "    row[\"Ann.Return\"]   = ann_return(r)\n",
    "    row[\"Ann.Ex.Return\"]= ann_return(r_ex)\n",
    "    row[\"Ann.Ex.Risk\"]  = ann_volatility(r_ex)\n",
    "    row[\"Ex.Sharpe\"]    = sharpe(r_ex)\n",
    "    row[\"MaxDD\"]        = max_drawdown(r)\n",
    "    row[\"Sortino\"]      = sortino(r_ex)\n",
    "    row[\"Calmar\"]       = calmar(r)\n",
    "\n",
    "\n",
    "    if \"Benchmark (Q-Rebalanced EW)\" in run_results:\n",
    "        row[\"InfoRatio\"] = information_ratio(r, run_results[\"Benchmark (Q-Rebalanced EW)\"][\"returns\"])\n",
    "    else:\n",
    "        row[\"InfoRatio\"] = np.nan\n",
    "\n",
    "    if wdf is not None:\n",
    "        row[\"Turnover\"] = ann_turnover(wdf, flags)\n",
    "    else:\n",
    "        row[\"Turnover\"] = np.nan\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df_stats = pd.DataFrame(rows)\n",
    "print(df_stats.to_string(index=False, float_format=lambda x: f\"{x:.3f}\"))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
