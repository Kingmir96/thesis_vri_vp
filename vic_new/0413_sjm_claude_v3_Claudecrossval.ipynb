{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbe6cf8-1fa8-4ab3-accf-aff550d3c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# For loading data and feature engineering\n",
    "from feature_set_v2 import MergedDataLoader\n",
    "\n",
    "# For data prep and pre-processing\n",
    "from jumpmodels.utils import filter_date_range \n",
    "from jumpmodels.preprocess import StandardScalerPD, DataClipperStd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For model fit and prediction\n",
    "from joblib import Parallel, delayed # allows parallel grid search on all 4 cores\n",
    "from jumpmodels.sparse_jump import SparseJumpModel\n",
    "\n",
    "# For plotting\n",
    "from jumpmodels.plot import plot_regimes_and_cumret, plot_cumret\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Portfolio allocation\n",
    "from pypfopt.black_litterman import BlackLittermanModel\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa0d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import feature_25 \n",
    "# importlib.reload(feature_25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f3fc3-04f0-4f0c-be06-10c33f170836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rolling_time_series_cv_sjm_long_short(\n",
    "    lam,\n",
    "    kappa,\n",
    "    X,\n",
    "    factor_returns,\n",
    "    market_returns,\n",
    "    n_splits=5,\n",
    "    initial_train_size=8*252,\n",
    "    val_size=6*252,  # Changed to 6-year validation window as mentioned\n",
    "    cost_per_100pct=0.0005,\n",
    "    annual_threshold=0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling expanding-window CV function. For each fold, it clips/scales,\n",
    "    fits a SparseJumpModel, does day-lag logic, and returns a single Sharpe.\n",
    "    We then average across folds.\n",
    "    \"\"\"\n",
    "    # Ensure DataFrames have aligned indices\n",
    "    common_index = X.index.intersection(factor_returns.index).intersection(market_returns.index)\n",
    "    if len(common_index) == 0:\n",
    "        raise ValueError(\"No common dates between X, factor_returns, and market_returns\")\n",
    "    \n",
    "    X = X.loc[common_index]\n",
    "    factor_returns = factor_returns.loc[common_index]\n",
    "    market_returns = market_returns.loc[common_index]\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    if n_samples <= initial_train_size:\n",
    "        raise ValueError(f\"Not enough data for cross-validation. n_samples={n_samples}, initial_train_size={initial_train_size}\")\n",
    "        \n",
    "    max_folds = (n_samples - initial_train_size) // val_size\n",
    "    actual_folds = min(n_splits, max_folds)\n",
    "    \n",
    "    if actual_folds <= 0:\n",
    "        raise ValueError(f\"Not enough data for any fold. max_folds={max_folds}, n_samples={n_samples}, initial_train_size={initial_train_size}, val_size={val_size}\")\n",
    "    \n",
    "    print(f\"Running cross-validation with {actual_folds} folds\")\n",
    "    print(f\"Data spans from {X.index[0]} to {X.index[-1]} ({len(X)} days)\")\n",
    "    print(f\"Training size: {initial_train_size} days, Validation size: {val_size} days\")\n",
    "    \n",
    "    max_feats = int(kappa**2)\n",
    "    sharpe_scores = []\n",
    "\n",
    "    def position_from_expected_return(ann_ret, threshold=annual_threshold):\n",
    "        if ann_ret > threshold: \n",
    "            return 1.0\n",
    "        elif ann_ret < -threshold: \n",
    "            return -1.0\n",
    "        else: \n",
    "            return ann_ret / threshold\n",
    "\n",
    "    start_of_val = initial_train_size\n",
    "\n",
    "    for fold_i in range(actual_folds):\n",
    "        train_end = start_of_val\n",
    "        val_end = start_of_val + val_size\n",
    "        if val_end > n_samples:\n",
    "            val_end = n_samples\n",
    "\n",
    "        # 1) Raw slices\n",
    "        X_train_cv_raw = X.iloc[:train_end]\n",
    "        y_train_cv     = factor_returns.iloc[:train_end]\n",
    "        m_train_cv     = market_returns.iloc[:train_end]\n",
    "\n",
    "        X_val_cv_raw   = X.iloc[train_end:val_end]\n",
    "        y_val_cv       = factor_returns.iloc[train_end:val_end]\n",
    "        m_val_cv       = market_returns.iloc[train_end:val_end]\n",
    "        \n",
    "        # Check for empty validation set\n",
    "        if len(X_val_cv_raw) == 0:\n",
    "            raise ValueError(f\"Empty validation set for fold {fold_i}\")\n",
    "\n",
    "        print(f\"\\nFold {fold_i+1}/{actual_folds}:\")\n",
    "        print(f\"  Training: {X_train_cv_raw.index[0]} to {X_train_cv_raw.index[-1]} ({len(X_train_cv_raw)} days)\")\n",
    "        print(f\"  Validation: {X_val_cv_raw.index[0]} to {X_val_cv_raw.index[-1]} ({len(X_val_cv_raw)} days)\")\n",
    "\n",
    "        # 2) Clipper/scaler on training portion only - fit_transform on train, transform on val\n",
    "        clipper_fold = DataClipperStd(mul=3.)\n",
    "        X_train_cv_raw_clipped = clipper_fold.fit_transform(X_train_cv_raw) \n",
    "        X_val_cv_raw_clipped   = clipper_fold.transform(X_val_cv_raw)\n",
    "\n",
    "        scaler_fold = StandardScaler()\n",
    "        X_train_cv = scaler_fold.fit_transform(X_train_cv_raw_clipped)\n",
    "        X_val_cv   = scaler_fold.transform(X_val_cv_raw_clipped)\n",
    "\n",
    "        # 3) Calculate active return on training set and fit SJM\n",
    "        active_train_cv = y_train_cv - m_train_cv\n",
    "        \n",
    "        # Check if active returns are valid\n",
    "        if active_train_cv.isna().any():\n",
    "            raise ValueError(f\"NaN values found in active returns for fold {fold_i}\")\n",
    "        \n",
    "        if active_train_cv.std() == 0:\n",
    "            raise ValueError(f\"Zero standard deviation in active returns for fold {fold_i}\")\n",
    "        \n",
    "        try:\n",
    "            model = SparseJumpModel(\n",
    "                n_components=2,\n",
    "                max_feats=max_feats,\n",
    "                jump_penalty=lam,\n",
    "                cont=False,\n",
    "                max_iter=30\n",
    "            )\n",
    "            model.fit(X_train_cv, ret_ser=active_train_cv, sort_by=\"cumret\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"SparseJumpModel fitting failed in fold {fold_i}: {e}\")\n",
    "\n",
    "        # Print regime counts and average returns to diagnose issues\n",
    "        train_states = model.predict(X_train_cv)\n",
    "        regime_counts = np.bincount(train_states)\n",
    "        print(f\"  Training regime counts: {regime_counts}\")\n",
    "        print(f\"  Regime average returns: {model.ret_}\")\n",
    "        \n",
    "        # Check if model.ret_ contains NaN values\n",
    "        if np.isnan(model.ret_).any():\n",
    "            raise ValueError(f\"NaN values in regime returns: {model.ret_}\")\n",
    "\n",
    "        # 4) Regime inference on validation\n",
    "        val_states = model.predict_online(X_val_cv)\n",
    "        val_regime_counts = np.bincount(val_states)\n",
    "        print(f\"  Validation regime counts: {val_regime_counts}\")\n",
    "\n",
    "        # 5) Evaluate strategy: day-lag logic\n",
    "        val_active_ret = y_val_cv - m_val_cv\n",
    "        strategy_ret = np.zeros(len(val_states)) # Initialize strategy returns to 0 for all days\n",
    "        prev_position = 0.0\n",
    "\n",
    "        # Compute expected active return per regime from training\n",
    "        state_avg_daily_active_ret = model.ret_\n",
    "        state_to_expected = {}\n",
    "        \n",
    "        # Ensure we have valid regime returns\n",
    "        for st in range(len(state_avg_daily_active_ret)):\n",
    "            state_to_expected[st] = state_avg_daily_active_ret[st] * 252  # annualize daily active returns\n",
    "            print(f\"  State {st} expected annual active return: {state_to_expected[st]:.4f}\")\n",
    "            # Check what position this would generate\n",
    "            pos = position_from_expected_return(state_to_expected[st], annual_threshold)\n",
    "            print(f\"  State {st} position: {pos:.4f}\")\n",
    "\n",
    "        # Run strategy simulation\n",
    "        for t in range(len(val_states)):\n",
    "            st = val_states[t]\n",
    "            # Calculate PnL based on previous day's position\n",
    "            daily_pnl = prev_position * val_active_ret.iloc[t]\n",
    "            \n",
    "            # Calculate new position based on current day's state\n",
    "            new_position = position_from_expected_return(state_to_expected.get(st, 0.0)) \n",
    "            \n",
    "            # Calculate trading costs\n",
    "            turnover = abs(new_position - prev_position)\n",
    "            cost = turnover * cost_per_100pct\n",
    "            \n",
    "            # Record strategy return\n",
    "            strategy_ret[t] = daily_pnl - cost\n",
    "            \n",
    "            # Update position for next day\n",
    "            prev_position = new_position\n",
    "\n",
    "        # Calculate Sharpe ratio\n",
    "        avg_ret = np.mean(strategy_ret)\n",
    "        std_ret = np.std(strategy_ret, ddof=1)\n",
    "        \n",
    "        if std_ret == 0:\n",
    "            raise ValueError(f\"Zero standard deviation in strategy returns for fold {fold_i}. Check if all positions or returns are identical.\")\n",
    "            \n",
    "        val_sharpe = (avg_ret/std_ret)*np.sqrt(252)\n",
    "        \n",
    "        print(f\"  Strategy stats: Avg ret = {avg_ret:.6f}, Std = {std_ret:.6f}, Sharpe = {val_sharpe:.4f}\")\n",
    "        \n",
    "        # Add some extra diagnostic information\n",
    "        pos_days = np.sum(strategy_ret > 0)\n",
    "        neg_days = np.sum(strategy_ret < 0)\n",
    "        zero_days = np.sum(strategy_ret == 0)\n",
    "        print(f\"  Strategy return days: +ve={pos_days}, -ve={neg_days}, zero={zero_days}\")\n",
    "        \n",
    "        sharpe_scores.append(val_sharpe)\n",
    "        start_of_val += 126  \n",
    "\n",
    "    # Return average Sharpe across all folds\n",
    "    if not sharpe_scores:\n",
    "        raise ValueError(\"No valid folds completed. Check validation data and parameters.\")\n",
    "        \n",
    "    avg_sharpe = np.mean(sharpe_scores)\n",
    "    print(f\"\\nAverage Sharpe across {len(sharpe_scores)} folds: {avg_sharpe:.4f}\")\n",
    "    return avg_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be595c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) HELPER: EWMA Covariance\n",
    "###############################################################################\n",
    "def ewm_covariance(returns: pd.DataFrame, halflife=126, min_periods=1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute an exponentially weighted covariance matrix over all data in 'returns'.\n",
    "    The half-life is 126 days, meaning weights decay by 50% every 126 days.\n",
    "    We then select the final NxN slice from the multi-index result.\n",
    "    \"\"\"\n",
    "    ewm_cov = returns.ewm(halflife=halflife, adjust=False, min_periods=min_periods).cov()\n",
    "    if len(returns) == 0:\n",
    "        # Return an empty DataFrame if no data\n",
    "        return pd.DataFrame()\n",
    "    last_date = returns.index[-1]\n",
    "    df_slice = ewm_cov.loc[last_date]  # shape (N, N)\n",
    "    df_slice.index.name = None\n",
    "    return df_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4b804ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2) HELPER: Rolling + Expanding Black–Litterman\n",
    "###############################################################################\n",
    "def run_bl_portfolio_pyopt_expanding(\n",
    "    factor_dict,\n",
    "    returns_df,\n",
    "    test_index,\n",
    "    tau=0.05,\n",
    "    delta=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    An expanding-window Black–Litterman procedure:\n",
    "    - For each date t in test_index, we take all historical data up to t-1 to compute:\n",
    "       (a) an EWMA covariance (with half-life=126)\n",
    "       (b) a BL prior using pi=\"equal\"  (i.e. all assets get equal baseline expected return)\n",
    "       (c) factor-based relative views: factor vs Market => P, Q\n",
    "    - Then we get posterior (bl_returns, bl_cov).\n",
    "    - Then run a standard Efficient Frontier (max_sharpe).\n",
    "    - We store the resulting weights for day t+1 (one-day lag).\n",
    "    \"\"\"\n",
    "    assets = list(returns_df.columns)   # e.g. [factor1, factor2, ..., Market]\n",
    "    factors = list(factor_dict.keys())  # e.g. [factor1, factor2, ...]\n",
    "    weights_df = pd.DataFrame(index=test_index, columns=assets, dtype=float)\n",
    "\n",
    "    test_dates = list(test_index)\n",
    "\n",
    "    for i, t in enumerate(test_dates):\n",
    "        # 1) Expand up to day t-1\n",
    "        window_end_idx = returns_df.index.get_loc(t)\n",
    "        if window_end_idx == 0:\n",
    "            # No prior data => skip\n",
    "            continue\n",
    "        expanding_slice = returns_df.iloc[:window_end_idx]\n",
    "\n",
    "        # 2) Compute the EWMA covariance\n",
    "        cov_matrix = ewm_covariance(expanding_slice, halflife=126, min_periods=60)\n",
    "        if cov_matrix.empty or cov_matrix.isna().any().any():\n",
    "            # If invalid, skip\n",
    "            continue\n",
    "\n",
    "        # 3) Construct the (P, Q) for factor vs Market\n",
    "        #    Q is the list of \"annual active returns\" for each factor's bull/bear regime\n",
    "        P_matrix = []\n",
    "        Q_values = []\n",
    "        for fac in factors:\n",
    "            # Which regime do we see for factor \"fac\" at day t?\n",
    "            st = factor_dict[fac]['states'].loc[t]\n",
    "            daily_active_ret = factor_dict[fac]['regime_returns'][st]  # daily factor active\n",
    "            annual_active_ret = daily_active_ret * 252\n",
    "            Q_values.append(annual_active_ret)\n",
    "\n",
    "            # row is +1 on factor, -1 on 'Market'\n",
    "            row = [0]*len(assets)\n",
    "            row[assets.index(fac)] = 1\n",
    "            row[assets.index(\"Market\")] = -1\n",
    "            P_matrix.append(row)\n",
    "\n",
    "        P = pd.DataFrame(P_matrix, columns=assets)\n",
    "        Q = pd.Series(Q_values)\n",
    "\n",
    "        # 4) Build the Black–Litterman model\n",
    "        #    We'll say pi=\"equal\" so that each asset has the same baseline prior\n",
    "        #    Then we incorporate the relative views in (P, Q).\n",
    "        #    tau=0.05 is a typical default.\n",
    "        try:\n",
    "            bl = BlackLittermanModel(\n",
    "                cov_matrix=cov_matrix,\n",
    "                pi=\"equal\",\n",
    "                P=P,\n",
    "                Q=Q,\n",
    "                tau=tau,\n",
    "                delta=delta,\n",
    "                # no \"omega\" => default is Proportional to var of view portfolio\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"BL Model error on day {t}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 5) Retrieve posterior returns & covariance\n",
    "        mu_bl = bl.bl_returns()\n",
    "        cov_bl = bl.bl_cov()  # posterior covariance\n",
    "\n",
    "        # 6) Mean-variance optimization\n",
    "        ef = EfficientFrontier(mu_bl, cov_bl, weight_bounds=(-0, 1))\n",
    "        try:\n",
    "            ef.max_sharpe()  # or ef.min_volatility() etc.\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization error on day {t}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 7) Store weights for day t+1 (one-day lag)\n",
    "        w_series = pd.Series(ef.clean_weights(), name=t)\n",
    "        if i < len(test_dates) - 1:\n",
    "            rebalance_day = test_dates[i+1]\n",
    "            weights_df.loc[rebalance_day] = w_series\n",
    "        else:\n",
    "            weights_df.loc[t] = w_series\n",
    "\n",
    "    return weights_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e8aec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Sharpe Ratio\n",
    "# ---------------------------------------------------------------------\n",
    "def annualized_sharpe(r):\n",
    "    return (r.mean() / r.std()) * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f68ef929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # manual build of best_hyperparams dict for BL model if we don't want to run the SJM CV again\n",
    "# best_hyperparams = {\n",
    "#     \"IWF\": {\n",
    "#         \"best_lambda\": 66.17434558908558,\n",
    "#         \"best_kappa\": 2.0\n",
    "#     },\n",
    "#     \"MTUM\": {\n",
    "#         \"best_lambda\": 239.5026619987486,\n",
    "#         \"best_kappa\": 2.0\n",
    "#     },\n",
    "#     \"QUAL\": {\n",
    "#         \"best_lambda\": 66.17434558908558,\n",
    "#         \"best_kappa\": 2.44948974278\n",
    "#     },\n",
    "#     \"SIZE\": {\n",
    "#         \"best_lambda\": 49.72235891449993,\n",
    "#         \"best_kappa\": 3.46410161514\n",
    "#     },\n",
    "#     \"USMV\": {\n",
    "#         \"best_lambda\": 751.3842180360134,\n",
    "#         \"best_kappa\": 2.44948974278\n",
    "#     },\n",
    "#     \"VLUE\": {\n",
    "#         \"best_lambda\": 57.36152510448679,\n",
    "#         \"best_kappa\": 2\n",
    "#     },\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data for factor iwf\n",
      "\n",
      "Loading data for factor mtum\n",
      "\n",
      "Loading data for factor qual\n",
      "\n",
      "Loading data for factor size\n",
      "\n",
      "Loading data for factor usmv\n",
      "\n",
      "Loading data for factor vlue\n",
      "\n",
      "==============================================================\n",
      "INITIAL CROSS-VALIDATION (BEFORE TEST PERIOD)\n",
      "==============================================================\n",
      "\n",
      "==============================================================\n",
      "Running initial SJM cross-validation for factor = iwf\n",
      "==============================================================\n",
      "Training window: 2011-01-03 00:00:00 to 2018-12-31 00:00:00 (7.99 years)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough data for cross-validation. n_samples=1990, initial_train_size=2016",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"d:\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_10904\\339533465.py\", line 29, in rolling_time_series_cv_sjm_long_short\nValueError: Not enough data for cross-validation. n_samples=1990, initial_train_size=2016\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 182\u001b[0m\n\u001b[0;32m    179\u001b[0m kappa_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m2\u001b[39m, np\u001b[38;5;241m.\u001b[39msqrt(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# Run cross-validation in parallel\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)(\n\u001b[0;32m    183\u001b[0m     delayed(rolling_time_series_cv_sjm_long_short)(\n\u001b[0;32m    184\u001b[0m         lam, kappa, \n\u001b[0;32m    185\u001b[0m         X_train,\n\u001b[0;32m    186\u001b[0m         factor_returns\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfac_ret\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[X_train\u001b[38;5;241m.\u001b[39mindex],\n\u001b[0;32m    187\u001b[0m         market_returns\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmkt_ret\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[X_train\u001b[38;5;241m.\u001b[39mindex]\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lam \u001b[38;5;129;01min\u001b[39;00m lambda_values\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m kappa \u001b[38;5;129;01min\u001b[39;00m kappa_values\n\u001b[0;32m    191\u001b[0m )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Process results\u001b[39;00m\n\u001b[0;32m    194\u001b[0m results_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(results)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(lambda_values), \u001b[38;5;28mlen\u001b[39m(kappa_values))\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Not enough data for cross-validation. n_samples=1990, initial_train_size=2016"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ############################################################################\n",
    "    # 0) Set parameters\n",
    "    ############################################################################\n",
    "    # We define how often to re-fit and re-cross-validate:\n",
    "    REFIT_FREQ = \"ME\"  # Monthly refit (can be \"QE\" or \"YE\" as desired)\n",
    "    RECROSS_VAL_FREQ = \"YE\"  # Yearly re-cross-validation\n",
    "    \n",
    "    # Training window configuration\n",
    "    WINDOW_TYPE = \"rolling\"  # Options: \"expanding\" or \"rolling\"\n",
    "    MIN_TRAINING_YEARS = 8  # Minimum 8 years of training data\n",
    "    MAX_TRAINING_YEARS = 12  # Maximum 12 years of training data\n",
    "    \n",
    "    # Paths & Tickers\n",
    "    directory = r\"C:\\Users\\victo\\git_new\\thesis_vri_vp\\data\"\n",
    "    factor_file = os.path.join(directory, \"1estimation_index_returns.csv\")\n",
    "    market_file = os.path.join(directory, \"1macro_data.csv\")\n",
    "    factors = [\"iwf\", \"mtum\", \"qual\", \"size\", \"usmv\", \"vlue\"]\n",
    "\n",
    "    # Date boundaries\n",
    "    train_start = \"2002-05-31\"\n",
    "    test_start = \"2017-01-01\"\n",
    "\n",
    "    # Cross-validation settings\n",
    "    lambda_values = np.linspace(30, 400, 10)  # For grid search\n",
    "    \n",
    "    ############################################################################\n",
    "    # 1) Load full data for all factors & market\n",
    "    ############################################################################\n",
    "    # We'll store each factor's entire X and returns in a dict for easy access\n",
    "    factor_data_dict = {}\n",
    "    factor_returns_list = []\n",
    "    for fac in factors:\n",
    "        print(f\"\\nLoading data for factor {fac}\")\n",
    "        data = MergedDataLoader(\n",
    "            factor_file=factor_file,\n",
    "            market_file=market_file,\n",
    "            ver=\"v2\",\n",
    "            factor_col=fac\n",
    "        ).load()\n",
    "        common_idx = (\n",
    "            data.X.index\n",
    "            .intersection(data.ret_ser.index)\n",
    "            .intersection(data.market_ser.index)\n",
    "        )\n",
    "\n",
    "        # Full factor data\n",
    "        X_full = data.X.loc[common_idx]\n",
    "        fac_ret_full = data.ret_ser.loc[common_idx]\n",
    "        mkt_ret_full = data.market_ser.loc[common_idx]\n",
    "        active_ret = fac_ret_full - mkt_ret_full\n",
    "\n",
    "        factor_data_dict[fac] = {\n",
    "            \"X\": X_full,\n",
    "            \"fac_ret\": fac_ret_full,\n",
    "            \"mkt_ret\": mkt_ret_full,\n",
    "            \"active_ret\": active_ret\n",
    "        }\n",
    "        factor_returns_list.append(fac_ret_full)\n",
    "\n",
    "    # We'll store the last loaded \"mkt_ret_full\" as \"all_market_ret\"\n",
    "    all_market_ret = mkt_ret_full\n",
    "\n",
    "    # Combine factor returns + market into a single DF\n",
    "    full_factors_df = pd.concat(factor_returns_list, axis=1).dropna()  # T x 6\n",
    "    full_df = pd.concat([full_factors_df, all_market_ret], axis=1).dropna()\n",
    "    full_df.columns = factors + [\"Market\"]\n",
    "\n",
    "    # Our \"test_index\" is from test_start onward\n",
    "    test_slice = filter_date_range(full_df, start_date=test_start)\n",
    "    test_index = test_slice.index.sort_values()\n",
    "\n",
    "    # Identify refit and re-cross-validation boundaries\n",
    "    refit_boundaries = (\n",
    "        test_index.to_series()\n",
    "        .resample(REFIT_FREQ)\n",
    "        .last()\n",
    "        .dropna()\n",
    "    )\n",
    "    \n",
    "    recross_val_boundaries = (\n",
    "        test_index.to_series()\n",
    "        .resample(RECROSS_VAL_FREQ)\n",
    "        .last()\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    ############################################################################\n",
    "    # 2) Initial Cross-Validation and Hyperparameter Selection\n",
    "    ############################################################################\n",
    "    best_hyperparams = {}\n",
    "    \n",
    "    print(\"\\n==============================================================\")\n",
    "    print(\"INITIAL CROSS-VALIDATION (BEFORE TEST PERIOD)\")\n",
    "    print(\"==============================================================\")\n",
    "    \n",
    "    # Helper function to get appropriate training window given constraints\n",
    "    def get_training_window(current_date, full_data, min_years=MIN_TRAINING_YEARS, max_years=MAX_TRAINING_YEARS, \n",
    "                           window_type=WINDOW_TYPE, train_start_date=train_start):\n",
    "        \"\"\"\n",
    "        Get training data window based on configuration.\n",
    "        \n",
    "        Args:\n",
    "            current_date: The current date (end of training window)\n",
    "            full_data: The full dataset (DataFrame with DatetimeIndex)\n",
    "            min_years: Minimum training window size in years\n",
    "            max_years: Maximum training window size in years\n",
    "            window_type: 'expanding' or 'rolling'\n",
    "            train_start_date: The earliest possible start date\n",
    "            \n",
    "        Returns:\n",
    "            start_date, end_date: The appropriate training window boundaries\n",
    "        \"\"\"\n",
    "        # End date is always the current_date\n",
    "        end_date = current_date\n",
    "        \n",
    "        # Convert date strings to datetime if needed\n",
    "        if isinstance(train_start_date, str):\n",
    "            train_start_date = pd.to_datetime(train_start_date)\n",
    "        if isinstance(current_date, str):\n",
    "            current_date = pd.to_datetime(current_date)\n",
    "            \n",
    "        # For expanding window: use all data from train_start to current_date, \n",
    "        # but respect max_years constraint\n",
    "        if window_type == \"expanding\":\n",
    "            # Calculate minimum start date based on max_years\n",
    "            min_start_date = current_date - pd.DateOffset(years=max_years)\n",
    "            # Use the later of train_start_date or min_start_date\n",
    "            start_date = max(train_start_date, min_start_date)\n",
    "            \n",
    "        # For rolling window: use exactly min_years of data, or more if min_years\n",
    "        # would put us before train_start_date\n",
    "        elif window_type == \"rolling\":\n",
    "            # Calculate desired start date based on min_years\n",
    "            desired_start = current_date - pd.DateOffset(years=min_years)\n",
    "            # Use the later of train_start_date or desired_start\n",
    "            start_date = max(train_start_date, desired_start)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown window_type: {window_type}\")\n",
    "            \n",
    "        # Filter data to get the actual window\n",
    "        full_index = full_data.index\n",
    "        available_dates = full_index[\n",
    "            (full_index >= start_date) & \n",
    "            (full_index <= end_date)\n",
    "        ]\n",
    "        \n",
    "        if len(available_dates) == 0:\n",
    "            raise ValueError(f\"No data available in the requested window: {start_date} to {end_date}\")\n",
    "            \n",
    "        # Return the actual start and end dates from available data\n",
    "        return available_dates[0], available_dates[-1]\n",
    "    \n",
    "    for factor_name in factors:\n",
    "        print(\"\\n==============================================================\")\n",
    "        print(f\"Running initial SJM cross-validation for factor = {factor_name}\")\n",
    "        print(\"==============================================================\")\n",
    "        \n",
    "        # Get data for this factor\n",
    "        data = factor_data_dict[factor_name]\n",
    "        X = data[\"X\"]\n",
    "        \n",
    "        # Get initial training window based on configuration\n",
    "        train_start_date, train_end_date = get_training_window(\n",
    "            pd.to_datetime(test_start), X, \n",
    "            min_years=MIN_TRAINING_YEARS, \n",
    "            max_years=MAX_TRAINING_YEARS,\n",
    "            window_type=WINDOW_TYPE,\n",
    "            train_start_date=train_start\n",
    "        )\n",
    "        \n",
    "        # Filter to initial train period\n",
    "        X_train = filter_date_range(X, start_date=train_start_date, end_date=train_end_date)\n",
    "        \n",
    "        print(f\"Training window: {train_start_date} to {train_end_date} ({(train_end_date - train_start_date).days / 365.25:.2f} years)\")\n",
    "        \n",
    "        # Define kappa values based on number of features\n",
    "        kappa_values = np.linspace(2, np.sqrt(X_train.shape[1]), 3)\n",
    "        \n",
    "        # Run cross-validation in parallel\n",
    "        results = Parallel(n_jobs=4)(\n",
    "            delayed(rolling_time_series_cv_sjm_long_short)(\n",
    "                lam, kappa, \n",
    "                X_train,\n",
    "                factor_returns=data[\"fac_ret\"].loc[X_train.index],\n",
    "                market_returns=data[\"mkt_ret\"].loc[X_train.index]\n",
    "            )\n",
    "            for lam in lambda_values\n",
    "            for kappa in kappa_values\n",
    "        )\n",
    "\n",
    "        # Process results\n",
    "        results_array = np.array(results).reshape(len(lambda_values), len(kappa_values))\n",
    "        best_index = np.argmax(results)\n",
    "        best_lambda = lambda_values[best_index // len(kappa_values)]\n",
    "        best_kappa = kappa_values[best_index % len(kappa_values)]\n",
    "        max_feats_best = int(best_kappa**2)\n",
    "        best_sharpe = results[best_index]\n",
    "\n",
    "        print(f\"[{factor_name}] Best Jump Penalty (λ): {best_lambda}\")\n",
    "        print(f\"[{factor_name}] Best Max Features (κ²): {max_feats_best}\")\n",
    "        print(f\"[{factor_name}] Sharpe with best hyperparams: {best_sharpe}\")\n",
    "\n",
    "        # Store best hyperparameters\n",
    "        best_hyperparams[factor_name] = {\n",
    "            \"best_lambda\": best_lambda,\n",
    "            \"best_kappa\": best_kappa\n",
    "        }\n",
    "\n",
    "        # Plot Sharpe vs. Lambda for each kappa (optional)\n",
    "        plt.figure()\n",
    "        for i, kp in enumerate(kappa_values):\n",
    "            plt.plot(lambda_values, results_array[:, i], label=f\"kappa={kp:.2f}\")\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Lambda')\n",
    "        plt.ylabel('Sharpe')\n",
    "        plt.title(f\"{factor_name}: Initial Sharpe vs. Lambda (Grid CV)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    ############################################################################\n",
    "    # 3) Loop over re-fit intervals with periodic re-cross-validation\n",
    "    ############################################################################\n",
    "    # For tracking performance and decisions\n",
    "    all_daily_states = {fac: pd.Series(dtype=float) for fac in factors}\n",
    "    all_daily_weights = []\n",
    "    all_daily_rets = []\n",
    "    hyperparameter_history = {fac: [] for fac in factors}  # Track parameter changes\n",
    "    training_window_history = []  # Track training window sizes\n",
    "    \n",
    "    # Keep track of the last re-cross-validation date\n",
    "    last_recross_val_date = None\n",
    "\n",
    "    print(\"\\n==============================================================\")\n",
    "    print(f\"BEGINNING OUT-OF-SAMPLE TRADING WITH {WINDOW_TYPE.upper()} WINDOW\")\n",
    "    print(f\"MIN: {MIN_TRAINING_YEARS} years, MAX: {MAX_TRAINING_YEARS} years\")\n",
    "    print(\"==============================================================\")\n",
    "\n",
    "    for i in range(len(refit_boundaries) - 1):\n",
    "        refit_date = refit_boundaries.iloc[i]\n",
    "        next_refit_date = refit_boundaries.iloc[i + 1]\n",
    "        \n",
    "        # Check if we need to re-cross-validate at this refit point\n",
    "        recross_val_needed = False\n",
    "        if refit_date in recross_val_boundaries.values:\n",
    "            # Check if this is a new re-cross-validation period\n",
    "            if last_recross_val_date is None or refit_date > last_recross_val_date:\n",
    "                recross_val_needed = True\n",
    "                last_recross_val_date = refit_date\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"RE-CROSS-VALIDATION AT DATE: {refit_date}\")\n",
    "                print(f\"{'='*80}\")\n",
    "                \n",
    "        # Re-cross-validate if needed to update hyperparameters\n",
    "        if recross_val_needed:\n",
    "            for factor_name in factors:\n",
    "                print(f\"\\nRe-cross-validation for factor {factor_name}\")\n",
    "                \n",
    "                # Get data for this factor\n",
    "                data = factor_data_dict[factor_name]\n",
    "                X = data[\"X\"]\n",
    "                factor_returns = data[\"fac_ret\"]\n",
    "                market_returns = data[\"mkt_ret\"]\n",
    "                \n",
    "                # Get appropriate training window based on configuration\n",
    "                train_start_date, train_end_date = get_training_window(\n",
    "                    refit_date, X, \n",
    "                    min_years=MIN_TRAINING_YEARS, \n",
    "                    max_years=MAX_TRAINING_YEARS,\n",
    "                    window_type=WINDOW_TYPE,\n",
    "                    train_start_date=train_start\n",
    "                )\n",
    "                \n",
    "                # Use data from train_start_date to refit_date for cross-validation\n",
    "                X_cv = filter_date_range(X, start_date=train_start_date, end_date=train_end_date)\n",
    "                \n",
    "                training_years = (train_end_date - train_start_date).days / 365.25\n",
    "                print(f\"Training window: {train_start_date} to {train_end_date} ({training_years:.2f} years)\")\n",
    "                \n",
    "                # Track training window\n",
    "                training_window_history.append({\n",
    "                    \"date\": refit_date,\n",
    "                    \"start_date\": train_start_date,\n",
    "                    \"end_date\": train_end_date,\n",
    "                    \"years\": training_years,\n",
    "                    \"window_type\": WINDOW_TYPE\n",
    "                })\n",
    "                \n",
    "                # Update kappa values based on current feature count\n",
    "                kappa_values = np.linspace(2, np.sqrt(X_cv.shape[1]), 3)\n",
    "                \n",
    "                # Run cross-validation in parallel\n",
    "                results = Parallel(n_jobs=4)(\n",
    "                    delayed(rolling_time_series_cv_sjm_long_short)(\n",
    "                        lam, kappa, \n",
    "                        X_cv,\n",
    "                        factor_returns=factor_returns.loc[X_cv.index],\n",
    "                        market_returns=market_returns.loc[X_cv.index]\n",
    "                    )\n",
    "                    for lam in lambda_values\n",
    "                    for kappa in kappa_values\n",
    "                )\n",
    "\n",
    "                # Process results\n",
    "                results_array = np.array(results).reshape(len(lambda_values), len(kappa_values))\n",
    "                best_index = np.argmax(results)\n",
    "                best_lambda = lambda_values[best_index // len(kappa_values)]\n",
    "                best_kappa = kappa_values[best_index % len(kappa_values)]\n",
    "                max_feats_best = int(best_kappa**2)\n",
    "                best_sharpe = results[best_index]\n",
    "\n",
    "                # Store previous hyperparameters for comparison\n",
    "                old_lambda = best_hyperparams[factor_name][\"best_lambda\"]\n",
    "                old_kappa = best_hyperparams[factor_name][\"best_kappa\"]\n",
    "                \n",
    "                print(f\"[{factor_name}] Previous λ: {old_lambda}, New λ: {best_lambda}\")\n",
    "                print(f\"[{factor_name}] Previous κ²: {int(old_kappa**2)}, New κ²: {max_feats_best}\")\n",
    "                print(f\"[{factor_name}] New best Sharpe: {best_sharpe}\")\n",
    "                \n",
    "                # Track parameter history\n",
    "                hyperparameter_history[factor_name].append({\n",
    "                    \"date\": refit_date,\n",
    "                    \"old_lambda\": old_lambda,\n",
    "                    \"new_lambda\": best_lambda,\n",
    "                    \"old_kappa\": old_kappa,\n",
    "                    \"new_kappa\": best_kappa,\n",
    "                    \"sharpe\": best_sharpe\n",
    "                })\n",
    "                \n",
    "                # Update best hyperparameters\n",
    "                best_hyperparams[factor_name] = {\n",
    "                    \"best_lambda\": best_lambda,\n",
    "                    \"best_kappa\": best_kappa\n",
    "                }\n",
    "                \n",
    "                # Plot Sharpe vs. Lambda if desired (can be commented out)\n",
    "                plt.figure()\n",
    "                for i, kp in enumerate(kappa_values):\n",
    "                    plt.plot(lambda_values, results_array[:, i], label=f\"kappa={kp:.2f}\")\n",
    "                plt.xscale('log')\n",
    "                plt.xlabel('Lambda')\n",
    "                plt.ylabel('Sharpe')\n",
    "                plt.title(f\"{factor_name} at {refit_date}: Sharpe vs. Lambda\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "        # Define the \"test chunk\" period\n",
    "        test_mask = (test_index > refit_date) & (test_index <= next_refit_date)\n",
    "        test_dates_chunk = test_index[test_mask]\n",
    "        if len(test_dates_chunk) == 0:\n",
    "            # No trading days in this chunk? Move on\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTrading for period: {refit_date} to {next_refit_date}\")\n",
    "        print(f\"Number of trading days: {len(test_dates_chunk)}\")\n",
    "        \n",
    "        # Build factor_dict for this chunk\n",
    "        factor_dict_chunk = {}\n",
    "\n",
    "        for fac in factors:\n",
    "            fac_info = factor_data_dict[fac]\n",
    "            X = fac_info[\"X\"]\n",
    "            ret_full = fac_info[\"fac_ret\"]\n",
    "            mkt_full = fac_info[\"mkt_ret\"]\n",
    "            active = fac_info[\"active_ret\"]\n",
    "\n",
    "            # -----------------------------------------------------------------\n",
    "            # 3A) Build training data with appropriate window\n",
    "            # -----------------------------------------------------------------\n",
    "            # Get appropriate training window based on configuration\n",
    "            train_start_date, train_end_date = get_training_window(\n",
    "                refit_date, X, \n",
    "                min_years=MIN_TRAINING_YEARS, \n",
    "                max_years=MAX_TRAINING_YEARS,\n",
    "                window_type=WINDOW_TYPE,\n",
    "                train_start_date=train_start\n",
    "            )\n",
    "            \n",
    "            X_train = filter_date_range(X, start_date=train_start_date, end_date=train_end_date)\n",
    "            y_train = filter_date_range(active, start_date=train_start_date, end_date=train_end_date)\n",
    "\n",
    "            # Scale + Clip the training set\n",
    "            clipper = DataClipperStd(mul=3.0)\n",
    "            scaler_pd = StandardScalerPD()\n",
    "            X_train_clip = clipper.fit_transform(X_train)\n",
    "            X_train_proc = scaler_pd.fit_transform(X_train_clip)\n",
    "\n",
    "            # Fit SJM with best hyperparams (which may have been updated in re-CV)\n",
    "            lam = best_hyperparams[fac][\"best_lambda\"]\n",
    "            kp = best_hyperparams[fac][\"best_kappa\"]\n",
    "            max_feats = int(kp**2)\n",
    "\n",
    "            sjm = SparseJumpModel(n_components=2, max_feats=max_feats, jump_penalty=lam)\n",
    "            sjm.fit(X_train_proc, ret_ser=y_train, sort_by=\"cumret\")\n",
    "\n",
    "            # -----------------------------------------------------------------\n",
    "            # 3B) Daily \"online\" inference from refit_date+1 to next_refit_date\n",
    "            # -----------------------------------------------------------------\n",
    "            # For each day in test_dates_chunk, do day-by-day scaling and inference\n",
    "            X_test_proc_list = []\n",
    "            for day in test_dates_chunk:\n",
    "                # Build incremental history from [train_start : day]\n",
    "                # For online scaling, we use all available history up to the current day\n",
    "                X_hist = X.loc[:day]\n",
    "\n",
    "                # Clip & scale X_hist\n",
    "                ctemp = DataClipperStd(mul=3.0)\n",
    "                X_hist_clip = ctemp.fit_transform(X_hist)\n",
    "\n",
    "                stemp = StandardScaler()\n",
    "                X_hist_scal = stemp.fit_transform(X_hist_clip)\n",
    "\n",
    "                # Transform just the row for 'day'\n",
    "                if day in X.index:\n",
    "                    X_day = X.loc[[day]]\n",
    "                    X_day_clip = ctemp.transform(X_day)\n",
    "                    X_day_scal = stemp.transform(X_day_clip)\n",
    "                    X_test_proc_list.append(\n",
    "                        pd.Series(X_day_scal.flatten(), index=X_day.columns, name=day)\n",
    "                    )\n",
    "\n",
    "            X_test_proc_df = pd.DataFrame(X_test_proc_list).sort_index()\n",
    "\n",
    "            # Predict states\n",
    "            if not X_test_proc_df.empty:\n",
    "                states_chunk = sjm.predict_online(X_test_proc_df)\n",
    "                states_series = pd.Series(states_chunk, index=X_test_proc_df.index)\n",
    "            else:\n",
    "                states_series = pd.Series(dtype=float)\n",
    "\n",
    "            # Accumulate to the \"global\" daily states for that factor\n",
    "            series_to_concat = [s for s in [all_daily_states[fac], states_series] if not s.empty]\n",
    "            if series_to_concat:\n",
    "                all_daily_states[fac] = pd.concat(series_to_concat).sort_index()\n",
    "            else:\n",
    "                all_daily_states[fac] = pd.Series(dtype=float)\n",
    "\n",
    "            # Store factor info needed for BL\n",
    "            factor_dict_chunk[fac] = {\n",
    "                \"ret\": ret_full,  # full factor ret\n",
    "                \"states\": states_series,\n",
    "                \"regime_returns\": {\n",
    "                    0: sjm.ret_[0],\n",
    "                    1: sjm.ret_[1],\n",
    "                },\n",
    "            }\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3C) Run BL daily over test_dates_chunk\n",
    "        # ---------------------------------------------------------------------\n",
    "        weights_chunk = run_bl_portfolio_pyopt_expanding(\n",
    "            factor_dict=factor_dict_chunk,\n",
    "            returns_df=full_df,\n",
    "            test_index=test_dates_chunk,\n",
    "            tau=0.05,\n",
    "            delta=2.5\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3D) Compute daily returns for test_dates_chunk\n",
    "        # ---------------------------------------------------------------------\n",
    "        daily_factor_rets = full_df.loc[test_dates_chunk, factors]\n",
    "        portfolio_rets = (weights_chunk * daily_factor_rets).sum(axis=1)\n",
    "\n",
    "        # Store results\n",
    "        all_daily_weights.append(weights_chunk)\n",
    "        all_daily_rets.append(portfolio_rets)\n",
    "\n",
    "    ############################################################################\n",
    "    # 4) Analyze Results and Compare to Benchmarks\n",
    "    ############################################################################\n",
    "    if len(all_daily_rets) == 0:\n",
    "        raise ValueError(\"No daily returns computed. Check your date ranges or data.\")\n",
    "\n",
    "    all_portfolio_rets = pd.concat(all_daily_rets).sort_index()\n",
    "    all_weights_df = pd.concat(all_daily_weights).sort_index()\n",
    "    cumrets = all_portfolio_rets.cumsum()\n",
    "\n",
    "    # Compare to equal-weight benchmark\n",
    "    overlap_idx = all_portfolio_rets.index.intersection(full_df.index)\n",
    "    test_returns_df = full_df.loc[overlap_idx, :]\n",
    "    ew_bench_rets = test_returns_df.mean(axis=1)\n",
    "    ew_bench_cum = ew_bench_rets.cumsum()\n",
    "    \n",
    "    # Also compute quarterly-rebalanced EW portfolio (as in your code)\n",
    "    quarterly_ends = test_returns_df.index.to_series().resample(\"QE\").last().dropna()\n",
    "    n_assets = test_returns_df.shape[1]\n",
    "    quarterly_ew_rets = pd.Series(index=test_returns_df.index, dtype=float)\n",
    "\n",
    "    for i in range(len(quarterly_ends) - 1):\n",
    "        start_q = quarterly_ends.iloc[i]\n",
    "        end_q = quarterly_ends.iloc[i + 1]\n",
    "        mask = (test_returns_df.index > start_q) & (test_returns_df.index <= end_q)\n",
    "        chunk_dates = test_returns_df.index[mask]\n",
    "        \n",
    "        w = np.ones(n_assets) / n_assets\n",
    "        \n",
    "        for day in chunk_dates:\n",
    "            r_i = test_returns_df.loc[day].values\n",
    "            r_p = np.dot(w, r_i)\n",
    "            quarterly_ew_rets.loc[day] = r_p\n",
    "            \n",
    "            w = w * (1 + r_i)\n",
    "            if (1 + r_p) != 0:\n",
    "                w /= (1 + r_p)\n",
    "\n",
    "    quarterly_ew_rets = quarterly_ew_rets.dropna()\n",
    "    quarterly_ew_cum = quarterly_ew_rets.cumsum()\n",
    "\n",
    "    ############################################################################\n",
    "    # 5) Plot Results and Print Performance Statistics\n",
    "    ############################################################################\n",
    "    # Plot 1: Cumulative returns comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    cumrets.plot(label=f\"Dynamic BL with {WINDOW_TYPE.capitalize()} Window ({MIN_TRAINING_YEARS}-{MAX_TRAINING_YEARS} yrs)\")\n",
    "    ew_bench_cum.plot(label=\"Daily EW Benchmark\", linestyle=\"--\")\n",
    "    quarterly_ew_cum.plot(label=\"Quarterly EW Benchmark\", linestyle=\"-.\")\n",
    "    \n",
    "    # Add vertical lines at re-cross-validation dates\n",
    "    for date in recross_val_boundaries:\n",
    "        if date in cumrets.index:\n",
    "            plt.axvline(x=date, color='r', linestyle='--', alpha=0.3, \n",
    "                        label=\"Re-Cross-Validation\" if date == recross_val_boundaries[0] else \"\")\n",
    "    \n",
    "    plt.title(\"Cumulative Returns Comparison (Test Period)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Factor weights over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    all_weights_df.plot()\n",
    "    plt.title(\"Factor Weights Over Time\")\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 3: Training window size over time (new plot to visualize window size)\n",
    "    if training_window_history:\n",
    "        tw_df = pd.DataFrame(training_window_history)\n",
    "        tw_df.set_index('date', inplace=True)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(tw_df.index, tw_df['years'])\n",
    "        plt.title(f\"Training Window Size Over Time ({WINDOW_TYPE} Window)\")\n",
    "        plt.ylabel(\"Window Size (Years)\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Plot 4: Regime visualization for each factor (optional)\n",
    "    for factor_name in factors:\n",
    "        if not all_daily_states[factor_name].empty:\n",
    "            states_df = all_daily_states[factor_name]\n",
    "            active_ret = factor_data_dict[factor_name][\"active_ret\"]\n",
    "            common_idx = states_df.index.intersection(active_ret.index)\n",
    "            \n",
    "            if len(common_idx) > 0:\n",
    "                ax, ax2 = plot_regimes_and_cumret(\n",
    "                    states_df.loc[common_idx], \n",
    "                    active_ret.loc[common_idx]\n",
    "                )\n",
    "                ax.set_title(f\"{factor_name} - Test Period Regimes\")\n",
    "                plt.show()\n",
    "\n",
    "    # Print performance statistics\n",
    "    print(\"\\n==============================================================\")\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"==============================================================\")\n",
    "    print(f\"Strategy: Dynamic BL with {WINDOW_TYPE.capitalize()} Window ({MIN_TRAINING_YEARS}-{MAX_TRAINING_YEARS} yrs)\")\n",
    "    print(f\"Period: {all_portfolio_rets.index[0]} to {all_portfolio_rets.index[-1]}\")\n",
    "    print(f\"Number of trading days: {len(all_portfolio_rets)}\")\n",
    "    print(\"\\nAnnualized Performance Metrics:\")\n",
    "    print(f\"Daily EW Benchmark Sharpe: {annualized_sharpe(ew_bench_rets):.4f}\")\n",
    "    print(f\"Quarterly EW Benchmark Sharpe: {annualized_sharpe(quarterly_ew_rets):.4f}\")\n",
    "    print(f\"Dynamic BL Portfolio Sharpe: {annualized_sharpe(all_portfolio_rets):.4f}\")\n",
    "    print(f\"\\nTotal Returns:\")\n",
    "    print(f\"Daily EW Benchmark Return: {ew_bench_cum.iloc[-1]:.4f}\")\n",
    "    print(f\"Quarterly EW Benchmark Return: {quarterly_ew_cum.iloc[-1]:.4f}\")\n",
    "    print(f\"Dynamic BL Portfolio Return: {cumrets.iloc[-1]:.4f}\")\n",
    "    \n",
    "    # Print hyperparameter evolution\n",
    "    print(\"\\n==============================================================\")\n",
    "    print(\"HYPERPARAMETER EVOLUTION\")\n",
    "    print(\"==============================================================\")\n",
    "    for fac in factors:\n",
    "        if hyperparameter_history[fac]:\n",
    "            print(f\"\\nFactor: {fac}\")\n",
    "            for entry in hyperparameter_history[fac]:\n",
    "                print(f\"Date: {entry['date']}\")\n",
    "                print(f\"  Lambda: {entry['old_lambda']:.2f} -> {entry['new_lambda']:.2f}\")\n",
    "                print(f\"  Kappa²: {int(entry['old_kappa']**2)} -> {int(entry['new_kappa']**2)}\")\n",
    "                print(f\"  Sharpe: {entry['sharpe']:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\nFactor: {fac} - No parameter changes\")\n",
    "\n",
    "    # Print training window evolution\n",
    "    if training_window_history:\n",
    "        print(\"\\n==============================================================\")\n",
    "        print(f\"TRAINING WINDOW EVOLUTION ({WINDOW_TYPE.upper()})\")\n",
    "        print(\"==============================================================\")\n",
    "        for entry in training_window_history:\n",
    "            print(f\"Date: {entry['date']}\")\n",
    "            print(f\"  Window: {entry['start_date']} to {entry['end_date']}\")\n",
    "            print(f\"  Size: {entry['years']:.2f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1ac09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
