{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7646fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from jumpmodels.utils import filter_date_range\n",
    "from jumpmodels.preprocess import StandardScalerPD, DataClipperStd\n",
    "from jumpmodels.sparse_jump import SparseJumpModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pypfopt.black_litterman import BlackLittermanModel, market_implied_prior_returns\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pypfopt import exceptions \n",
    "\n",
    "#sys.path.append('/Users/victor/Documents/thesis_vri_vp/vic_new')         # for mac\n",
    "sys.path.append('C:\\\\Users\\\\victo\\\\git_new\\\\thesis_vri_vp\\\\vic_new')      # for windows\n",
    "from feature_set_v2 import MergedDataLoader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e65d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Global parameters ------------------------------------------------------------------\n",
    "REFIT_FREQ        = \"ME\"        \n",
    "MIN_TRAINING_YEARS= 8\n",
    "MAX_TRAINING_YEARS= 12\n",
    "INITIAL_TRAIN_START = \"2002-05-31\"\n",
    "test_start        = \"2017-01-01\"\n",
    "\n",
    "# Pick method to drive backtest: \"grid\", \"bayes\" or \"history\" ---------------------------\n",
    "cv_choice = \"bayes\"\n",
    "\n",
    "# Paths & tickers -----------------------------------------------------------------------\n",
    "script_dir = os.getcwd()\n",
    "base_dir   = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))\n",
    "data_dir   = os.path.join(base_dir, \"data_new\")\n",
    "\n",
    "factor_file = os.path.join(data_dir, \"1estimation_index_returns.csv\")\n",
    "market_file = os.path.join(data_dir, \"1macro_data.csv\")\n",
    "etf_file    = os.path.join(data_dir, \"2trading_etf_returns_aligned.csv\")\n",
    "\n",
    "factors = [\"iwf\", \"mtum\", \"qual\", \"size\", \"usmv\", \"vlue\"]   # used everywhere\n",
    "\n",
    "grid_df    = pd.read_parquet(\"cv_params_grid.parquet\")\n",
    "bayes_df   = pd.read_parquet(\"cv_params_bayes_v2.parquet\") # v2 is the one searching between 20-2000\n",
    "history_df = pd.read_parquet(\"cv_params_history.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4281c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# HYPERPARAMETERS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "df_map = {\n",
    "    \"grid\":    grid_df,\n",
    "    \"bayes\":   bayes_df,\n",
    "    \"history\": history_df\n",
    "}\n",
    "cv_df = df_map[cv_choice]\n",
    "\n",
    "# ─────────────────────────────────────────────────────\n",
    "# HYPERPARAMETER SMOOTHING SETUP\n",
    "# ─────────────────────────────────────────────────────\n",
    "# pick one\n",
    "SMOOTH_METHOD = \"none\"   # options: \"none\", \"rolling_median\", \"ewma\"\n",
    "SMOOTH_WINDOW = 3        # # of folds to include in the window\n",
    "# ─────────────────────────────────────────────────────\n",
    "\n",
    "# … right after cv_df = df_map[cv_choice] …\n",
    "cv_df = cv_df.sort_values([\"factor\",\"date\"])\n",
    "\n",
    "if SMOOTH_METHOD == \"none\":\n",
    "    # simply copy original λ & κ forward\n",
    "    cv_df[\"sm_lambda\"] = cv_df[\"best_lambda\"]\n",
    "    cv_df[\"sm_kappa\"]  = cv_df[\"best_kappa\"]\n",
    "\n",
    "elif SMOOTH_METHOD == \"rolling_median\":\n",
    "    # Centered rolling median\n",
    "    cv_df[\"sm_lambda\"] = (\n",
    "        cv_df\n",
    "        .groupby(\"factor\")[\"best_lambda\"]\n",
    "        .transform(lambda x: x.rolling(SMOOTH_WINDOW, min_periods=1, center=True).median())\n",
    "    )\n",
    "    cv_df[\"sm_kappa\"] = (\n",
    "        cv_df\n",
    "        .groupby(\"factor\")[\"best_kappa\"]\n",
    "        .transform(lambda x: x.rolling(SMOOTH_WINDOW, min_periods=1, center=True).median())\n",
    "    )\n",
    "\n",
    "elif SMOOTH_METHOD == \"ewma\":\n",
    "    # Exponential‐weight moving average\n",
    "    cv_df[\"sm_lambda\"] = (\n",
    "        cv_df\n",
    "        .groupby(\"factor\")[\"best_lambda\"]\n",
    "        .transform(lambda x: x.ewm(span=SMOOTH_WINDOW, min_periods=1).mean())\n",
    "    )\n",
    "    cv_df[\"sm_kappa\"] = (\n",
    "        cv_df\n",
    "        .groupby(\"factor\")[\"best_kappa\"]\n",
    "        .transform(lambda x: x.ewm(span=SMOOTH_WINDOW, min_periods=1).mean())\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown SMOOTH_METHOD {SMOOTH_METHOD!r}\")\n",
    "\n",
    "# round κ back to integer\n",
    "cv_df[\"sm_kappa\"] = cv_df[\"sm_kappa\"].round().astype(int)\n",
    "\n",
    "# overwrite with the chosen values\n",
    "cv_df[\"best_lambda\"] = cv_df[\"sm_lambda\"]\n",
    "cv_df[\"best_kappa\"]  = cv_df[\"sm_kappa\"]\n",
    "\n",
    "# (optional) drop helpers\n",
    "cv_df.drop(columns=[\"sm_lambda\",\"sm_kappa\"], inplace=True)\n",
    "\n",
    "# ─────────────────────────────────────────────────────\n",
    "# Now build saved_hyperparams exactly as before\n",
    "# ─────────────────────────────────────────────────────\n",
    "saved_hyperparams = {}\n",
    "for fac in factors:\n",
    "    sub = cv_df[cv_df[\"factor\"] == fac].sort_values(\"date\")\n",
    "    saved_hyperparams[fac] = [\n",
    "        {\n",
    "            \"date\":      row[\"date\"],\n",
    "            \"new_lambda\": row[\"best_lambda\"],\n",
    "            \"new_kappa\":  row[\"best_kappa\"]\n",
    "        }\n",
    "        for _, row in sub.iterrows()\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c8ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for iwf\n",
      "Loading data for mtum\n",
      "Loading data for qual\n",
      "Loading data for size\n",
      "Loading data for usmv\n",
      "Loading data for vlue\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# DATA‑LOADING BLOCK  (pulled from old notebook)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) Load full data for every factor + market ------------------------------------------------\n",
    "factor_data_dict  = {}\n",
    "factor_returns_ls = []\n",
    "\n",
    "for fac in factors:\n",
    "    print(f\"Loading data for {fac}\")\n",
    "    data = MergedDataLoader(\n",
    "        factor_file=factor_file,\n",
    "        market_file=market_file,\n",
    "        ver=\"v2\",\n",
    "        factor_col=fac\n",
    "    ).load()\n",
    "\n",
    "    common_idx = (data.X.index\n",
    "                  .intersection(data.ret_ser.index)\n",
    "                  .intersection(data.market_ser.index))\n",
    "\n",
    "    X_full        = data.X.loc[common_idx]\n",
    "    fac_ret_full  = data.ret_ser.loc[common_idx]\n",
    "    mkt_ret_full  = data.market_ser.loc[common_idx]\n",
    "    active_ret    = fac_ret_full - mkt_ret_full\n",
    "\n",
    "    factor_data_dict[fac] = {\n",
    "        \"X\"        : X_full,\n",
    "        \"fac_ret\"  : fac_ret_full,\n",
    "        \"mkt_ret\"  : mkt_ret_full,\n",
    "        \"active_ret\": active_ret,\n",
    "    }\n",
    "    factor_returns_ls.append(fac_ret_full)\n",
    "\n",
    "# save last loop’s mkt_ret_full as market series\n",
    "all_market_ret = mkt_ret_full\n",
    "\n",
    "# 2) Assemble master return dataframe (factors + Market + rf) -------------------------------\n",
    "full_factors_df = pd.concat(factor_returns_ls, axis=1).dropna()\n",
    "full_df = pd.concat([full_factors_df, all_market_ret], axis=1).dropna()\n",
    "full_df.columns = factors + [\"Market\"]\n",
    "\n",
    "# risk‑free\n",
    "etf_df   = pd.read_csv(etf_file, index_col=0, parse_dates=True).dropna().sort_index()\n",
    "rf_ser   = etf_df[\"rf\"]\n",
    "full_df  = pd.concat([full_df, rf_ser], axis=1).dropna()\n",
    "full_df.columns = factors + [\"Market\", \"rf\"]\n",
    "\n",
    "# 3) Define test index (everything from 2017‑01‑01 on) --------------------------------------\n",
    "test_slice = full_df.loc[test_start:]\n",
    "test_index = test_slice.index.sort_values()\n",
    "# ──────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fbc3797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_12792\\3065937769.py:118: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  views[fac] = (pd.concat(views[fac])\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_12792\\3065937769.py:118: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  views[fac] = (pd.concat(views[fac])\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_12792\\3065937769.py:118: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  views[fac] = (pd.concat(views[fac])\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_12792\\3065937769.py:118: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  views[fac] = (pd.concat(views[fac])\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_12792\\3065937769.py:118: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  views[fac] = (pd.concat(views[fac])\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_12792\\3065937769.py:118: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  views[fac] = (pd.concat(views[fac])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1  BUILD & CACHE FACTOR‑VIEWS  (run once, takes minutes)\n",
    "# ------------------------------------------------------------\n",
    "VIEWS_FILE = \"bayes_factor_views_v2.pkl\" # \"SAVEfactor_views.pkl\" is the views for the outperforming sharpe run\n",
    "FORCE_REBUILD = False \n",
    "\n",
    "def _fit_one_factor(fac, refit_date, test_dates_chunk,\n",
    "                    factor_data_dict, hyperparams,\n",
    "                    min_years, max_years, init_start):\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def get_train_window(current_date, full_data):\n",
    "        train_end  = current_date\n",
    "        train_start= max(train_end - pd.DateOffset(years=max_years),\n",
    "                         pd.to_datetime(init_start))\n",
    "        if (train_end - train_start) < pd.Timedelta(days=365.25*min_years):\n",
    "            train_start = train_end - pd.DateOffset(years=min_years)\n",
    "        idx = full_data.index\n",
    "        subset = idx[(idx >= train_start) & (idx <= train_end)]\n",
    "        start_date, end_date = subset.min(), subset.max()\n",
    "        return start_date, end_date \n",
    "\n",
    "    # ---------- data ----------\n",
    "    fac_data = factor_data_dict[fac]\n",
    "    X   = fac_data[\"X\"]\n",
    "    ret = fac_data[\"fac_ret\"]\n",
    "    act = fac_data[\"active_ret\"]\n",
    "\n",
    "    lam = hyperparams[\"new_lambda\"]\n",
    "    kp  = hyperparams[\"new_kappa\"]\n",
    "    train_start, train_end = get_train_window(refit_date, X)\n",
    "\n",
    "    # ---------- preprocess ----------\n",
    "    clipper = DataClipperStd(mul=3.0)\n",
    "    scaler  = StandardScaler()\n",
    "    X_train = scaler.fit_transform(clipper.fit_transform(\n",
    "                 filter_date_range(X, train_start, train_end)))\n",
    "    active_train = filter_date_range(act, train_start, train_end)\n",
    "\n",
    "    # ---------- fit SJM ----------\n",
    "    sjm = SparseJumpModel(n_components=2,\n",
    "                          max_feats=int(kp**2),\n",
    "                          jump_penalty=lam)\n",
    "    \n",
    "    train_idx = filter_date_range(X, train_start, train_end).index\n",
    "    X_train_df = pd.DataFrame(X_train, index=train_idx, columns=X.columns)\n",
    "    sjm.fit(X_train_df, ret_ser=active_train, sort_by=\"cumret\")\n",
    "\n",
    "    ret_train = filter_date_range(ret, train_start, train_end)\n",
    "\n",
    "    # regime‑level abs returns\n",
    "    train_states = sjm.predict(X_train_df)\n",
    "    abs_ret = {}\n",
    "    for st in range(2):\n",
    "        st_idx = (train_states==st)\n",
    "        abs_ret[st] = ret_train.loc[st_idx].mean() * 252\n",
    "\n",
    "    # ---------- online prediction for test dates ----------\n",
    "    states = {}\n",
    "    for day in test_dates_chunk:\n",
    "        X_hist = X.loc[:day]                          # all history up to 'day'\n",
    "        temp_clipper = DataClipperStd(mul=3.0)\n",
    "        X_hist_clip  = temp_clipper.fit_transform(X_hist)\n",
    "\n",
    "        temp_scaler  = StandardScaler()\n",
    "        _ = temp_scaler.fit_transform(X_hist_clip)    # fit on *all* history\n",
    "\n",
    "        if day in X.index:\n",
    "            X_day_clip   = temp_clipper.transform(X.loc[[day]])\n",
    "            X_day_scaled = temp_scaler.transform(X_day_clip)\n",
    "            states[day]  = sjm.predict_online(\n",
    "                pd.DataFrame(X_day_scaled,\n",
    "                            index=[day],\n",
    "                            columns=X.columns)).iloc[0]\n",
    "\n",
    "    # assemble mini‑df for this factor & period\n",
    "    out = pd.DataFrame({\"state\": pd.Series(states)},\n",
    "                       index=list(states.keys()))\n",
    "    out[\"ann_abs_ret\"] = out[\"state\"].map(abs_ret)\n",
    "    return fac, out\n",
    "\n",
    "def build_factor_views(factor_data_dict, saved_hyperparams, factors,\n",
    "                       test_index,\n",
    "                       refit_freq=\"ME\", min_years=8, max_years=12,\n",
    "                       init_start=\"2002-05-31\"):\n",
    "\n",
    "    views = {f:[] for f in factors}\n",
    "    refit_dates = (test_index.to_series()\n",
    "                   .resample(refit_freq)\n",
    "                   .last()\n",
    "                   .dropna())\n",
    "\n",
    "    for j, refit_date in enumerate(refit_dates):\n",
    "        if j < len(refit_dates)-1:\n",
    "            next_refit = refit_dates.iloc[j+1]\n",
    "        else:\n",
    "            next_refit = test_index[-1]\n",
    "        test_mask = (test_index>refit_date)&(test_index<=next_refit)\n",
    "        test_chunk = test_index[test_mask]\n",
    "\n",
    "        # ---- parallel over factors ----\n",
    "        jobs = []\n",
    "        for fac in factors:\n",
    "            # latest hyperparams before refit_date\n",
    "            hp_hist = [h for h in saved_hyperparams[fac]\n",
    "                       if pd.to_datetime(h[\"date\"])<=refit_date]\n",
    "            if not hp_hist: continue\n",
    "            hp = hp_hist[-1]\n",
    "            jobs.append(delayed(_fit_one_factor)(\n",
    "                fac, refit_date, test_chunk,\n",
    "                factor_data_dict, hp,\n",
    "                min_years, max_years, init_start))\n",
    "        for fac, df in Parallel(n_jobs=-1)(jobs):\n",
    "            views[fac].append(df)\n",
    "\n",
    "    # concat & tidy\n",
    "    for fac in factors:\n",
    "        views[fac] = (pd.concat(views[fac])\n",
    "                      .sort_index()\n",
    "                      .loc[:,[\"state\",\"ann_abs_ret\"]])\n",
    "    return views\n",
    "\n",
    "\n",
    "# --------- build or load ----------         \n",
    "if FORCE_REBUILD or not os.path.exists(VIEWS_FILE):\n",
    "    factor_views = build_factor_views(factor_data_dict, saved_hyperparams, factors, \n",
    "                                      test_index,\n",
    "                                      refit_freq=REFIT_FREQ, \n",
    "                                      min_years=8, max_years=12, init_start=\"2002-05-31\")\n",
    "    with open(VIEWS_FILE, \"wb\") as f:\n",
    "        pickle.dump(factor_views, f)\n",
    "else:\n",
    "    with open(VIEWS_FILE, \"rb\") as f:\n",
    "        factor_views = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a007d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2  FAST BLACK‑LITTERMAN FUNCTION  (run as often as you like)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def ewm_covariance(returns, halflife=126, min_periods=60):\n",
    "    ewm_cov = returns.ewm(halflife=halflife,\n",
    "                          adjust=False,\n",
    "                          min_periods=min_periods).cov()\n",
    "    if returns.empty: return pd.DataFrame()\n",
    "    return ewm_cov.loc[returns.index[-1]]\n",
    "\n",
    "def detect_state_shifts(views, factors):\n",
    "    # 1 col per factor with the model‑state\n",
    "    state_df = pd.concat({f: views[f][\"state\"] for f in factors}, axis=1)\n",
    "    # True when *any* factor changes state vs. the day before\n",
    "    return state_df.ne(state_df.shift()).any(axis=1)\n",
    "\n",
    "def run_bl_once(views, returns_df,\n",
    "                shift_series=None,\n",
    "                tau=0.05, delta=2.5,\n",
    "                trade_market=True,\n",
    "                use_bl_cov=False,\n",
    "                allow_market_short=False,\n",
    "                allow_factor_short=False,\n",
    "                use_bl_prior=False,     \n",
    "                fallback_strategy=\"HOLD_RFR\", # \"HOLD_RFR\", \"SHORT_MARKET\"\n",
    "                tcost=0.0005):\n",
    "\n",
    "    assets  = returns_df.columns.tolist()\n",
    "    factors = list(views.keys())\n",
    "    if trade_market:\n",
    "        trade_assets = [a for a in returns_df.columns if a != \"rf\"]\n",
    "    else:\n",
    "        trade_assets = [a for a in returns_df.columns\n",
    "                        if a not in {\"rf\", \"Market\"}]\n",
    "    cash_asset = \"rf\"\n",
    "\n",
    "    market_caps = {etf: 1.0 for etf in trade_assets}\n",
    "\n",
    "    # ---------- per‑asset bounds ----------\n",
    "    bounds = []\n",
    "    for a in trade_assets:\n",
    "        if a == \"Market\":\n",
    "            bounds.append((-1, 1) if allow_market_short else (0, 1))\n",
    "        else:\n",
    "            bounds.append((-1, 1) if allow_factor_short else (0, 1))\n",
    "\n",
    "    w = pd.DataFrame(index=returns_df.index,\n",
    "                     columns=trade_assets + [cash_asset], # was + [cash_asset]\n",
    "                     dtype=float)\n",
    "\n",
    "    for t in returns_df.index:\n",
    "        # ------ carry weights forward if no trade today ------\n",
    "        if shift_series is not None and not shift_series.loc[t]:\n",
    "                w.loc[t] = w.shift(1).loc[t] \n",
    "                continue\n",
    "        \n",
    "        # ------ optimiser block ------\n",
    "        hist  = returns_df[trade_assets].loc[:t].iloc[:-1]\n",
    "        cov   = ewm_covariance(hist) * 252\n",
    "        if cov.empty or cov.isna().any().any():\n",
    "            continue\n",
    "\n",
    "        if use_bl_prior:\n",
    "            prior_for_bl = market_implied_prior_returns(market_caps, delta, cov)\n",
    "        else:\n",
    "            prior_for_bl = \"equal\"          # or None\n",
    "\n",
    "\n",
    "        q = {fac: views[fac].loc[t, \"ann_abs_ret\"] for fac in factors}\n",
    "\n",
    "        bl = BlackLittermanModel(\n",
    "                cov,\n",
    "                pi=prior_for_bl,\n",
    "                tau=tau,\n",
    "                delta=delta,\n",
    "                absolute_views=q)\n",
    "\n",
    "        cov_for_ef = bl.bl_cov() if use_bl_cov else cov\n",
    "\n",
    "        ef = EfficientFrontier(bl.bl_returns(), cov_for_ef,\n",
    "                               weight_bounds=bounds,\n",
    "                               solver=\"SCS\"                         # \"OSQP\" is standard and probably fastest \n",
    "                               )\n",
    "\n",
    "        rf_annual = returns_df.loc[t, cash_asset] * 252\n",
    "\n",
    "        if (bl.bl_returns() <= rf_annual).all():\n",
    "            w.loc[t] = 0.0                                             # start clean\n",
    "\n",
    "            if fallback_strategy == \"HOLD_RFR\":\n",
    "                w.loc[t, cash_asset] = 1.0                             # 100 % cash\n",
    "\n",
    "            elif fallback_strategy == \"SHORT_MARKET\" and \"Market\" in trade_assets:\n",
    "                w.loc[t, \"Market\"] = -1.0\n",
    "                w.loc[t, cash_asset] = 1.0      # offset the short\n",
    "\n",
    "\n",
    "            # add more elif blocks here for any new fallbacks you invent\n",
    "            continue                                                   # next date\n",
    "\n",
    "        # —— Otherwise run the optimiser (now guaranteed feasible) ——\n",
    "        ef.max_sharpe(risk_free_rate=rf_annual)\n",
    "\n",
    "        w_t = ef.clean_weights()\n",
    "        w.loc[t, trade_assets] = pd.Series(w_t)\n",
    "\n",
    "    # ---------- P&L ----------\n",
    "    pnl = (w.shift(1).fillna(0) * returns_df).sum(axis=1)\n",
    "    if tcost > 0:\n",
    "        pnl -= w.diff().abs().sum(axis=1).fillna(0) * tcost\n",
    "\n",
    "    return w, pnl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15935178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           Strategy  Sharpe  Turnover\n",
      "     Long only base   0.254    42.881\n",
      "         + BL prior   0.349    51.211\n",
      "           + BL cov   0.254    42.793\n",
      "+ BL cov + BL prior   0.353    51.116\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3  QUICK EXPERIMENTS\n",
    "# ------------------------------------------------------------\n",
    "def annualized_sharpe(r):          # helper\n",
    "    return (r.mean() / r.std()) * np.sqrt(252)\n",
    "\n",
    "def ann_turnover(w):\n",
    "    daily_turn = w.diff().abs().sum(axis=1).mean()\n",
    "    return daily_turn * 252\n",
    "\n",
    "cfgs = [\n",
    "    dict(label=\"Long only base\",  tau=0.05, delta=2.5,\n",
    "         use_bl_cov=False, use_bl_prior=False, allow_market_short=False, allow_factor_short=False),\n",
    "    dict(label=\"+ BL prior\",  tau=0.05, delta=2.5,\n",
    "         use_bl_cov=False, use_bl_prior=True, allow_market_short=False, allow_factor_short=False),\n",
    "#     dict(label=\"Long‑Only - allow mkt short\",  tau=0.05, delta=2.5,\n",
    "#          use_bl_cov=False, allow_market_short=True, allow_factor_short=False),\n",
    "    dict(label=\"+ BL cov\",  tau=0.05, delta=2.5,\n",
    "         use_bl_cov=True, use_bl_prior=False, allow_market_short=False, allow_factor_short=False),\n",
    "    dict(label=\"+ BL cov + BL prior\",  tau=0.05, delta=2.5,\n",
    "         use_bl_cov=True, use_bl_prior=True, allow_market_short=False, allow_factor_short=False)\n",
    "#     dict(label=\"BL cov - allow mkt short\",  tau=0.05, delta=2.5,\n",
    "#          use_bl_cov=True, allow_market_short=True, allow_factor_short=False),\n",
    "]\n",
    "\n",
    "#     dict(label=\"L/S Market\", tau=0.05, delta=2.5,\n",
    "#         use_bl_cov=False, allow_market_short=True,  allow_factor_short=False),\n",
    "\n",
    "test_df = full_df.loc[test_index]\n",
    "\n",
    "shift_days = detect_state_shifts(factor_views, factors).reindex(test_df.index, fill_value=False)\n",
    "\n",
    "shift_days.iloc[0] = True\n",
    "\n",
    "run_results = {}                   # label → dict(rets, wts, cfg)\n",
    "for c in cfgs:\n",
    "    label = c.pop(\"label\")         # remove label before **c\n",
    "    wts, rets = run_bl_once(factor_views, test_df, shift_series=shift_days, **c)  # set shift_series to None to trade daily. set to shift_days to only trade on regime shifts\n",
    "    run_results[label] = dict(returns=rets, weights=wts, cfg=c)\n",
    "    #print(f\"{label:12s}  Sharpe {annualized_sharpe(rets):6.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "rows = []\n",
    "for label, res in run_results.items():\n",
    "    rows.append({\n",
    "        \"Strategy\": label,\n",
    "        \"Sharpe\": annualized_sharpe(res[\"returns\"]),\n",
    "        \"Turnover\": ann_turnover(res[\"weights\"])\n",
    "    })\n",
    "\n",
    "# make and print the table\n",
    "df_table   = pd.DataFrame(rows)\n",
    "print(df_table.to_string(index=False,float_format=lambda x: f\"{x:.3f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c3306b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Strategy  Sharpe  Turnover\n",
      "     Long only base   0.603    16.801\n",
      "         + BL prior   0.507    20.849\n",
      "           + BL cov   0.605    16.777\n",
      "+ BL cov + BL prior   0.509    20.738\n"
     ]
    }
   ],
   "source": [
    "# build a list of dicts from your run_results\n",
    "rows = []\n",
    "for label, res in run_results.items():\n",
    "    rows.append({\n",
    "        \"Strategy\": label,\n",
    "        \"Sharpe\": annualized_sharpe(res[\"returns\"]),\n",
    "        \"Turnover\": ann_turnover(res[\"weights\"])\n",
    "    })\n",
    "\n",
    "# make and print the table\n",
    "df_table   = pd.DataFrame(rows)\n",
    "print(df_table.to_string(index=False,float_format=lambda x: f\"{x:.3f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb0b4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e754cec6ee4a5d92a26fdf03cd1931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='Compare:', index=(0, 1), options=('Long only base', '+ BL pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff4f1d6de924f3a81c85c17be77420b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(SelectMultiple(description='Compare:', index=(0, 1), options=('Long only base', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# Interactive comparison of configs  +  weights‑over‑time viewer\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import SelectMultiple, Dropdown, ToggleButtons, VBox, HBox, interact\n",
    "\n",
    "# 1) -------------  compute the fixed quarterly‑EW benchmark (once) ----\n",
    "try:\n",
    "    quarterly_ew_rets\n",
    "    quarterly_ew_cum\n",
    "except NameError:\n",
    "    TCOST = 0.0005                                   # keep in‑sync with runs\n",
    "    overlap_idx     = test_index.intersection(etf_df.index)\n",
    "    test_returns_df = etf_df.loc[overlap_idx]        # factors + Market + rf\n",
    "    n_assets        = test_returns_df.shape[1]\n",
    "\n",
    "    first_test_date = test_returns_df.index[0]\n",
    "    q_dates = (test_returns_df.index.to_series()\n",
    "               .resample(\"QE\").last().dropna().index)\n",
    "    if first_test_date not in q_dates:\n",
    "        q_dates = q_dates.insert(0, first_test_date)\n",
    "\n",
    "    prev_w            = np.zeros(n_assets)\n",
    "    quarterly_ew_rets = pd.Series(index=test_returns_df.index, dtype=float)\n",
    "\n",
    "    for i in range(len(q_dates) - 1):\n",
    "        start_q, end_q = q_dates[i], q_dates[i + 1]\n",
    "        mask    = (test_returns_df.index > start_q) & (test_returns_df.index <= end_q)\n",
    "        dates   = test_returns_df.index[mask]\n",
    "\n",
    "        w              = np.ones(n_assets) / n_assets             # new EW weights\n",
    "        turnover       = np.sum(np.abs(w - prev_w))\n",
    "        rebalance_cost = turnover * TCOST\n",
    "\n",
    "        for day in dates:\n",
    "            r_i = test_returns_df.loc[day].values\n",
    "            r_p = np.dot(w, r_i) - rebalance_cost\n",
    "            quarterly_ew_rets.loc[day] = r_p\n",
    "\n",
    "            w = w * (1 + r_i)\n",
    "            if (1 + r_p) != 0:\n",
    "                w /= (1 + r_p)\n",
    "            rebalance_cost = 0.0                                   # cost once only\n",
    "        prev_w = w.copy()\n",
    "\n",
    "    quarterly_ew_rets = quarterly_ew_rets.dropna()\n",
    "    quarterly_ew_cum  = quarterly_ew_rets.cumsum()\n",
    "\n",
    "# 2) -------------  widgets for return / Sharpe / weights --------------\n",
    "labels   = list(run_results)\n",
    "cmp_sel  = SelectMultiple(\n",
    "    options     = labels,\n",
    "    value       = tuple(labels[:2]),         # default: first two configs\n",
    "    description = \"Compare:\",\n",
    "    rows        = min(8, len(labels)),\n",
    "    style       = {\"description_width\":\"70px\"},\n",
    ")\n",
    "wgt_sel  = Dropdown(\n",
    "    options     = labels,\n",
    "    value       = labels[0],                 # default: first config\n",
    "    description = \"Weights:\",\n",
    "    style       = {\"description_width\":\"70px\"},\n",
    ")\n",
    "sign_sel = ToggleButtons(\n",
    "    options     = [(\"Both\", \"both\"), (\"Positive\", \"pos\"), (\"Negative\", \"neg\")],\n",
    "    value       = \"both\",\n",
    "    description = \"Show:\",\n",
    "    style       = {\"description_width\":\"70px\"},\n",
    ")\n",
    "\n",
    "def annualized_sharpe(x):\n",
    "    return (x.mean() / x.std()) * np.sqrt(252)\n",
    "\n",
    "# build list of all dates where any config actually has weights\n",
    "active_dates = pd.Index([])\n",
    "for cfg in run_results.values():\n",
    "    w = cfg[\"weights\"].drop(columns=\"rf\").dropna(how=\"all\")\n",
    "    active_dates = active_dates.union(w.index)\n",
    "active_dates = active_dates.sort_values()\n",
    "\n",
    "# filter EW returns to exactly those dates, then re‑cumulate\n",
    "ew_rets = quarterly_ew_rets.loc[quarterly_ew_rets.index.isin(active_dates)]\n",
    "ew_cum  = ew_rets.cumsum()\n",
    "\n",
    "def _update(compare, weights_cfg, sign_filter):\n",
    "    if not compare:\n",
    "        print(\"Pick ≥1 config to display.\"); return\n",
    "\n",
    "    # ---- cumulative returns ----\n",
    "    plt.figure(figsize=(12,6))\n",
    "    ew_cum.plot(label=\"Quarterly EW (5 bps)\", lw=2)\n",
    "    for lab in compare: \n",
    "        run_results[lab][\"returns\"].cumsum().plot(label=lab, lw=1.5)\n",
    "    plt.title(\"Cumulative Returns\"); plt.grid(True); plt.legend(); plt.show()\n",
    "\n",
    "    # ---- Sharpe table ----\n",
    "    print(\"Annualised Sharpe ratios\")\n",
    "    print(f\"  EW benchmark : {annualized_sharpe(ew_rets):6.3f}\")\n",
    "    for lab in compare:\n",
    "        print(f\"  {lab:<12s}: {annualized_sharpe(run_results[lab]['returns']):6.3f}\")\n",
    "\n",
    "    # ---- weights over time for the chosen config ----\n",
    "    wdf = run_results[weights_cfg][\"weights\"].drop(columns='rf').dropna(how='all')\n",
    "    if sign_filter == \"pos\":        # show only positive weights\n",
    "        wdf = wdf.where(wdf > 0, 0)\n",
    "    elif sign_filter == \"neg\":      # show only negative weights\n",
    "        wdf = wdf.where(wdf < 0, 0)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.stackplot(wdf.index, wdf.T.values, labels=wdf.columns)\n",
    "    filt = {\"both\":\"(all)\", \"pos\":\"(positive)\", \"neg\":\"(negative)\"}[sign_filter]\n",
    "    plt.title(f\"Weights over time – {weights_cfg} {filt}\")\n",
    "    plt.xlabel(\"Trading days\"); plt.ylabel(\"Weight\")\n",
    "    plt.legend(loc=\"center left\", bbox_to_anchor=(1,.5), fontsize=\"small\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "ui = VBox([HBox([cmp_sel, wgt_sel, sign_sel])])\n",
    "interact(_update, compare=cmp_sel, weights_cfg=wgt_sel, sign_filter=sign_sel)\n",
    "ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43eeda72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helper to reproduce the BL expected‑return vector for ONE date ---\n",
    "from pypfopt.black_litterman import BlackLittermanModel, market_implied_prior_returns\n",
    "\n",
    "def bl_expected_returns_for_day(day, returns_df, views, tau=0.05, delta=2.5,\n",
    "                                use_bl_prior=False):\n",
    "    trade_assets = [c for c in returns_df.columns if c != \"rf\"]\n",
    "    cov = ewm_covariance(returns_df[trade_assets].loc[:day].iloc[:-1]) * 252\n",
    "    market_caps = {a: 1.0 for a in trade_assets}\n",
    "\n",
    "    if use_bl_prior:\n",
    "        pi = market_implied_prior_returns(market_caps, delta, cov)\n",
    "    else:\n",
    "        pi = \"equal\"          # same as in your main function\n",
    "\n",
    "    q = {fac: views[fac].loc[day, \"ann_abs_ret\"] for fac in views.keys()}\n",
    "\n",
    "    bl = BlackLittermanModel(cov, pi=pi, tau=tau, delta=delta,\n",
    "                             absolute_views=q)\n",
    "    return bl.bl_returns()    # pandas Series of ERs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2112843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long only base      : no positions\n",
      "+ BL prior          : no positions\n",
      "+ BL cov            : no positions\n",
      "+ BL cov + BL prior : no positions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# figure out your first‐month window\n",
    "first_date = min(cfg[\"weights\"].index.min() for cfg in run_results.values())\n",
    "end_1m    = first_date + pd.DateOffset(months=1)\n",
    "\n",
    "for name, cfg in run_results.items():\n",
    "    # drop the rf‐column and slice\n",
    "    w = cfg[\"weights\"].drop(columns=\"rf\")\n",
    "    w1 = w.loc[(w.index >= first_date) & (w.index <  end_1m)]\n",
    "    \n",
    "    # check if any weight ever deviates from zero\n",
    "    has_pos = (w1.abs() > 0).any().any()\n",
    "    print(f\"{name:20s}: {'positions' if has_pos else 'no positions'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793de9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
