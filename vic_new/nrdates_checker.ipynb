{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1deb736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# For loading data and feature engineering\n",
    "from feature_set_natracker import MergedDataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c71c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates in MergedDataLoader result for 'iwf': 5590\n",
      "Number of dates in the normal data: 5654\n"
     ]
    }
   ],
   "source": [
    "# Define file locations.\n",
    "directory = r\"C:\\Users\\victo\\git_new\\thesis_vri_vp\\data\"\n",
    "factor_file = os.path.join(directory, \"1estimation_index_returns.csv\")\n",
    "market_file = os.path.join(directory, \"1new_market_data.csv\")\n",
    "\n",
    "# List of factors to process.\n",
    "all_factors = [\"iwf\", \"mtum\", \"qual\", \"size\", \"usmv\", \"vlue\"]\n",
    "\n",
    "# Dictionary to hold data loaded via MergedDataLoader.\n",
    "merged_data = {}\n",
    "\n",
    "for factor_name in all_factors:\n",
    "    \n",
    "    # Load data using MergedDataLoader and store in the dictionary.\n",
    "    data = MergedDataLoader(\n",
    "        factor_file=factor_file,\n",
    "        market_file=market_file,\n",
    "        ver=\"v2\",\n",
    "        factor_col=factor_name\n",
    "    ).load(start_date=\"2002-05-31\", end_date=\"2025-04-03\")\n",
    "    \n",
    "    merged_data[factor_name] = data\n",
    "\n",
    "# Load estimation_index_returns.csv normally\n",
    "normal_data = pd.read_csv(factor_file, parse_dates=[\"date\"]).dropna()\n",
    "normal_data.set_index(\"date\", inplace=True)\n",
    "\n",
    "# Example: compare date counts using the first factor's data for the merged results.\n",
    "first_factor = all_factors[0]\n",
    "merged_dates = merged_data[first_factor].X.index.unique()\n",
    "normal_dates = normal_data.index.unique()\n",
    "\n",
    "print(f\"Number of dates in MergedDataLoader result for '{first_factor}': {len(merged_dates)}\")\n",
    "print(f\"Number of dates in the normal data: {len(normal_dates)}\")\n",
    "\n",
    "# # After loading your data, inspect the dropped rows but format Timestamps to dates only.\n",
    "# for factor_name, data in merged_data.items():\n",
    "#     print(f\"\\nFactor: {factor_name}\")\n",
    "\n",
    "#     print(\"\\nDropped observations from raw files (per column):\")\n",
    "#     for file_type, col_drops in data.dropped_obs.items():\n",
    "#         print(f\"  File: {file_type}\")\n",
    "#         for col, dates in col_drops.items():\n",
    "#             dates_only = [d.date() for d in dates]  # Convert each Timestamp to date.\n",
    "#             print(f\"    Column '{col}': {dates_only}\")\n",
    "\n",
    "#     print(\"\\nDropped rows during pipeline processing:\")\n",
    "#     for step, dates in data.dropped_pipeline.items():\n",
    "#         dates_only = [d.date() for d in dates]\n",
    "#         print(f\"  Step '{step}': {dates_only}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54abe2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Factor: iwf\n",
      "\n",
      "Dropped observations from raw files (per column):\n",
      "  File: factor_file\n",
      "  File: market_file\n",
      "\n",
      "Dropped rows during pipeline processing:\n",
      "\n",
      "Factor: mtum\n",
      "\n",
      "Dropped observations from raw files (per column):\n",
      "  File: factor_file\n",
      "  File: market_file\n",
      "\n",
      "Dropped rows during pipeline processing:\n",
      "\n",
      "Factor: qual\n",
      "\n",
      "Dropped observations from raw files (per column):\n",
      "  File: factor_file\n",
      "  File: market_file\n",
      "\n",
      "Dropped rows during pipeline processing:\n",
      "\n",
      "Factor: size\n",
      "\n",
      "Dropped observations from raw files (per column):\n",
      "  File: factor_file\n",
      "  File: market_file\n",
      "\n",
      "Dropped rows during pipeline processing:\n",
      "\n",
      "Factor: usmv\n",
      "\n",
      "Dropped observations from raw files (per column):\n",
      "  File: factor_file\n",
      "  File: market_file\n",
      "\n",
      "Dropped rows during pipeline processing:\n",
      "\n",
      "Factor: vlue\n",
      "\n",
      "Dropped observations from raw files (per column):\n",
      "  File: factor_file\n",
      "  File: market_file\n",
      "\n",
      "Dropped rows during pipeline processing:\n"
     ]
    }
   ],
   "source": [
    "# After loading your data, inspect the dropped rows but format Timestamps to dates only.\n",
    "for factor_name, data in merged_data.items():\n",
    "    print(f\"\\nFactor: {factor_name}\")\n",
    "\n",
    "    print(\"\\nDropped observations from raw files (per column):\")\n",
    "    for file_type, col_drops in data.dropped_obs.items():\n",
    "        print(f\"  File: {file_type}\")\n",
    "        for col, dates in col_drops.items():\n",
    "            dates_only = [d.date() for d in dates]  # Convert each Timestamp to date.\n",
    "            print(f\"    Column '{col}': {dates_only}\")\n",
    "\n",
    "    print(\"\\nDropped rows during pipeline processing:\")\n",
    "    for step, dates in data.dropped_pipeline.items():\n",
    "        dates_only = [d.date() for d in dates]\n",
    "        print(f\"  Step '{step}': {dates_only}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of dates in MergedDataLoader result for 'iwf': 4642\n",
    "# Number of dates in the normal data: 5752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of dates in MergedDataLoader result for 'iwf': 5570\n",
    "# Number of dates in the normal data: 5752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70da116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates in MergedDataLoader result for 'iwf': 5590\n",
      "Number of dates in the normal data: 5654\n",
      "\n",
      "Comparison of dropped dates:\n",
      "Dates present in normal data but dropped by MergedDataLoader:\n",
      "[datetime.date(2002, 5, 30), datetime.date(2002, 5, 31), datetime.date(2002, 6, 3), datetime.date(2002, 6, 4), datetime.date(2002, 6, 5), datetime.date(2002, 6, 6), datetime.date(2002, 6, 7), datetime.date(2002, 6, 10), datetime.date(2002, 6, 11), datetime.date(2002, 6, 12), datetime.date(2002, 6, 13), datetime.date(2002, 6, 14), datetime.date(2002, 6, 17), datetime.date(2002, 6, 18), datetime.date(2002, 6, 19), datetime.date(2002, 6, 20), datetime.date(2002, 6, 21), datetime.date(2002, 6, 24), datetime.date(2002, 6, 25), datetime.date(2002, 6, 26), datetime.date(2002, 6, 27), datetime.date(2002, 6, 28), datetime.date(2002, 7, 1), datetime.date(2002, 7, 2), datetime.date(2002, 7, 3), datetime.date(2002, 7, 8), datetime.date(2002, 7, 9), datetime.date(2002, 7, 10), datetime.date(2002, 7, 11), datetime.date(2002, 7, 12), datetime.date(2002, 7, 15), datetime.date(2002, 7, 16), datetime.date(2002, 7, 17), datetime.date(2002, 7, 18), datetime.date(2002, 7, 19), datetime.date(2002, 7, 22), datetime.date(2002, 7, 23), datetime.date(2002, 7, 24), datetime.date(2002, 7, 25), datetime.date(2002, 7, 26), datetime.date(2002, 7, 29), datetime.date(2002, 7, 30), datetime.date(2002, 7, 31), datetime.date(2002, 8, 1), datetime.date(2002, 8, 2), datetime.date(2002, 8, 5), datetime.date(2002, 8, 6), datetime.date(2002, 8, 7), datetime.date(2002, 8, 8), datetime.date(2002, 8, 9), datetime.date(2002, 8, 12), datetime.date(2002, 8, 13), datetime.date(2002, 8, 14), datetime.date(2002, 8, 15), datetime.date(2002, 8, 16), datetime.date(2002, 8, 19), datetime.date(2002, 8, 20), datetime.date(2002, 8, 21), datetime.date(2002, 8, 22), datetime.date(2002, 8, 23), datetime.date(2002, 8, 26), datetime.date(2002, 8, 27), datetime.date(2002, 8, 28), datetime.date(2024, 3, 31)]\n",
      "\n",
      "Dates present in MergedDataLoader but missing in normal data:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Define file locations.\n",
    "directory = r\"C:\\Users\\victo\\git_new\\thesis_vri_vp\\data\"\n",
    "factor_file = os.path.join(directory, \"1estimation_index_returns.csv\")\n",
    "market_file = os.path.join(directory, \"1new_market_data.csv\")\n",
    "\n",
    "# List of factors to process.\n",
    "all_factors = [\"iwf\", \"mtum\", \"qual\", \"size\", \"usmv\", \"vlue\"]\n",
    "\n",
    "# Dictionary to hold data loaded via MergedDataLoader.\n",
    "merged_data = {}\n",
    "\n",
    "for factor_name in all_factors:\n",
    "    # Load data using MergedDataLoader and store in the dictionary.\n",
    "    data = MergedDataLoader(\n",
    "        factor_file=factor_file,\n",
    "        market_file=market_file,\n",
    "        ver=\"v2\",\n",
    "        factor_col=factor_name\n",
    "    ).load(start_date=\"2002-05-31\", end_date=\"2025-04-03\")\n",
    "    merged_data[factor_name] = data\n",
    "\n",
    "# Load the CSV normally.\n",
    "normal_data = pd.read_csv(factor_file, parse_dates=[\"date\"]).dropna()\n",
    "normal_data.set_index(\"date\", inplace=True)\n",
    "\n",
    "# Pick a factor (use the first factor) to compare the date indices.\n",
    "first_factor = all_factors[0]\n",
    "merged_dates = merged_data[first_factor].X.index.unique()\n",
    "normal_dates = normal_data.index.unique()\n",
    "\n",
    "print(f\"Number of dates in MergedDataLoader result for '{first_factor}': {len(merged_dates)}\")\n",
    "print(f\"Number of dates in the normal data: {len(normal_dates)}\")\n",
    "\n",
    "# Convert both merged_dates and normal_dates to sets of date objects.\n",
    "normal_date_set = {d.date() for d in normal_dates}\n",
    "merged_date_set = {d.date() for d in merged_dates}\n",
    "\n",
    "# Compute the non-overlapping dates.\n",
    "# These are dates present in the normal CSV but missing in the merged result.\n",
    "dropped_by_merged = normal_date_set - merged_date_set\n",
    "\n",
    "# These are dates present in the merged result but missing in the normal CSV.\n",
    "dropped_by_normal = merged_date_set - normal_date_set\n",
    "\n",
    "print(\"\\nComparison of dropped dates:\")\n",
    "print(\"Dates present in normal data but dropped by MergedDataLoader:\")\n",
    "print(sorted(dropped_by_merged))\n",
    "print(\"\\nDates present in MergedDataLoader but missing in normal data:\")\n",
    "print(sorted(dropped_by_normal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd731e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DownsideDev_log_21 in April 2024 for factor 'iwf':\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DownsideDev_log_21'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DownsideDev_log_21'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m april_features \u001b[38;5;241m=\u001b[39m df_features\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-04\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownsideDev_log_21 in April 2024 for factor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miwf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(april_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownsideDev_log_21\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DownsideDev_log_21'"
     ]
    }
   ],
   "source": [
    "# Load merged data for your factors.\n",
    "merged_data = {}\n",
    "for factor_name in all_factors:\n",
    "    data = MergedDataLoader(\n",
    "        factor_file=factor_file,\n",
    "        market_file=market_file,\n",
    "        ver=\"v2\",\n",
    "        factor_col=factor_name\n",
    "    ).load(start_date=\"2002-05-31\", end_date=\"2025-04-03\")\n",
    "    merged_data[factor_name] = data\n",
    "\n",
    "# Now, after all data are loaded, inspect the April 2024 features for factor 'iwf'.\n",
    "df_features = merged_data['iwf'].X\n",
    "april_features = df_features.loc[\"2024-04\"]\n",
    "print(\"DownsideDev_log_21 in April 2024 for factor 'iwf':\")\n",
    "print(april_features[\"DownsideDev_log_21\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f07f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
