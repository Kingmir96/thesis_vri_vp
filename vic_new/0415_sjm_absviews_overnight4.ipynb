{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe6cf8-1fa8-4ab3-accf-aff550d3c9eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.12.3)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# For loading data and feature engineering\n",
    "from feature_set_v2 import MergedDataLoader\n",
    "\n",
    "# For data prep and pre-processing\n",
    "from jumpmodels.utils import filter_date_range \n",
    "from jumpmodels.preprocess import StandardScalerPD, DataClipperStd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For model fit and prediction\n",
    "from joblib import Parallel, delayed # allows parallel grid search on all 4 cores\n",
    "from jumpmodels.sparse_jump import SparseJumpModel\n",
    "\n",
    "# For plotting\n",
    "from jumpmodels.plot import plot_regimes_and_cumret, plot_cumret\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Portfolio allocation\n",
    "from pypfopt.black_litterman import BlackLittermanModel, market_implied_prior_returns\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be595c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) HELPER: EWMA Covariance\n",
    "###############################################################################\n",
    "def ewm_covariance(returns: pd.DataFrame, halflife=126, min_periods=1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute an exponentially weighted covariance matrix over all data in 'returns'.\n",
    "    The half-life is 126 days, meaning weights decay by 50% every 126 days.\n",
    "    We then select the final NxN slice from the multi-index result.\n",
    "    \"\"\"\n",
    "    ewm_cov = returns.ewm(halflife=halflife, adjust=False, min_periods=min_periods).cov()\n",
    "    if len(returns) == 0:\n",
    "        # Return an empty DataFrame if no data\n",
    "        return pd.DataFrame()\n",
    "    last_date = returns.index[-1]\n",
    "    df_slice = ewm_cov.loc[last_date]  # shape (N, N)\n",
    "    df_slice.index.name = None\n",
    "    return df_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18c991",
   "metadata": {},
   "source": [
    "Weights: \n",
    "    Factors L\n",
    "    Market L\n",
    "\n",
    "cov_matrix: cov_bl\n",
    "\n",
    "pi: \"equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b804ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2) HELPER: Rolling + Expanding Black–Litterman\n",
    "###############################################################################\n",
    "def run_bl_portfolio_pyopt_expanding(\n",
    "    factor_dict,\n",
    "    returns_df,\n",
    "    test_index,\n",
    "    tau=0.05,\n",
    "    delta=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    An expanding-window Black–Litterman procedure:\n",
    "    - For each date t in test_index, we take all historical data up to t-1 to compute:\n",
    "       (a) an EWMA covariance (with half-life=126)\n",
    "       (b) a BL prior using pi=\"equal\"  (i.e. all assets get equal baseline expected return)\n",
    "       (c) factor-based relative views: factor vs Market => P, Q\n",
    "    - Then we get posterior (bl_returns, bl_cov).\n",
    "    - Then run a standard Efficient Frontier (max_sharpe).\n",
    "    - We store the resulting weights for day t+1 (one-day lag).\n",
    "    \"\"\"\n",
    "    assets = list(returns_df.columns)   # e.g. [factor1, factor2, ..., Market]\n",
    "    factors = list(factor_dict.keys())  # e.g. [factor1, factor2, ...]\n",
    "    weights_df = pd.DataFrame(index=test_index, columns=assets, dtype=float)\n",
    "\n",
    "    bounds = []\n",
    "    for asset in assets:\n",
    "        if asset == \"Market\":\n",
    "            bounds.append((0, 1))  # allow short up to -100%\n",
    "        else:\n",
    "            bounds.append((0, 1))   # no shorting these factors\n",
    "\n",
    "    test_dates = list(test_index)\n",
    "\n",
    "    \n",
    "    # Assign \"market caps\" equally to each ETF => that implies an equal-weight reference portfolio. To be used to calculate the prior\n",
    "    market_caps = {etf: 1.0 for etf in assets} # The actual numeric value doesn’t matter as much as the ratio 1:1:1:... across all ETFs.\n",
    "    \n",
    "\n",
    "    for i, t in enumerate(test_dates):\n",
    "        # 1) Expand up to day t-1\n",
    "        window_end_idx = returns_df.index.get_loc(t)\n",
    "        if window_end_idx == 0:\n",
    "            # No prior data => skip\n",
    "            continue\n",
    "        expanding_slice = returns_df.iloc[:window_end_idx]\n",
    "\n",
    "        # 2) Compute the EWMA covariance\n",
    "        cov_matrix = ewm_covariance(expanding_slice, halflife=126, min_periods=60) * 252\n",
    "        if cov_matrix.empty or cov_matrix.isna().any().any():\n",
    "            # If invalid, skip\n",
    "            continue\n",
    "\n",
    "        # 3) Construct abs views - P, Q not needed now\n",
    "        viewdict = {}\n",
    "        for fac in factors:\n",
    "            # figure out which regime fac is in\n",
    "            st = factor_dict[fac]['states'].loc[t]\n",
    "            daily_abs_ret = factor_dict[fac]['abs_regime_returns'][st]\n",
    "            annual_abs_ret = daily_abs_ret * 252\n",
    "            \n",
    "            # put that in the dictionary\n",
    "            viewdict[fac] = annual_abs_ret\n",
    "\n",
    "        # 4) Calculate the prior\n",
    "        prior = market_implied_prior_returns(market_caps, delta, cov_matrix) \n",
    "\n",
    "        # 5) Build the Black–Litterman model\n",
    "        #    We'll say pi=\"equal\" so that each asset has the same baseline prior\n",
    "        #    Then we incorporate the relative views in (P, Q).\n",
    "        #    tau=0.05 is a typical default.\n",
    "        try:\n",
    "            bl = BlackLittermanModel(\n",
    "                cov_matrix=cov_matrix,\n",
    "                pi=\"equal\",\n",
    "                tau=tau,\n",
    "                delta=delta,\n",
    "                absolute_views=viewdict,  \n",
    "                # no \"omega\" => default is Proportional to var of view portfolio\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"BL Model error on day {t}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 5) Retrieve posterior returns & covariance\n",
    "        mu_bl = bl.bl_returns()\n",
    "        cov_bl = bl.bl_cov()  # posterior covariance\n",
    "\n",
    "        # 6) Mean-variance optimization\n",
    "        ef = EfficientFrontier(mu_bl, cov_bl, weight_bounds=bounds)\n",
    "        try:\n",
    "            ef.max_sharpe()  # or ef.min_volatility() etc.\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization error on day {t}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 7) Store weights for day t+1 (one-day lag)\n",
    "        w_series = pd.Series(ef.clean_weights(), name=t)\n",
    "        if i < len(test_dates) - 1:\n",
    "            rebalance_day = test_dates[i+1]\n",
    "            weights_df.loc[rebalance_day] = w_series\n",
    "        else:\n",
    "            weights_df.loc[t] = w_series\n",
    "\n",
    "    return weights_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8aec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Sharpe Ratio\n",
    "# ---------------------------------------------------------------------\n",
    "def annualized_sharpe(r):\n",
    "    return (r.mean() / r.std()) * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676976cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trading_with_saved_hyperparams(\n",
    "    factor_data_dict,\n",
    "    saved_hyperparams,\n",
    "    factors,\n",
    "    test_index,\n",
    "    full_df,\n",
    "    REFIT_FREQ=\"ME\",\n",
    "    MIN_TRAINING_YEARS=8,\n",
    "    MAX_TRAINING_YEARS=12,\n",
    "    INITIAL_TRAIN_START=\"2002-05-31\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run trading strategy using pre-computed hyperparameters without re-doing cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "        factor_data_dict: Dictionary containing factor data\n",
    "        saved_hyperparams: Dictionary with saved hyperparameters from previous run\n",
    "        factors: List of factor names\n",
    "        test_index: DatetimeIndex for the test period\n",
    "        full_df: DataFrame with all factor returns and market returns\n",
    "        REFIT_FREQ: Frequency of model refitting\n",
    "        MIN_TRAINING_YEARS: Minimum years for training window\n",
    "        MAX_TRAINING_YEARS: Maximum years for training window\n",
    "        INITIAL_TRAIN_START: Start date of initial training window\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing performance results\n",
    "    \"\"\"\n",
    "    # Define refit boundaries\n",
    "    refit_boundaries = (\n",
    "        test_index.to_series()\n",
    "        .resample(REFIT_FREQ)\n",
    "        .last()\n",
    "        .dropna()\n",
    "    )\n",
    "    \n",
    "    # Tracking variables\n",
    "    all_daily_states = {fac: pd.Series(dtype=float) for fac in factors}\n",
    "    all_daily_weights = []\n",
    "    all_daily_rets = []\n",
    "    \n",
    "    # Helper function for get_train_val_windows (same as in original code)\n",
    "    def get_train_window(\n",
    "        current_date, \n",
    "        full_data, \n",
    "        min_train_years=MIN_TRAINING_YEARS,\n",
    "        max_train_years=MAX_TRAINING_YEARS,\n",
    "        initial_train_start=INITIAL_TRAIN_START\n",
    "    ):\n",
    "        # Training window ends at current date\n",
    "        train_end = current_date\n",
    "        \n",
    "        # Try the largest training window (max_train_years)\n",
    "        train_start_candidate = train_end - pd.DateOffset(years=max_train_years)\n",
    "        \n",
    "        # Don't go before global earliest start\n",
    "        if initial_train_start is not None:\n",
    "            earliest_start_dt = pd.to_datetime(initial_train_start)\n",
    "            train_start_candidate = max(train_start_candidate, earliest_start_dt)\n",
    "        \n",
    "        # Ensure we have at least min_train_years\n",
    "        if (train_end - train_start_candidate) < pd.Timedelta(days=365.25 * min_train_years):\n",
    "            train_start_candidate = train_end - pd.DateOffset(years=min_train_years)\n",
    "            if initial_train_start is not None:\n",
    "                train_start_candidate = max(train_start_candidate, earliest_start_dt)\n",
    "        \n",
    "        # Filter actual data\n",
    "        idx = full_data.index\n",
    "        train_dates = idx[(idx >= train_start_candidate) & (idx <= train_end)]\n",
    "        \n",
    "        if len(train_dates) == 0:\n",
    "            raise ValueError(\n",
    "                f\"No data found for train [{train_start_candidate} to {train_end}]\"\n",
    "            )\n",
    "        \n",
    "        return train_dates[0], train_dates[-1]\n",
    "    \n",
    "    # Process each refit period\n",
    "    for j in range(len(refit_boundaries)):\n",
    "        refit_date = refit_boundaries.iloc[j]\n",
    "        \n",
    "        # Define the trading period\n",
    "        if j < len(refit_boundaries) - 1:\n",
    "            next_refit_date = refit_boundaries.iloc[j + 1]\n",
    "        else:\n",
    "            # For the last refit period, use end of data\n",
    "            next_refit_date = test_index[-1]\n",
    "        \n",
    "        test_mask = (test_index > refit_date) & (test_index <= next_refit_date)\n",
    "        test_dates_chunk = test_index[test_mask]\n",
    "        \n",
    "        if len(test_dates_chunk) == 0:\n",
    "            continue\n",
    "                \n",
    "        print(f\"\\nTrading period: {refit_date} to {next_refit_date} ({len(test_dates_chunk)} days)\")\n",
    "        \n",
    "        # Build factor dict for BL portfolio construction\n",
    "        factor_dict_chunk = {}\n",
    "        \n",
    "        # Process each factor for this refit period\n",
    "        for fac in factors:\n",
    "            # Get the appropriate hyperparameters for this factor\n",
    "            # Find the most recent hyperparameter setting prior to or at the refit date\n",
    "            fac_params = saved_hyperparams[fac]\n",
    "            param_dates = [pd.to_datetime(entry['date']) for entry in fac_params]\n",
    "            param_dates = [d for d in param_dates if d <= refit_date]\n",
    "            \n",
    "            if not param_dates:\n",
    "                print(f\"No saved hyperparameters found for {fac} before {refit_date}\")\n",
    "                continue\n",
    "                \n",
    "            most_recent_param_date = max(param_dates)\n",
    "            hyperparams = next(entry for entry in fac_params \n",
    "                              if pd.to_datetime(entry['date']) == most_recent_param_date)\n",
    "            \n",
    "            lam = hyperparams['new_lambda']\n",
    "            kp = hyperparams['new_kappa']\n",
    "            \n",
    "            print(f\"Using hyperparameters for {fac}: lambda={lam:.2f}, kappa={kp:.2f} (from {most_recent_param_date})\")\n",
    "            \n",
    "            # Get data for this factor\n",
    "            fac_data = factor_data_dict[fac]\n",
    "            X = fac_data[\"X\"]\n",
    "            ret_full = fac_data[\"fac_ret\"]\n",
    "            mkt_full = fac_data[\"mkt_ret\"]\n",
    "            active = fac_data[\"active_ret\"]\n",
    "            \n",
    "            # Get appropriate training window for this refit date\n",
    "            train_start, train_end = get_train_window(\n",
    "                refit_date, X,\n",
    "                min_train_years=MIN_TRAINING_YEARS,\n",
    "                max_train_years=MAX_TRAINING_YEARS,\n",
    "                initial_train_start=INITIAL_TRAIN_START\n",
    "            )\n",
    "            \n",
    "            # Get training data\n",
    "            X_train = filter_date_range(X, start_date=train_start, end_date=train_end)\n",
    "            active_train = filter_date_range(active, start_date=train_start, end_date=train_end)\n",
    "            factor_returns_train = filter_date_range(ret_full, start_date=train_start, end_date=train_end)\n",
    "\n",
    "            \n",
    "            # Process training data\n",
    "            clipper = DataClipperStd(mul=3.0)\n",
    "            X_train_clip = clipper.fit_transform(X_train)\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = pd.DataFrame(\n",
    "                scaler.fit_transform(X_train_clip),\n",
    "                index=X_train_clip.index,\n",
    "                columns=X_train_clip.columns\n",
    "            )\n",
    "            \n",
    "            # Fit SJM with saved hyperparameters\n",
    "            max_feats = int(kp**2)\n",
    "            sjm = SparseJumpModel(\n",
    "                n_components=2, \n",
    "                max_feats=max_feats, \n",
    "                jump_penalty=lam\n",
    "            )\n",
    "            sjm.fit(X_train_scaled, ret_ser=active_train, sort_by=\"cumret\")\n",
    "            \n",
    "            # Daily online inference for trading period\n",
    "            X_test_proc_list = []\n",
    "            for day in test_dates_chunk:\n",
    "                # For online scaling, we use all available history up to the current day\n",
    "                X_hist = X.loc[:day]\n",
    "                \n",
    "                # Clip & scale using all history\n",
    "                temp_clipper = DataClipperStd(mul=3.0)\n",
    "                X_hist_clip = temp_clipper.fit_transform(X_hist)\n",
    "                \n",
    "                temp_scaler = StandardScaler()\n",
    "                _ = temp_scaler.fit_transform(X_hist_clip)  # Fit on all history\n",
    "                \n",
    "                # Transform just the current day\n",
    "                if day in X.index:\n",
    "                    X_day = X.loc[[day]]\n",
    "                    X_day_clip = temp_clipper.transform(X_day)\n",
    "                    X_day_scaled = temp_scaler.transform(X_day_clip)\n",
    "                    X_test_proc_list.append(\n",
    "                        pd.Series(X_day_scaled.flatten(), index=X_day.columns, name=day)\n",
    "                    )\n",
    "            \n",
    "            X_test_scaled = pd.DataFrame(X_test_proc_list).sort_index()\n",
    "            \n",
    "            # Predict states\n",
    "            # Old:\n",
    "            # if not X_test_scaled.empty:\n",
    "            #     states_chunk = sjm.predict_online(X_test_scaled)\n",
    "            #     states_series = pd.Series(states_chunk, index=X_test_scaled.index)\n",
    "            # else:\n",
    "            #     states_series = pd.Series(dtype=float)\n",
    "            \n",
    "            # New:\n",
    "            if X_test_scaled.empty:\n",
    "                raise ValueError(f\"No scaled data available for prediction on trading dates: {test_dates_chunk[0]} to {test_dates_chunk[-1]}\")\n",
    "                \n",
    "            states_chunk = sjm.predict_online(X_test_scaled)\n",
    "            states_series = pd.Series(states_chunk, index=X_test_scaled.index)\n",
    "            \n",
    "            # Update global state tracking\n",
    "            all_daily_states[fac] = pd.concat(\n",
    "                [s for s in [all_daily_states[fac], states_series] if not s.empty]\n",
    "            ).sort_index()\n",
    "            \n",
    "            train_states = sjm.predict(X_train_scaled)\n",
    "            train_state_series = pd.Series(train_states, index=X_train_scaled.index)\n",
    "\n",
    "            abs_regime_returns = {}\n",
    "            for st in range(sjm.n_components):\n",
    "                st_dates = train_state_series[train_state_series == st].index\n",
    "                abs_regime_returns[st] = factor_returns_train.loc[st_dates].mean()\n",
    "\n",
    "\n",
    "            # Store factor info for BL\n",
    "            factor_dict_chunk[fac] = {\n",
    "                \"ret\": ret_full,\n",
    "                \"states\": states_series,\n",
    "                \"regime_returns\": {\n",
    "                    0: sjm.ret_[0],\n",
    "                    1: sjm.ret_[1],\n",
    "                },\n",
    "                \"abs_regime_returns\" : {\n",
    "                    0: abs_regime_returns[0],\n",
    "                    1: abs_regime_returns[1],\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        \n",
    "        # Run Black-Litterman portfolio construction with your modified version\n",
    "        weights_chunk = run_bl_portfolio_pyopt_expanding(\n",
    "            factor_dict=factor_dict_chunk,\n",
    "            returns_df=full_df,\n",
    "            test_index=test_dates_chunk,\n",
    "            tau=0.05,  # You can modify these parameters as needed\n",
    "            delta=2.5   # for your BL changes\n",
    "        )\n",
    "        \n",
    "        # Compute portfolio returns\n",
    "        daily_factor_rets = full_df.loc[test_dates_chunk, factors]\n",
    "        portfolio_rets = (weights_chunk * daily_factor_rets).sum(axis=1)\n",
    "        \n",
    "        # Store results\n",
    "        all_daily_weights.append(weights_chunk)\n",
    "        all_daily_rets.append(portfolio_rets)\n",
    "    \n",
    "    # Combine all results\n",
    "    if len(all_daily_rets) == 0:\n",
    "        raise ValueError(\"No daily returns computed. Check your date ranges or data.\")\n",
    "\n",
    "    all_portfolio_rets = pd.concat(all_daily_rets).sort_index()\n",
    "    all_weights_df = pd.concat(all_daily_weights).sort_index()\n",
    "    \n",
    "    return {\n",
    "        \"returns\": all_portfolio_rets,\n",
    "        \"weights\": all_weights_df,\n",
    "        \"states\": all_daily_states\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_saved_hyperparameters(hyperparameter_history):\n",
    "    \"\"\"\n",
    "    Format the hyperparameter history into a structured format\n",
    "    for easier lookup during trading.\n",
    "    \"\"\"\n",
    "    # Convert the format to be more suitable for lookup\n",
    "    formatted_params = {}\n",
    "    \n",
    "    for factor, history in hyperparameter_history.items():\n",
    "        formatted_params[factor] = history\n",
    "        \n",
    "    return formatted_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84380ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_history = {\n",
    "    'iwf': [\n",
    "        {'date': pd.Timestamp('2017-01-31 00:00:00'), 'old_lambda': None, 'new_lambda': 194.44444444444446, 'old_kappa': None, 'new_kappa': 2.802775637731995, 'sharpe': -0.00040810662925682057},\n",
    "        {'date': pd.Timestamp('2017-07-31 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 194.44444444444446, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': 0.17071973179690386},\n",
    "        {'date': pd.Timestamp('2018-01-31 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 358.8888888888889, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.3578102263098243},\n",
    "        {'date': pd.Timestamp('2018-07-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 358.8888888888889, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': 0.4841917963037153},\n",
    "        {'date': pd.Timestamp('2019-01-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 358.8888888888889, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': 0.0021833092539811937},\n",
    "        {'date': pd.Timestamp('2019-07-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 400.0, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': -0.08661718291998324},\n",
    "        {'date': pd.Timestamp('2020-01-31 00:00:00'), 'old_lambda': 400.0, 'new_lambda': 400.0, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.04537836286365537},\n",
    "        {'date': pd.Timestamp('2020-07-31 00:00:00'), 'old_lambda': 400.0, 'new_lambda': 400.0, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.3796416073101208},\n",
    "        {'date': pd.Timestamp('2021-01-29 00:00:00'), 'old_lambda': 400.0, 'new_lambda': 400.0, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': 0.3143905966885326},\n",
    "        {'date': pd.Timestamp('2021-07-30 00:00:00'), 'old_lambda': 400.0, 'new_lambda': 71.11111111111111, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.6565757223197191},\n",
    "        {'date': pd.Timestamp('2022-01-31 00:00:00'), 'old_lambda': 71.11111111111111, 'new_lambda': 30.0, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.5661972679926426},\n",
    "        {'date': pd.Timestamp('2022-07-29 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 194.44444444444446, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.5025574640511644},\n",
    "        {'date': pd.Timestamp('2023-01-31 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 194.44444444444446, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.4412874588946835},\n",
    "        {'date': pd.Timestamp('2023-07-31 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.6244571972898924},\n",
    "        {'date': pd.Timestamp('2024-01-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.5496996539335203},\n",
    "        {'date': pd.Timestamp('2024-07-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.5045344785461053},\n",
    "        {'date': pd.Timestamp('2025-01-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 153.33333333333334, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.6748125011222248},\n",
    "        {'date': pd.Timestamp('2025-04-03 00:00:00'), 'old_lambda': 153.33333333333334, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.5220707952568912}\n",
    "    ],\n",
    "    'mtum': [\n",
    "        {'date': pd.Timestamp('2017-01-31 00:00:00'), 'old_lambda': None, 'new_lambda': 153.33333333333334, 'old_kappa': None, 'new_kappa': 3.605551275463989, 'sharpe': 0.9737531059727994},\n",
    "        {'date': pd.Timestamp('2017-07-31 00:00:00'), 'old_lambda': 153.33333333333334, 'new_lambda': 71.11111111111111, 'old_kappa': 3.605551275463989, 'new_kappa': 2.0, 'sharpe': 1.2385288514186117},\n",
    "        {'date': pd.Timestamp('2018-01-31 00:00:00'), 'old_lambda': 71.11111111111111, 'new_lambda': 71.11111111111111, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 1.5384280936967225},\n",
    "        {'date': pd.Timestamp('2018-07-31 00:00:00'), 'old_lambda': 71.11111111111111, 'new_lambda': 71.11111111111111, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 1.2449318483644662},\n",
    "        {'date': pd.Timestamp('2019-01-31 00:00:00'), 'old_lambda': 71.11111111111111, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 1.2849216385777595},\n",
    "        {'date': pd.Timestamp('2019-07-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 1.3550258164181153},\n",
    "        {'date': pd.Timestamp('2020-01-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 1.2946985716778718},\n",
    "        {'date': pd.Timestamp('2020-07-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 1.2443701401904304},\n",
    "        {'date': pd.Timestamp('2021-01-29 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 30.0, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 1.0609358262039597},\n",
    "        {'date': pd.Timestamp('2021-07-30 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 317.7777777777778, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.5336681869528377},\n",
    "        {'date': pd.Timestamp('2022-01-31 00:00:00'), 'old_lambda': 317.7777777777778, 'new_lambda': 71.11111111111111, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': 0.42085014436258855},\n",
    "        {'date': pd.Timestamp('2022-07-29 00:00:00'), 'old_lambda': 71.11111111111111, 'new_lambda': 71.11111111111111, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.4776764452885165},\n",
    "        {'date': pd.Timestamp('2023-01-31 00:00:00'), 'old_lambda': 71.11111111111111, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 3.605551275463989, 'sharpe': 0.29425697780975163},\n",
    "        {'date': pd.Timestamp('2023-07-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 3.605551275463989, 'new_kappa': 3.605551275463989, 'sharpe': 0.12389367197312393},\n",
    "        {'date': pd.Timestamp('2024-01-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 235.55555555555557, 'old_kappa': 3.605551275463989, 'new_kappa': 2.802775637731995, 'sharpe': 0.30857404399436067},\n",
    "        {'date': pd.Timestamp('2024-07-31 00:00:00'), 'old_lambda': 235.55555555555557, 'new_lambda': 358.8888888888889, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': 0.39916193058999083},\n",
    "        {'date': pd.Timestamp('2025-01-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 276.6666666666667, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.29269886811293266},\n",
    "        {'date': pd.Timestamp('2025-04-03 00:00:00'), 'old_lambda': 276.6666666666667, 'new_lambda': 358.8888888888889, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.48260717116695645}\n",
    "    ],\n",
    "    'qual': [\n",
    "        {'date': pd.Timestamp('2017-01-31 00:00:00'), 'old_lambda': None, 'new_lambda': 112.22222222222223, 'old_kappa': None, 'new_kappa': 2.0, 'sharpe': -0.008942944608080107},\n",
    "        {'date': pd.Timestamp('2017-07-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 194.44444444444446, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': 0.3729840213884896},\n",
    "        {'date': pd.Timestamp('2018-01-31 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 276.6666666666667, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': 0.4569557693250369},\n",
    "        {'date': pd.Timestamp('2018-07-31 00:00:00'), 'old_lambda': 276.6666666666667, 'new_lambda': 276.6666666666667, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.46645808224234},\n",
    "        {'date': pd.Timestamp('2019-01-31 00:00:00'), 'old_lambda': 276.6666666666667, 'new_lambda': 317.7777777777778, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.44970951739981563},\n",
    "        {'date': pd.Timestamp('2019-07-31 00:00:00'), 'old_lambda': 317.7777777777778, 'new_lambda': 358.8888888888889, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.4565696219766751},\n",
    "        {'date': pd.Timestamp('2020-01-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 276.6666666666667, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.515467869569287},\n",
    "        {'date': pd.Timestamp('2020-07-31 00:00:00'), 'old_lambda': 276.6666666666667, 'new_lambda': 30.0, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': 0.34082022634710635},\n",
    "        {'date': pd.Timestamp('2021-01-29 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 3.605551275463989, 'sharpe': 0.43043837227656606},\n",
    "        {'date': pd.Timestamp('2021-07-30 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 235.55555555555557, 'old_kappa': 3.605551275463989, 'new_kappa': 2.0, 'sharpe': 0.48545612296409024},\n",
    "        {'date': pd.Timestamp('2022-01-31 00:00:00'), 'old_lambda': 235.55555555555557, 'new_lambda': 235.55555555555557, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': 0.24680166988096894},\n",
    "        {'date': pd.Timestamp('2022-07-29 00:00:00'), 'old_lambda': 235.55555555555557, 'new_lambda': 194.44444444444446, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': 0.3768471604383053},\n",
    "        {'date': pd.Timestamp('2023-01-31 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 235.55555555555557, 'old_kappa': 2.0, 'new_kappa': 3.605551275463989, 'sharpe': 0.52515080472428},\n",
    "        {'date': pd.Timestamp('2023-07-31 00:00:00'), 'old_lambda': 235.55555555555557, 'new_lambda': 276.6666666666667, 'old_kappa': 3.605551275463989, 'new_kappa': 3.605551275463989, 'sharpe': 0.5032881257329227},\n",
    "        {'date': pd.Timestamp('2024-01-31 00:00:00'), 'old_lambda': 276.6666666666667, 'new_lambda': 276.6666666666667, 'old_kappa': 3.605551275463989, 'new_kappa': 2.0, 'sharpe': 0.5601170142538924},\n",
    "        {'date': pd.Timestamp('2024-07-31 00:00:00'), 'old_lambda': 276.6666666666667, 'new_lambda': 30.0, 'old_kappa': 2.0, 'new_kappa': 3.605551275463989, 'sharpe': 0.6556926212774876},\n",
    "        {'date': pd.Timestamp('2025-01-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 3.605551275463989, 'new_kappa': 2.802775637731995, 'sharpe': 0.6397538574423197},\n",
    "        {'date': pd.Timestamp('2025-04-03 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 3.605551275463989, 'sharpe': 0.595839463815995}\n",
    "    ],\n",
    "    'size': [\n",
    "        {'date': pd.Timestamp('2017-01-31 00:00:00'), 'old_lambda': None, 'new_lambda': 30.0, 'old_kappa': None, 'new_kappa': 3.605551275463989, 'sharpe': 0.18325758924372446},\n",
    "        {'date': pd.Timestamp('2017-07-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 3.605551275463989, 'new_kappa': 3.605551275463989, 'sharpe': 0.35998093208599363},\n",
    "        {'date': pd.Timestamp('2018-01-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 3.605551275463989, 'new_kappa': 2.802775637731995, 'sharpe': 0.20754182203863084},\n",
    "        {'date': pd.Timestamp('2018-07-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 3.605551275463989, 'sharpe': 0.169026897105994},\n",
    "        {'date': pd.Timestamp('2019-01-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 3.605551275463989, 'new_kappa': 3.605551275463989, 'sharpe': 0.170613601033382},\n",
    "        {'date': pd.Timestamp('2019-07-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 3.605551275463989, 'new_kappa': 3.605551275463989, 'sharpe': 0.08232751636587375},\n",
    "        {'date': pd.Timestamp('2020-01-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 194.44444444444446, 'old_kappa': 3.605551275463989, 'new_kappa': 3.605551275463989, 'sharpe': 0.07975853398451298},\n",
    "        {'date': pd.Timestamp('2020-07-31 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 194.44444444444446, 'old_kappa': 3.605551275463989, 'new_kappa': 3.605551275463989, 'sharpe': 0.20993440803949895},\n",
    "        {'date': pd.Timestamp('2021-01-29 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 235.55555555555557, 'old_kappa': 3.605551275463989, 'new_kappa': 3.605551275463989, 'sharpe': 0.16038247250831156},\n",
    "        {'date': pd.Timestamp('2021-07-30 00:00:00'), 'old_lambda': 235.55555555555557, 'new_lambda': 317.7777777777778, 'old_kappa': 3.605551275463989, 'new_kappa': 2.0, 'sharpe': -0.018202918861167948},\n",
    "        {'date': pd.Timestamp('2022-01-31 00:00:00'), 'old_lambda': 317.7777777777778, 'new_lambda': 400.0, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.036480646039279585},\n",
    "        {'date': pd.Timestamp('2022-07-29 00:00:00'), 'old_lambda': 400.0, 'new_lambda': 358.8888888888889, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': 0.36563761781827314},\n",
    "        {'date': pd.Timestamp('2023-01-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 317.7777777777778, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': 0.32185546293215544},\n",
    "        {'date': pd.Timestamp('2023-07-31 00:00:00'), 'old_lambda': 317.7777777777778, 'new_lambda': 317.7777777777778, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.2798812792489098},\n",
    "        {'date': pd.Timestamp('2024-01-31 00:00:00'), 'old_lambda': 317.7777777777778, 'new_lambda': 358.8888888888889, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': 0.452087066336667},\n",
    "        {'date': pd.Timestamp('2024-07-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 358.8888888888889, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.5006537077202152},\n",
    "        {'date': pd.Timestamp('2025-01-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 317.7777777777778, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.5716957249478238},\n",
    "        {'date': pd.Timestamp('2025-04-03 00:00:00'), 'old_lambda': 317.7777777777778, 'new_lambda': 317.7777777777778, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.4431237089045836}\n",
    "    ],\n",
    "    'usmv': [\n",
    "        {'date': pd.Timestamp('2017-01-31 00:00:00'), 'old_lambda': None, 'new_lambda': 30.0, 'old_kappa': None, 'new_kappa': 2.0, 'sharpe': -0.018377230113554608},\n",
    "        {'date': pd.Timestamp('2017-07-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 400.0, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.22709101622189878},\n",
    "        {'date': pd.Timestamp('2018-01-31 00:00:00'), 'old_lambda': 400.0, 'new_lambda': 194.44444444444446, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': -0.11574479947038052},\n",
    "        {'date': pd.Timestamp('2018-07-31 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 194.44444444444446, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': -0.13169461167089427},\n",
    "        {'date': pd.Timestamp('2019-01-31 00:00:00'), 'old_lambda': 194.44444444444446, 'new_lambda': 112.22222222222223, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': -0.04023257907985476},\n",
    "        {'date': pd.Timestamp('2019-07-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 71.11111111111111, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': -0.09485108586318038},\n",
    "        {'date': pd.Timestamp('2020-01-31 00:00:00'), 'old_lambda': 71.11111111111111, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': -0.20505014291457693},\n",
    "        {'date': pd.Timestamp('2020-07-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 2.0, 'new_kappa': 2.802775637731995, 'sharpe': -0.21124341082069295},\n",
    "        {'date': pd.Timestamp('2021-01-29 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.27357211138032833},\n",
    "        {'date': pd.Timestamp('2021-07-30 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.12318043827830945},\n",
    "        {'date': pd.Timestamp('2022-01-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 3.605551275463989, 'sharpe': -0.027488565563862734},\n",
    "        {'date': pd.Timestamp('2022-07-29 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 3.605551275463989, 'new_kappa': 2.802775637731995, 'sharpe': 0.00615644148977025},\n",
    "        {'date': pd.Timestamp('2023-01-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 276.6666666666667, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.1102054318041255},\n",
    "        {'date': pd.Timestamp('2023-07-31 00:00:00'), 'old_lambda': 276.6666666666667, 'new_lambda': 153.33333333333334, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.18972149133249572},\n",
    "        {'date': pd.Timestamp('2024-01-31 00:00:00'), 'old_lambda': 153.33333333333334, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.09270063511915752},\n",
    "        {'date': pd.Timestamp('2024-07-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 30.0, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.23279385139850495},\n",
    "        {'date': pd.Timestamp('2025-01-31 00:00:00'), 'old_lambda': 30.0, 'new_lambda': 153.33333333333334, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.30721578219158546},\n",
    "        {'date': pd.Timestamp('2025-04-03 00:00:00'), 'old_lambda': 153.33333333333334, 'new_lambda': 153.33333333333334, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': 0.25005959115900506}\n",
    "    ],\n",
    "    'vlue': [\n",
    "        {'date': pd.Timestamp('2017-01-31 00:00:00'), 'old_lambda': None, 'new_lambda': 112.22222222222223, 'old_kappa': None, 'new_kappa': 2.0, 'sharpe': 0.9808698623368385},\n",
    "        {'date': pd.Timestamp('2017-07-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 317.7777777777778, 'old_kappa': 2.0, 'new_kappa': 3.605551275463989, 'sharpe': 0.8394277858533714},\n",
    "        {'date': pd.Timestamp('2018-01-31 00:00:00'), 'old_lambda': 317.7777777777778, 'new_lambda': 276.6666666666667, 'old_kappa': 3.605551275463989, 'new_kappa': 2.0, 'sharpe': 1.006140466878844},\n",
    "        {'date': pd.Timestamp('2018-07-31 00:00:00'), 'old_lambda': 276.6666666666667, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 1.0366275635711821},\n",
    "        {'date': pd.Timestamp('2019-01-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 112.22222222222223, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 1.0882139211247348},\n",
    "        {'date': pd.Timestamp('2019-07-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 358.8888888888889, 'old_kappa': 2.0, 'new_kappa': 3.605551275463989, 'sharpe': 0.6059643491080812},\n",
    "        {'date': pd.Timestamp('2020-01-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 358.8888888888889, 'old_kappa': 3.605551275463989, 'new_kappa': 2.802775637731995, 'sharpe': 0.39346946308603115},\n",
    "        {'date': pd.Timestamp('2020-07-31 00:00:00'), 'old_lambda': 358.8888888888889, 'new_lambda': 153.33333333333334, 'old_kappa': 2.802775637731995, 'new_kappa': 3.605551275463989, 'sharpe': 0.350841470339303},\n",
    "        {'date': pd.Timestamp('2021-01-29 00:00:00'), 'old_lambda': 153.33333333333334, 'new_lambda': 153.33333333333334, 'old_kappa': 3.605551275463989, 'new_kappa': 3.605551275463989, 'sharpe': 0.4540450646485634},\n",
    "        {'date': pd.Timestamp('2021-07-30 00:00:00'), 'old_lambda': 153.33333333333334, 'new_lambda': 235.55555555555557, 'old_kappa': 3.605551275463989, 'new_kappa': 2.802775637731995, 'sharpe': 0.45721111621311916},\n",
    "        {'date': pd.Timestamp('2022-01-31 00:00:00'), 'old_lambda': 235.55555555555557, 'new_lambda': 112.22222222222223, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.4393294952136704},\n",
    "        {'date': pd.Timestamp('2022-07-29 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 153.33333333333334, 'old_kappa': 2.802775637731995, 'new_kappa': 2.802775637731995, 'sharpe': 0.538921576147189},\n",
    "        {'date': pd.Timestamp('2023-01-31 00:00:00'), 'old_lambda': 153.33333333333334, 'new_lambda': 112.22222222222223, 'old_kappa': 2.802775637731995, 'new_kappa': 3.605551275463989, 'sharpe': 0.3856975910925221},\n",
    "        {'date': pd.Timestamp('2023-07-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 153.33333333333334, 'old_kappa': 3.605551275463989, 'new_kappa': 2.802775637731995, 'sharpe': 0.3300623151991221},\n",
    "        {'date': pd.Timestamp('2024-01-31 00:00:00'), 'old_lambda': 153.33333333333334, 'new_lambda': 153.33333333333334, 'old_kappa': 2.802775637731995, 'new_kappa': 3.605551275463989, 'sharpe': 0.31531828237806936},\n",
    "        {'date': pd.Timestamp('2024-07-31 00:00:00'), 'old_lambda': 153.33333333333334, 'new_lambda': 112.22222222222223, 'old_kappa': 3.605551275463989, 'new_kappa': 2.802775637731995, 'sharpe': 0.22277643742023429},\n",
    "        {'date': pd.Timestamp('2025-01-31 00:00:00'), 'old_lambda': 112.22222222222223, 'new_lambda': 400.0, 'old_kappa': 2.802775637731995, 'new_kappa': 2.0, 'sharpe': -0.059024223017440335},\n",
    "        {'date': pd.Timestamp('2025-04-03 00:00:00'), 'old_lambda': 400.0, 'new_lambda': 400.0, 'old_kappa': 2.0, 'new_kappa': 2.0, 'sharpe': 0.05808658892901897}\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Modified main function for more efficient processing\n",
    "if __name__ == \"__main__\":\n",
    "    ############################################################################\n",
    "    # 0) Set parameters\n",
    "    ############################################################################\n",
    "    # We define how often to re-fit and re-cross-validate:\n",
    "    REFIT_FREQ = \"ME\"  # Monthly refit\n",
    "    RECROSS_VAL_FREQ = \"6ME\"  # Every 6 months recross-validation\n",
    "    \n",
    "    # Training window configuration\n",
    "    MIN_TRAINING_YEARS = 8  # Minimum 8 years of training data\n",
    "    MAX_TRAINING_YEARS = 12  # Maximum 12 years of training data\n",
    "    VAL_YEARS = 6  # 6-year validation window\n",
    "    \n",
    "    # Paths & Tickers\n",
    "    directory = r\"C:\\Users\\victo\\git_new\\thesis_vri_vp\\data\"\n",
    "    factor_file = os.path.join(directory, \"1estimation_index_returns.csv\")\n",
    "    market_file = os.path.join(directory, \"1macro_data.csv\")\n",
    "    factors = [\"iwf\", \"mtum\", \"qual\", \"size\", \"usmv\", \"vlue\"]\n",
    "\n",
    "    # Date boundaries\n",
    "    INITIAL_TRAIN_START = \"2002-05-31\"\n",
    "    test_start = \"2017-01-01\"\n",
    "\n",
    "    # Cross-validation settings\n",
    "    lambda_values = np.linspace(30, 400, 10)  # For grid search\n",
    "    \n",
    "    ############################################################################\n",
    "    # 1) Load full data for all factors & market\n",
    "    ############################################################################\n",
    "    # We'll store each factor's entire X and returns in a dict for easy access\n",
    "    factor_data_dict = {}\n",
    "    factor_returns_list = []\n",
    "    for fac in factors:\n",
    "        print(f\"\\nLoading data for factor {fac}\")\n",
    "        data = MergedDataLoader(\n",
    "            factor_file=factor_file,\n",
    "            market_file=market_file,\n",
    "            ver=\"v2\",\n",
    "            factor_col=fac\n",
    "        ).load()\n",
    "        common_idx = (\n",
    "            data.X.index\n",
    "            .intersection(data.ret_ser.index)\n",
    "            .intersection(data.market_ser.index)\n",
    "        )\n",
    "\n",
    "        # Full factor data\n",
    "        X_full = data.X.loc[common_idx]\n",
    "        fac_ret_full = data.ret_ser.loc[common_idx]\n",
    "        mkt_ret_full = data.market_ser.loc[common_idx]\n",
    "        active_ret = fac_ret_full - mkt_ret_full ################################################################################## we can access with attribute\n",
    "\n",
    "        factor_data_dict[fac] = {\n",
    "            \"X\": X_full,\n",
    "            \"fac_ret\": fac_ret_full,\n",
    "            \"mkt_ret\": mkt_ret_full,\n",
    "            \"active_ret\": active_ret\n",
    "        }\n",
    "        factor_returns_list.append(fac_ret_full)\n",
    "\n",
    "    # We'll store the last loaded \"mkt_ret_full\" as \"all_market_ret\"\n",
    "    all_market_ret = mkt_ret_full\n",
    "\n",
    "    # Combine factor returns + market into a single DF\n",
    "    full_factors_df = pd.concat(factor_returns_list, axis=1).dropna()  # T x 6\n",
    "    full_df = pd.concat([full_factors_df, all_market_ret], axis=1).dropna()\n",
    "    full_df.columns = factors + [\"Market\"]\n",
    "\n",
    "    # Our \"test_index\" is from test_start onward\n",
    "    test_slice = filter_date_range(full_df, start_date=test_start)\n",
    "    test_index = test_slice.index.sort_values()\n",
    "\n",
    "    # Define refit and re-cross-validation boundaries\n",
    "    refit_boundaries = (\n",
    "        test_index.to_series()\n",
    "        .resample(REFIT_FREQ)\n",
    "        .last()\n",
    "        .dropna()\n",
    "    )\n",
    "    \n",
    "    recross_val_boundaries = (\n",
    "        test_index.to_series()\n",
    "        .resample(RECROSS_VAL_FREQ)\n",
    "        .last()\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    # Helper function to get appropriate training and validation windows\n",
    "    def get_train_val_windows(\n",
    "        current_date, \n",
    "        full_data, \n",
    "        min_train_years=8,\n",
    "        max_train_years=12, \n",
    "        val_years=6,\n",
    "        initial_train_start=None\n",
    "    ):\n",
    "        # 1) Validation window\n",
    "        val_end = current_date\n",
    "        val_start = val_end - pd.DateOffset(years=val_years)\n",
    "        \n",
    "        # 2) Training window ends just before validation\n",
    "        train_end = val_start - pd.DateOffset(days=1)\n",
    "        \n",
    "        # 3) By default, try the *largest* training window (max_train_years)\n",
    "        train_start_candidate = train_end - pd.DateOffset(years=max_train_years)\n",
    "        \n",
    "        # 4) Don’t go before your global earliest start\n",
    "        if initial_train_start is not None:\n",
    "            earliest_start_dt = pd.to_datetime(initial_train_start)\n",
    "            train_start_candidate = max(train_start_candidate, earliest_start_dt)\n",
    "        \n",
    "        # 5) Ensure we have at least min_train_years from train_start to train_end\n",
    "        if (train_end - train_start_candidate) < pd.Timedelta(days=365.25 * min_train_years):\n",
    "            # Move train_start_candidate to enforce minimum\n",
    "            train_start_candidate = train_end - pd.DateOffset(years=min_train_years)\n",
    "            # Also don’t go before earliest global date\n",
    "            if initial_train_start is not None:\n",
    "                train_start_candidate = max(train_start_candidate, earliest_start_dt)\n",
    "        \n",
    "        # Filter actual data\n",
    "        idx = full_data.index\n",
    "        train_dates = idx[(idx >= train_start_candidate) & (idx <= train_end)]\n",
    "        val_dates   = idx[(idx >= val_start) & (idx <= val_end)]\n",
    "        \n",
    "        if len(train_dates) == 0 or len(val_dates) == 0:\n",
    "            raise ValueError(\n",
    "                f\"No data found for train [{train_start_candidate} to {train_end}]\"\n",
    "                f\" or val [{val_start} to {val_end}]\"\n",
    "            )\n",
    "        \n",
    "        return train_dates[0], train_dates[-1], val_dates[0], val_dates[-1]\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    # 2) Main Trading Loop with Periodic Re-Cross-Validation\n",
    "    ############################################################################\n",
    "    \n",
    "    saved_hyperparams = format_saved_hyperparameters(hyperparameter_history)\n",
    "    \n",
    "    # Run trading with saved hyperparameters\n",
    "    results = run_trading_with_saved_hyperparams(\n",
    "        factor_data_dict=factor_data_dict,\n",
    "        saved_hyperparams=saved_hyperparams,\n",
    "        factors=factors,\n",
    "        test_index=test_index,\n",
    "        full_df=full_df,\n",
    "        REFIT_FREQ=REFIT_FREQ,\n",
    "        MIN_TRAINING_YEARS=MIN_TRAINING_YEARS,\n",
    "        MAX_TRAINING_YEARS=MAX_TRAINING_YEARS,\n",
    "        INITIAL_TRAIN_START=INITIAL_TRAIN_START\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    # 3) Analyze Results and Compare to Benchmarks\n",
    "    ############################################################################\n",
    "    if len(\"returns\") == 0:\n",
    "        raise ValueError(\"No daily returns computed. Check your date ranges or data.\")\n",
    "\n",
    "    all_portfolio_rets = results[\"returns\"]\n",
    "    all_weights_df = results[\"weights\"]\n",
    "    all_daily_states = results[\"states\"]  # If you need this for analysis\n",
    "    cumrets = all_portfolio_rets.cumsum()\n",
    "\n",
    "    # Compare to equal-weight benchmark\n",
    "    overlap_idx = all_portfolio_rets.index.intersection(full_df.index)\n",
    "    test_returns_df = full_df.loc[overlap_idx, :]\n",
    "    ew_bench_rets = test_returns_df.mean(axis=1)\n",
    "    ew_bench_cum = ew_bench_rets.cumsum()\n",
    "    \n",
    "    # Also compute quarterly-rebalanced EW portfolio\n",
    "    quarterly_ends = test_returns_df.index.to_series().resample(\"QE\").last().dropna()\n",
    "    n_assets = test_returns_df.shape[1]\n",
    "    quarterly_ew_rets = pd.Series(index=test_returns_df.index, dtype=float)\n",
    "\n",
    "    for i in range(len(quarterly_ends) - 1):\n",
    "        start_q = quarterly_ends.iloc[i]\n",
    "        end_q = quarterly_ends.iloc[i + 1]\n",
    "        mask = (test_returns_df.index > start_q) & (test_returns_df.index <= end_q)\n",
    "        chunk_dates = test_returns_df.index[mask]\n",
    "        \n",
    "        w = np.ones(n_assets) / n_assets\n",
    "        \n",
    "        for day in chunk_dates:\n",
    "            r_i = test_returns_df.loc[day].values\n",
    "            r_p = np.dot(w, r_i)\n",
    "            quarterly_ew_rets.loc[day] = r_p\n",
    "            \n",
    "            w = w * (1 + r_i)\n",
    "            if (1 + r_p) != 0:\n",
    "                w /= (1 + r_p)\n",
    "\n",
    "    quarterly_ew_rets = quarterly_ew_rets.dropna()\n",
    "    quarterly_ew_cum = quarterly_ew_rets.cumsum()\n",
    "\n",
    "    ############################################################################\n",
    "    # 4) Plot Results and Print Performance Statistics\n",
    "    ############################################################################\n",
    "    # Plot 1: Cumulative returns comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    cumrets.plot(label=\"Dynamic BL Portfolio\")\n",
    "    ew_bench_cum.plot(label=\"Daily EW Benchmark\", linestyle=\"--\")\n",
    "    quarterly_ew_cum.plot(label=\"Quarterly EW Benchmark\", linestyle=\"-.\")\n",
    "    \n",
    "    # Add vertical lines at re-cross-validation dates\n",
    "    for date in recross_val_boundaries:\n",
    "        if date in cumrets.index:\n",
    "            plt.axvline(x=date, color='r', linestyle='--', alpha=0.3, \n",
    "                        label=\"Re-Cross-Validation\" if date == recross_val_boundaries.iloc[0] else \"\")\n",
    "    \n",
    "    plt.title(\"Cumulative Returns Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Factor weights over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.stackplot(all_weights_df.index, all_weights_df.T.values, labels=all_weights_df.columns)\n",
    "    plt.title(\"Out-of-Sample Portfolio Weights Over Time\")\n",
    "    plt.xlabel(\"Trading Days\")\n",
    "    plt.ylabel(\"Weights\")\n",
    "    plt.legend(\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1, 0.5),\n",
    "        fontsize='small'  # smaller font size for legend\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Print performance statistics\n",
    "    print(\"\\n==============================================================\")\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"==============================================================\")\n",
    "    print(f\"Period: {all_portfolio_rets.index[0]} to {all_portfolio_rets.index[-1]}\")\n",
    "    print(f\"Number of trading days: {len(all_portfolio_rets)}\")\n",
    "    print(\"\\nAnnualized Performance Metrics:\")\n",
    "    print(f\"Daily EW Benchmark Sharpe: {annualized_sharpe(ew_bench_rets):.4f}\")\n",
    "    print(f\"Quarterly EW Benchmark Sharpe: {annualized_sharpe(quarterly_ew_rets):.4f}\")\n",
    "    print(f\"Dynamic BL Portfolio Sharpe: {annualized_sharpe(all_portfolio_rets):.4f}\")\n",
    "    print(f\"\\nTotal Returns:\")\n",
    "    print(f\"Daily EW Benchmark Return: {ew_bench_cum.iloc[-1]:.4f}\")\n",
    "    print(f\"Quarterly EW Benchmark Return: {quarterly_ew_cum.iloc[-1]:.4f}\")\n",
    "    print(f\"Dynamic BL Portfolio Return: {cumrets.iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# ======================================================\n",
    "# For Daily EW Benchmark Comparison:\n",
    "# ======================================================\n",
    "# Align the dynamic portfolio and daily EW benchmark returns by their common dates\n",
    "aligned_bl_daily, aligned_ew_daily = all_portfolio_rets.align(ew_bench_rets, join='inner')\n",
    "\n",
    "# Paired t-test for daily returns\n",
    "t_stat_daily, p_value_daily = stats.ttest_rel(aligned_bl_daily, aligned_ew_daily)\n",
    "print(\"\\nPaired t-test for BL dynamic vs. Daily EW Benchmark:\")\n",
    "print(f\"t-statistic: {t_stat_daily:.4f}, p-value: {p_value_daily:.4f}\")\n",
    "\n",
    "# Wilcoxon signed-rank test for daily returns\n",
    "wilcoxon_stat_daily, wilcoxon_p_daily = stats.wilcoxon(aligned_bl_daily, aligned_ew_daily)\n",
    "print(\"\\nWilcoxon test for BL dynamic vs. Daily EW Benchmark:\")\n",
    "print(f\"Statistic: {wilcoxon_stat_daily:.4f}, p-value: {wilcoxon_p_daily:.4f}\")\n",
    "\n",
    "# ======================================================\n",
    "# For Quarterly EW Benchmark Comparison:\n",
    "# ======================================================\n",
    "# Since both series are daily, if the quarterly benchmark is provided as daily data,\n",
    "# simply align them by their dates.\n",
    "aligned_bl_quarterly, aligned_quarterly = all_portfolio_rets.align(quarterly_ew_rets, join='inner')\n",
    "\n",
    "# Paired t-test for the quarterly EW benchmark (using daily data)\n",
    "t_stat_quarterly, p_value_quarterly = stats.ttest_rel(aligned_bl_quarterly, aligned_quarterly)\n",
    "print(\"\\nPaired t-test for BL dynamic vs. Quarterly EW Benchmark:\")\n",
    "print(f\"t-statistic: {t_stat_quarterly:.4f}, p-value: {p_value_quarterly:.4f}\")\n",
    "\n",
    "# Wilcoxon signed-rank test for the quarterly EW benchmark\n",
    "wilcoxon_stat_quarterly, wilcoxon_p_quarterly = stats.wilcoxon(aligned_bl_quarterly, aligned_quarterly)\n",
    "print(\"\\nWilcoxon test for BL dynamic vs. Quarterly EW Benchmark:\")\n",
    "print(f\"Statistic: {wilcoxon_stat_quarterly:.4f}, p-value: {wilcoxon_p_quarterly:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print hyperparameter evolution\n",
    "    # print(\"\\n==============================================================\")\n",
    "    # print(\"HYPERPARAMETER EVOLUTION\")\n",
    "    # print(\"==============================================================\")\n",
    "    # for fac in factors:\n",
    "    #     if hyperparameter_history[fac]:\n",
    "    #         print(f\"\\nFactor: {fac}\")\n",
    "    #         for entry in hyperparameter_history[fac]:\n",
    "    #             print(f\"Date: {entry['date']}\")\n",
    "    #             #print(f\"  Lambda: {entry['old_lambda']:.2f} -> {entry['new_lambda']:.2f}\")\n",
    "    #             print(f\"  Kappa²: {int(entry['old_kappa']**2)} -> {int(entry['new_kappa']**2)}\")\n",
    "    #             print(f\"  Sharpe: {entry['sharpe']:.4f}\")\n",
    "    #     else:\n",
    "    #         print(f\"\\nFactor: {fac} - No parameter changes\")\n",
    "\n",
    "    # # Print training window evolution\n",
    "    # if training_window_history:\n",
    "    #     print(\"\\n==============================================================\")\n",
    "    #     print(f\"TRAINING WINDOW EVOLUTION ({WINDOW_TYPE.upper()})\")\n",
    "    #     print(\"==============================================================\")\n",
    "    #     for entry in training_window_history:\n",
    "    #         print(f\"Date: {entry['date']}\")\n",
    "    #         print(f\"  Window: {entry['start_date']} to {entry['end_date']}\")\n",
    "    #         print(f\"  Size: {entry['years']:.2f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68050ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hyperparameters_over_time(hyperparameter_history, factor):\n",
    "    \"\"\"\n",
    "    Plot the evolution of hyperparameters (lambda and kappa) over time for a given factor.\n",
    "    \n",
    "    Parameters:\n",
    "      hyperparameter_history (dict): Dictionary with factor keys and lists of hyperparameter history records.\n",
    "      factor (str): The name of the factor to plot.\n",
    "    \"\"\"\n",
    "    # Convert the list of history records to a DataFrame\n",
    "    df = pd.DataFrame(hyperparameter_history[factor])\n",
    "    \n",
    "    # Ensure the 'date' column is in datetime format and sort by date\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.sort_values('date', inplace=True)\n",
    "    \n",
    "    # Set up the figure with dual y-axes for lambda and kappa\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot lambda on the primary y-axis (left)\n",
    "    ax1.plot(df['date'], df['new_lambda'], marker='o', color='blue', label='Lambda')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Lambda', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    # Create a second y-axis to plot kappa\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df['date'], df['new_kappa'], marker='x', linestyle='--', color='red', label='Kappa')\n",
    "    ax2.set_ylabel('Kappa', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    # Optional: Combine legends from both axes for clarity\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.title(f\"Evolution of Hyperparameters for Factor {factor}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: visualize hyperparameters for each factor\n",
    "# Assume hyperparameter_history is a dictionary like:\n",
    "# {\n",
    "#   \"factor1\": [{\"date\": \"2020-01-01\", \"old_lambda\": ..., \"new_lambda\": ..., \"old_kappa\": ..., \"new_kappa\": ..., \"sharpe\": ...}, ...],\n",
    "#   \"factor2\": [ ... ],\n",
    "#   ...\n",
    "# }\n",
    "for fac in hyperparameter_history:\n",
    "    plot_hyperparameters_over_time(hyperparameter_history, fac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40393dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
